<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Offline Processor on INFINI Labs</title>
    <link>/en/docs/latest/gateway/references/processors/</link>
    <description>Recent content in Offline Processor on INFINI Labs</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="/en/docs/latest/gateway/references/processors/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>bulk_indexing</title>
      <link>/en/docs/latest/gateway/references/processors/bulk_indexing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/bulk_indexing/</guid>
      <description>bulk_indexing #  Description #  The bulk_indexing processor is used to asynchronously consume bulk requests in queues.
Configuration Example #  A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: bulk_size_in_mb: 1 queue_selector.labels: type: bulk_reshuffle level: cluster Parameter Description #     Name Type Description     elasticsearch string The default Elasticsearch cluster ID, which will be used if elasticsearch is not specified in the queue Labels   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.</description>
    </item>
    
    <item>
      <title>dag</title>
      <link>/en/docs/latest/gateway/references/processors/dag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/dag/</guid>
      <description>dag #  Description #  The dag processor is used to manage the concurrent scheduling of tasks.
Configuration Example #  The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.
pipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: &#39;announce champion&#39; - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message.</description>
    </item>
    
    <item>
      <title>dump_hash</title>
      <link>/en/docs/latest/gateway/references/processors/dump_hash/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/dump_hash/</guid>
      <description>dump_hash #  Description #  The dump_hash processor is used to export index documents of a cluster and calculate the hash value.
Configuration Example #  A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1&#39;s doc indices: &amp;quot;medcl-dr3&amp;quot; scroll_time: &amp;quot;10m&amp;quot; elasticsearch: &amp;quot;source&amp;quot; query: &amp;quot;field1:elastic&amp;quot; fields: &amp;quot;doc_hash&amp;quot; output_queue: &amp;quot;source_docs&amp;quot; batch_size: 10000 slice_size: 5 Parameter Description #     Name Type Description     elasticsearch string Name of a target cluster   scroll_time string Scroll session timeout duration   batch_size int Scroll batch size, which is set to 5000 by default   slice_size int Slice size, which is set to 1 by default   sort_type string Document sorting type, which is set to asc by default   sort_field string Document sorting field   indices string Index   level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests.</description>
    </item>
    
    <item>
      <title>flow_runner</title>
      <link>/en/docs/latest/gateway/references/processors/flow_runner/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/flow_runner/</guid>
      <description>flow_runner #  Description #  The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.
Configuration Example #  A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: &amp;quot;primary_deadletter_requests&amp;quot; flow: primary-flow-post-processing when: cluster_available: [ &amp;quot;primary&amp;quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   flow string Flow used to consume requests in consumption queues   commit_on_tag string A message is committed only when a specified tag exists in the context of the current request.</description>
    </item>
    
    <item>
      <title>index_diff</title>
      <link>/en/docs/latest/gateway/references/processors/index_diff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/index_diff/</guid>
      <description>index_diff #  Description #  The index_diff processor is used to compare differences between two result sets.
Configuration Example #  A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: &amp;quot;diff_result&amp;quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: &#39;source_docs&#39; target_queue: &#39;target_docs&#39; Parameter Description #     Name Type Description     source_queue string Name of source data   target_queue string Name of target data   diff_queue string Queue that stores difference results   buffer_size int Memory buffer size   keep_source bool Whether difference results contain document source information   text_report bool Whether to output results in text form    </description>
    </item>
    
    <item>
      <title>indexing_merge</title>
      <link>/en/docs/latest/gateway/references/processors/indexing_merge/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/indexing_merge/</guid>
      <description>indexing_merge #  描述 #  The indexing_merge processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the bulk_indexing processor, and batch writes are used instead of single requests to improve write throughput.
Configuration Example #  A simple example is as follows:
pipeline: - name: indexing_merge auto_start: true keep_running: true processor: - indexing_merge: input_queue: &amp;quot;request_logging&amp;quot; elasticsearch: &amp;quot;logging-server&amp;quot; index_name: &amp;quot;infini_gateway_requests&amp;quot; output_queue: name: &amp;quot;gateway_requests&amp;quot; label: tag: &amp;quot;request_logging&amp;quot; worker_size: 1 bulk_size_in_mb: 10 - name: logging_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ &amp;quot;logging-server&amp;quot; ] Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description>
    </item>
    
    <item>
      <title>json_indexing</title>
      <link>/en/docs/latest/gateway/references/processors/json_indexing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/json_indexing/</guid>
      <description>json_indexing #  Description #  The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.
Configuration Example #  A simple example is as follows:
pipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: &amp;quot;gateway_requests&amp;quot; elasticsearch: &amp;quot;dev&amp;quot; input_queue: &amp;quot;request_logging&amp;quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description>
    </item>
    
    <item>
      <title>queue_consumer</title>
      <link>/en/docs/latest/gateway/references/processors/queue_consumer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/queue_consumer/</guid>
      <description>queue_consumer #  Description #  The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.
Configuration Example #  A simple example is as follows:
pipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: &amp;quot;backup&amp;quot; elasticsearch: &amp;quot;backup&amp;quot; waiting_after: [ &amp;quot;backup_failure_requests&amp;quot;] worker_size: 20 when: cluster_available: [ &amp;quot;backup&amp;quot; ] Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.</description>
    </item>
    
    <item>
      <title>replay</title>
      <link>/en/docs/latest/gateway/references/processors/replay/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/en/docs/latest/gateway/references/processors/replay/</guid>
      <description>replay #  Description #  The replay processor is used to replay requests recorded by the record filter.
Configuration Example #  A simple example is as follows:
pipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: &amp;quot;http&amp;quot; host: &amp;quot;localhost:8000&amp;quot; Parameter Description #     Name Type Description     filename string Name of a file that contains replayed messages   schema string Request protocol type: http or https   host string Target server that receives requests, in the format of host:port    </description>
    </item>
    
  </channel>
</rss>
