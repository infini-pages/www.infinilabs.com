'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/en/docs/latest/gateway/references/filters/echo/','title':"echo",'section':"Online Filter",'content':"echo #  Description #  The echo filter is used to output specified characters in the returned result. It is often used for debugging.\nFunction Demonstration #    Configuration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; The echo filter allows you to set the number of times that same characters can be output repeatedly. See the following example.\n... - echo: message: \u0026quot;hello gateway\\n\u0026quot; repeat: 3 ... Parameter Description #     Name Type Description     message string Characters to be output   repeat int Number of repetition times   stdout bool Whether the terminal also outputs the characters. The default value is false.    "});index.add({'id':1,'href':'/en/docs/latest/gateway/','title':"INFINI Gateway",'section':"Docs",'content':"INFINI Gateway #  Introduction #  INFINI Gateway is a high performance gateway for Elasticsearch. It offers a broad range of features and is easy to use. INFINI Gateway works in the same way as a common reverse proxy. It is usually deployed in front of Elasticsearch clusters. All requests are sent to the gateway instead of Elasticsearch, and then the gateway forwards the requests to the back-end Elasticsearch clusters. The gateway is deployed between the client and Elasticsearch. Therefore, the gateway can be configured to perform index-level traffic control and throttling, cache acceleration for common queries, query request audit, and dynamic modification of query results.\nLearn More  Features #   The application-layer INFINI Gateway is especially designed for Elasticsearch and offers powerful features.\n  High availability: The gateway supports non-stop indexing and is capable of automatically processing faults occurring on Elasticsearch, without affecting normal data ingestion. Write acceleration: The gateway can automatically merge independent index requests into a bulk request, thereby reducing back-end pressure and improving indexing efficiency. Query acceleration: Query cache can be configured on INFINI Gateway and Kibana dashboards can accelerate the query seamlessly and intelligently to fully enhance search experience. Seamless retry: The gateway automatically processes faults occurring on Elasticsearch, and migrates and retries query requests. Traffic cloning: The gateway can replicate traffic to multiple different Elasticsearch clusters and supports traffic migration through canary deployment. One-click rebuilding: The optimized high-speed index rebuilding and automatic processing of incremental data enable the gateway to seamlessly switch between old and new indexes. Secure transmission: The gateway supports the Transport Layer Security (TLS) and Hypertext Transfer Protocol Secure (HTTPS) protocols. It can automatically generate certification files and supports specified trust certification files. Precision routing: The gateway supports the load balancing mode using multiple algorithms, in which load routing strategies can be separately configured for indexing and query, providing great flexibility. Traffic control and throttling: Multiple traffic control and throttling rules can be configured to implement index-level traffic control and ensure the stability of back-end clusters. Concurrency control: The gateway can control cluster- and node-level concurrent TCP connections to ensure the stability of back-end clusters and nodes. No single point of failure (SPOF): The built-in virtual IP-based high availability solution supports dual-node hot standby and automatic failover to prevent SPOFs. Request observability: The gateway is equipped with the logging and indicator monitoring features to fully analyze Elasticsearch requests.  Get Started Now  Community #   Join Slack Community\nWho Is Using INFINI Gateway? #  If you are using INFINI Gateway and feel it pretty good, please let us know. All our user cases are located here. Thank you for your support.\n"});index.add({'id':2,'href':'/en/docs/latest/console/reference/initialization/','title':"Initialization Guide",'section':"Reference",'content':"Initialization Guide #  Introduction #  After the initial install, it will enter the initialization guide page where you need to initialize some configurations, such as system cluster and default user。\n配置 #  Connecting to system cluster (elasticsearch required version 7.3 or above).\n  TLS\nDefault is http，enable is https。\n  Auth\nAuthentication is not required to default, it is required to enable。\n    Test Connection\nTest the connection configuration and proceed to the next step after success.\n  Initialization #  When entering the initialization step, it will verify whether the old data exists in the cluster. You can choose to use the old data or delete the data before initialization.\n Old data exists  You can use the script prompted to delete the old data, click Refresh, and enter the default user initialization.\nYou can also skip this step and reuse the existing data.\n  No old data exists\nIt will enter the initialization default user interface.\n  Finish #  After initialization is completed, you can enter the Console。\n"});index.add({'id':3,'href':'/en/docs/latest/gateway/overview/','title':"Overview",'section':"INFINI Gateway",'content':"Overview #  Introduction #  INFINI Gateway is a high performance gateway for Elasticsearch. It offers a broad range of features and is easy to use. INFINI Gateway works in the same way as a common reverse proxy. It is usually deployed in front of Elasticsearch clusters. All requests are sent to the gateway instead of Elasticsearch, and then the gateway forwards the requests to the back-end Elasticsearch clusters. The gateway is deployed between the client and Elasticsearch. Therefore, the gateway can be configured to perform index-level traffic control and throttling, cache acceleration for common queries, query request audit, and dynamic modification of query results.\nFeatures #  INFINI Gateway caters to Elasticsearch well. Many Elasticsearch-related service scenarios and characteristics are taken into account in the design. Therefore, INFINI Gateway is tailored to provide many useful features for Elasticsearch.\nLightweight INFINI Gateway is written in Golang. The installation package is only about 10 MB and has no external environment dependency. Deployment and installation are very simple. Users can simply download the binary executable file of the gateway program from the platform and execute the program file.  Optimal Performance INFINI Gateway is designed to run in an optimized state during programming. Test results show that INFINI Gateway provides a speed that is over 25% faster than mainstream gateway counterparts, and which has been optimized to allow Elasticsearch to double the write and query speeds.   Cross-Version Support INFINI Gateway is compatible with different Elasticsearch versions to ensure seamless adaptation of the service code. The back-end Elasticsearch cluster versions can be upgraded seamlessly, which reduces the complexity of version upgrade and data migration.  Observability INFINI Gateway can dynamically intercept and analyze requests generated during the running of Elasticsearch. Users can learn about the running status of the entire cluster from indicators and logs, in an effort to improve performance and optimize services. It can be also used for auditing and slow query analysis.   High Availability INFINI Gateway has multiple built-in high availability (HA) solutions. The front-end request entry supports virtual IP-based dual-node hot standby. The back-end cluster supports auto perception of the cluster topology, auto discovery of nodes that go online/offline, auto processing of back-end faults, and auto retry and migration of requests.  Flexible and Extensible Each module of INFINI Gateway can be independently extended and each request can be flexibly handled and routed. INFINI Gateway supports intelligent learning of routes and provides rich internal filters. The processing logic of each request can be dynamically modified. INFINI Gateway can also be extended using plug-ins.   Seamless Integration #  The external interfaces provided by INFINI Gateway are fully compatible with Elasticsearch\u0026rsquo;s native interfaces. The integration is very simple and can be completed by changing the configuration pointed to Elasticsearch to the address of the gateway.\nWhy Is INFINI Gateway Needed? #  I largely understand the above integration interaction diagram and am familiar with the use of Elasticsearch. Why do I need to place a gateway in front of it?\nIf the scale of your Elasticsearch cluster is quite large, consider the following scenarios:\nWAF and Security #  The prevalence of Elasticsearch makes it a prime target for hackers, which necessitates the use of a Web application firewall (WAF). Whether it\u0026rsquo;s the use of cross-site script attacks, cross-site scripting injection, weak passwords, brute force cracking, or unreasonable query parameter abuse by programmers, INFINI Gateway is able to detect and verify requests from different Web application clients. It utilizes a series of Elasticsearch security policies to ensure security and legitimacy and block illegitimate requests in real time.\nCluster Upgrade #  The Elasticsearch iteration is pretty fast and cluster upgrade needs to be handled frequently. However, the following points must be considered in the cluster upgrade:\n Minimal downtime: The service data writing and query cannot be interrupted due to the cluster upgrade. Data can be continuously written but data cannot be lost due to the restart of back-end nodes. Cluster traffic switching. You need to determine when and how to switch traffic from the old cluster to the new one, whether to modify the service code or configuration file, how to roll back and restore the system, and whether to release a new deployment package.  With INFINI Gateway, you do not need to care about the back-end Elasticsearch clusters in the service code but only need to access the fixed address of the gateway. Then, INFINI Gateway will solve all problems for you.\nIndex Rebuilding #  Index rebuilding is required when mappings or the segmentation dictionary is modified. Data writing cannot be stopped during rebuilding, data must be consistent after rebuilding, and new data and modified data must be handled, which are cumbersome. INFINI Gateway supports one-click index rebuilding and automatically records any document modifications that take place during rebuilding. It switches from the old index to the new one seamlessly after rebuilding, which is completely imperceptive to front-end applications.\nThrottling and Traffic Control #  A cluster may break down due to burst traffic or become overburdened due to large indexes. For this, you need to manage abnormal traffic in order to protect the entire Elasticsearch cluster against abnormal traffic and even malicious attacks. INFINI Gateway can control the traffic flexibly and allows setting index-level traffic control rules. There are a thousand traffic control rules for a thousand indexes.\nSlow Query #  The built-in cache function of INFINI Gateway can cache the most common queries and warm up specific queries according to periodic query plans to ensure that queries are hit each time for front-end services, thereby increasing query speed and improving user\u0026rsquo;s service query experience.\nSlow Indexing #  INFINI Gateway can combine many small batches of Elasticsearch index requests from different clients into one large bulk request, and deliver the index requests to a specified node of a specified shard through shard-level precision routing. In this way, back-end Elasticsearch does not need to forward the requests, saving Elasticsearch resources and bandwidth, and improving overall throughput and performance of the cluster.\nRequest Mutation #  What happens if you discover errors in query statements after the code goes live? With INFINI gateway, you do not need to worry about it. You can rewrite a specified query of a specified service online and correct the query statements dynamically, without re-publishing the application, which offers convenience and flexibility. If you are not satisfied with the JSON query results returned by Elasticsearch, you can utilize INFINI gateway to dynamically replace query results, and even merge data from other sources such as Hbase and MySQL into the required JSON data, which is then returned to the client.\nRequest Analysis #  People complain about the slow response of Elasticsearch, but do you know which indexes in Elasticsearch are slow? Which queries cause slow response? Which users are accountable for the slow response? INFINI Gateway tracks Elasticsearch from clusters to indexes, from indexes to queries, and from applications to users so that you know every detail about the Elasticsearch clusters.\nIn a word, using Elasticsearch together with INFINI gateway will give you a splendid experience.\nArchitecture #  The architecture diagram below shows the core modules of INFINI Gateway.\nThe modules that carry external requests as a proxy are as follows: entry, router, flow, and filter. One entry needs one router, one router can route requests to multiple flows, and one flow is composed of multiple filters.\nEntry #  The entry module defines the request entry for the gateway. INFINI Gateway supports the Hypertext Transfer Protocol (HTTP) and Hypertext Transfer Protocol Secure (HTTPS) modes. It automatically generates certification files in HTTPS mode.\nRouter #  The router module mainly defines routing rules for requests, and routes requests to a specified flow according to the method and request address.\nFlow #  The flow module mainly defines the processing logic of data. Each request will go through a series of filter operations, and a flow is used to organize these filter operations.\nFilter #  The filter module is composed of several different filter components. Each filter is designed to cope with only one task, and multiple filters compose a single flow.\nPipeline #  The pipeline module is composed of several different processor components. Compared with a flow, a pipeline focuses on the processing of offline tasks.\nQueue #  The queue module is an abstract message queue, such as local disk-based reliability message persistence, Redis, Kafka, and other adapters. Different back-end adapters can be set for queues based on scenarios.\nAt the bottom layer of the framework used by INFINI Gateway, there are some common modules, such as the API used to provide an external programming entry and the Elastic module used to handle the API encapsulation for different versions of Elasticsearch.\nNext #   View Downloading and Installation  "});index.add({'id':4,'href':'/en/docs/latest/console/reference/platform/overview/','title':"Platform Overview",'section':"Platform",'content':"Platform Overview #  Introduction #  You can view the main indicators at these levels(cluster, node, indice and host) to understand the operation status.\nCluster #  Node #  Indice #  Host #  Host data comes from INFINI Agent reporting and node discovery of elasticsearch.\nDiscover Host #  Click the button \u0026ldquo;Discover Host\u0026rdquo; on the right side of the host list，then click the button \u0026ldquo;add hosts\u0026rdquo; to add the host to the host list after checking.\n"});index.add({'id':5,'href':'/en/docs/latest/console/','title':"INFINI Console",'section':"Docs",'content':"INFINI Console #  Introduction #  INFINI Console is a very lightweight multi-cluster, cross-version unified Elasticsearch governance platform. Through the centralized management of Elasticsearch, you can quickly and conveniently manage multiple sets of Elasticsearch clusters within the enterprise.\nArchitecture #  Features #   INFINI Console is powerful, lightweight, and very easy to use.\n  Support multi-cluster management, you can manage any number of clusters at the same time in one platform; Multi-version Elasticsearch support, support 1.x, 2.x, 5.x, 6.x, 7.x, 8.x; Supports grouping and managing cluster metadata information on a project-by-project basis, support custom tags; Support dynamic clusters registration, and the target cluster can be accessed and managed on the fly; Supports viewing the historical version of cluster metadata, and can view the changes and audits of cluster metadata. Developer tools support multiple workspaces to switch quickly, support smart suggestion, frequent used commands support to save and load; Supports unified multi-cluster level index and API interface granularity permission control; Supports a unified alerting engine across clusters and flexibly configure alarm rules based on thresholds; Support unified monitoring of any version, including cluster, node, index and other very detailed dimensions of the metrics viewing and analysis; Support common management operations of indices, support quick viewing and browsing of indices, and support updates and deletes of documents in the index; Support the creation of indexed data views, you can modify the display format of fields, and support the quick viewing of time series index data; Support for cross-platform deployment environments, support for MacOS (Intel and M1), Windows (32-bit and 64-bit), Linux (32-bit and 64-bit); Support x86, arm5, arm6, arm7, mips, mipsle, mips64 and other CPU architectures: Support for Docker containers and K8s cloud-native environments; Support for the management of INFINI gateways;  INFINI Console is written in Golang, the installation package is very small, only about 11MB, there is no external environment dependency, deployment and installation are very simple, just need to download the binary executable file of the corresponding platform, start the application and then you are good to go.\nGet Started Now  Screenshot #  Community #   Join Slack Community\nWho Is Using? #  If you are using INFINI Console and feel it pretty good, please let us know. All our user cases are located here. Thank you for your support.\n"});index.add({'id':6,'href':'/en/docs/latest/release-notes/','title':"Release notes",'section':"Docs",'content':"INFINI release notes #  Find out what’s new in INFINI Search Platform! Release notes contain information about new features, improvements, known issues, and bug fixes in each release. You can find release notes for each component in the docs section. We suggest that you regularly visit the release notes to learn about updates.\n  INFINI Search  INFINI Gateway  INFINI Console  INFINI Agent  INFINI Loadgen  "});index.add({'id':7,'href':'/en/docs/latest/console/reference/data/','title':"Data",'section':"Reference",'content':"Data management #  Introduction #  INFINI Console Data allows you to seamlessly roam in different business clusters, supports common index management operations, quickly view and browse document information in the index, and edit and delete documents on the spot. At the same time, it supports creating data views, modifying the display format of fields, and quickly viewing time series index data with one click.\n"});index.add({'id':8,'href':'/en/docs/latest/console/reference/devtool/','title':"Dev Tools",'section':"Reference",'content':"Dev Tools #  Introduction #  Use Dev Tools to quickly write and execute Elasticsearch queries and other elasticsearch APIs. When installation verification is enabled, all requests will go through API level permission verification\nOpen Dev Tools #  Use the Ctrl+Shift+O shortcut to open or click the icon in the upper right corner of the console.\nExecute request shortcuts #  Command+Enter or Ctrl+Enter\nMulti-cluster multi-tab page support #  The Dev Tools supports the use of tab pages to open multiple clusters at the same time. Even if it is the same cluster, multiple clusters can be opened, and the status of the tab pages is independent. The tab page uses the cluster name as the title by default, and can be modified by double-clicking the tab page title. Below the Dev Tools is a status bar, and on the left is the health status, http address, and version information of the current cluster. On the right is the response status and duration of the elasticsearch interface request.\nView request header information #  After using the Dev Tools to execute the elasticsearch request, you can click the headers Tab page on the right to view the request header information.\n"});index.add({'id':9,'href':'/en/docs/latest/console/getting-started/install/','title':"Installation",'section':"Quick Start",'content':"Installing the Console #  INFINI Console supports mainstream operating systems and platforms, the package is small, without any additional external dependencies, it should be very fast to install :)\nPreparation before Installation #  Prepare an Elasticsearch cluster that can store data. The required version is 7.3 or above, which is used for INFINI Console to store related data.\nDownloading #  Select a package for downloading in the following URL based on your operating system and platform:\n https://release.infinilabs.com/console/\nContainer Deployment #  INFINI Console also supports Docker container deployment.\nLearn More  Configure #  After downloading and decompressing the installation package of INFINI Console, open the console.yml configuration file, we can see the following configuration sections:\n#Elasticsearch cluster version should be v7.3+ for storing INFINI Console related data elasticsearch: - name: default enabled: true monitored: true endpoint: http://192.168.3.188:9299 basic_auth: username: elastic password: ZBdkVQUUdF1Sir4X4BGB In general, we only need to modify the endpoint in the configuration. If Elasticsearch has security verification enabled, we need to modify the username and password configurations.\nThe username here requires full access to cluster metadata, index metadata, and .infini* indexes, and to create index templates.\nFor the initial settings of the index template, please refer here to Learn more。\nStarting the Console #  The Console can be started by directly running the program (the mac version is used here, and the program file names of different platforms are slightly different), as follows:\n➜ BOOTSTRAP_USERNAME=admin BOOTSTRAP_PASSWORD=123456 ./console-mac-amd64 ___ __ ___ ___ / __\\/ / /___\\/\\ /\\ / \\ / / / / // // / \\ \\/ /\\ / / /__/ /__/ \\_//\\ \\_/ / /_// \\____|____|___/ \\___/___,' ___ ___ __ __ ___ __ __ / __\\/___\\/\\ \\ \\/ _\\ /___\\/ / /__\\ / / // // \\/ /\\ \\ // // / /_\\ / /__/ \\_// /\\ / _\\ \\/ \\_// /__//__ \\____|___/\\_\\ \\/ \\__/\\___/\\____|__/ [CONSOLE] INFINI Cloud Console, The easiest way to operate your own elasticsearch platform. [CONSOLE] 0.3.0_SNAPSHOT, 2022-03-31 10:26:41, 2023-12-31 10:10:10, fa04f6010144b7c5267c71ccaee30230ddf2432d [03-31 20:27:40] [INF] [app.go:174] initializing console. [03-31 20:27:40] [INF] [app.go:175] using config: /Users/shiyang/infini/console-0.3.0_SNAPSHOT-447-mac-amd64/console.yml. [03-31 20:27:40] [INF] [instance.go:72] workspace: /Users/shiyang/infini/console-0.3.0_SNAPSHOT-447-mac-amd64/data/console/nodes/c92psf1pdamk8rdhgqpg [03-31 20:27:40] [INF] [app.go:283] console is up and running now. [03-31 20:27:40] [INF] [metrics.go:63] ip:192.168.3.12, host:shiyangdeMacBook-Pro.local, labels:, tags: [03-31 20:27:40] [INF] [elastic.go:136] loading [5] remote elasticsearch configs [03-31 20:27:40] [INF] [actions.go:280] elasticsearch [default] is available [03-31 20:27:40] [INF] [actions.go:280] elasticsearch [lsy_cluster_1] is available [03-31 20:27:40] [INF] [actions.go:280] elasticsearch [es-v710] is available [03-31 20:27:40] [INF] [actions.go:280] elasticsearch [es-v7140] is available [03-31 20:27:40] [INF] [ui.go:197] ui listen at: http://0.0.0.0:9000 [03-31 20:27:40] [INF] [module.go:116] all modules are started Seeing the above startup information, it means that the Console has successfully run and monitored port 9000. You can log in and use by accessing port 9000 in the browser. The initial user name and password information are passed through the environment variables BOOTSTRAP_USERNAME, BOOTSTRAP_PASSWORD ` to set.\nShutting Down the Console #  To shut down INFINI Console, hold down Ctrl+C. The following information will be displayed:\n^C [CONSOLE] got signal: interrupt, start shutting down [03-31 20:33:10] [INF] [module.go:145] all modules are stopped [03-31 20:33:10] [INF] [app.go:267] console now terminated. [CONSOLE] 0.3.0_SNAPSHOT, uptime: 5m30.307832s __ _ __ ____ __ _ __ __ / // |/ // __// // |/ // / / // || // _/ / // || // / /_//_/|_//_/ /_//_/|_//_/ ©INFINI.LTD, All Rights Reserved. System Service #  To run the Console as a background task, run the following commands:\n➜ ./console -service install Success ➜ ./console -service start Success Unloading the service is simple. To unload the service, run the following commands:\n➜ ./console -service stop Success ➜ ./console -service uninstall Success "});index.add({'id':10,'href':'/en/docs/latest/console/reference/agent/manage/manage/','title':"Overview",'section':"Agent",'content':"Agent Overview #  Overview #  Agent Manage includes functions such as auditing, viewing status, tasks setting, etc. It is the place to manage Agent.\nAdd Agent #  Agent \u0026gt; Instance click Discover Agent。 Select the correct Agent and click Add Agents.\nDelete Agent #  Agent \u0026gt; Instance，click Delete, delete after confirmation。\nTask Settings #  Agent \u0026gt; Instance，click Task Setting，configure the task, then click Save\n"});index.add({'id':11,'href':'/en/docs/latest/gateway/tutorial/log4j2_filtering/','title':"Protect Elasticsearch from Apache Log4j Vulnerability",'section':"Tutorials",'content':"Protect Elasticsearch from Apache Log4j Vulnerability #  CVE Address\n https://github.com/advisories/GHSA-jfh8-c2jp-5v3q\nVulnerability Description\nApache Log4j is a very popular open source logging toolkit used for the Java runtime environment. Many Java frameworks including Elasticsearch of the latest version, use this component. Therefore, the scope of impact is huge.\nThe latest vulnerability existing in the execution of Apache Log4j\u0026rsquo;s remote code was revealed recently. Attackers can construct malicious requests and utilize this vulnerability to execute arbitrary code on a target server. As a result, the server can be controlled by hackers, who can then conduct page tampering, data theft, mining, extortion, and other behaviors. Users who use this component are advised to immediately initiate emergency response for fixing.\nBasically, if a log output by Log4j contains the keyword ${, the log is replaced as a variable and then the variable operation is executed. Attackers can maliciously construct log content to make Java processes to execute arbitrary commands, achieving the attack purpose.\nVulnerability Level: very urgent\nThe vulnerability is caused by the lookup function provided by Log4j2. This function allows developers to read configurations in the environment by using a number of protocols. However, the input is not strictly judged in the implementation, resulting in the vulnerability.\nImpact Scope: Java products: Apache Log4j 2.x \u0026lt; 2.15.0-rc2\nAttack Detection\nYou can check logs for jndi:ldap://, jndi:rmi, and other characters to find out possible attacks.\nHandling Method #  If Elasticsearch does not support configuration modification, Jar package replacement of Log4j, or cluster restart, you can use INFINI Gateway to intercept requests, replace parameters, and even directly block requests. You can use INFINI Gateway to check parameters in requests sent to Elasticsearch and replace or reject content that contains the sensitive keyword ${. In this way, INFINI Gateway can prevent the execution of malicious attack commands during Log4j logging after attack-contained requests are sent to Elasticsearch, thereby preventing attacks.\nReference Configuration #  Download the latest 1.5.0-SNAPSHOT version: http://release.elasticsearch.cn/gateway/snapshot/\nThe context_filter filter of INFINI Gateway can be used to detect the keywords of the request context _ctx.request.to_string and filter out malicious traffic, thereby blocking attacks.\npath.data: data path.logs: log entry: - name: es_entrypoint enabled: true router: default max_concurrency: 20000 network: binding: 0.0.0.0:8000 router: - name: default default_flow: main_flow flow: - name: main_flow filter: - context_filter: context: _ctx.request.to_string action: redirect_flow status: 403 flow: log4j_matched_flow must_not: # any match will be filtered regex: - \\$\\{.*?\\} - \u0026quot;%24%7B.*?%7D\u0026quot; #urlencode contain: - \u0026quot;jndi:\u0026quot; - \u0026quot;jndi:ldap:\u0026quot; - \u0026quot;jndi:rmi:\u0026quot; - \u0026quot;jndi%3A\u0026quot; #urlencode - \u0026quot;jndi%3Aldap%3A\u0026quot; #urlencode - \u0026quot;jndi%3Armi%3A\u0026quot; #urlencode - elasticsearch: elasticsearch: es-server - name: log4j_matched_flow filter: - echo: message: 'Apache Log4j 2, Boom!' elasticsearch: - name: es-server enabled: true endpoints: - http://localhost:9200 Use urlencode to convert the test command ${java:os} into %24%7Bjava%3Aos%7D.\nRequest calling execution result when requests do not need to pass through the gateway:\n~% curl 'http://localhost:9200/index1/_search?q=%24%7Bjava%3Aos%7D' {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;index_not_found_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;no such index\u0026quot;,\u0026quot;resource.type\u0026quot;:\u0026quot;index_or_alias\u0026quot;,\u0026quot;resource.id\u0026quot;:\u0026quot;index1\u0026quot;,\u0026quot;index_uuid\u0026quot;:\u0026quot;_na_\u0026quot;,\u0026quot;index\u0026quot;:\u0026quot;index1\u0026quot;},\u0026quot;status\u0026quot;:404}% Logs on Elasticsearch are as follows:\n[2021-12-11T01:49:50,303][DEBUG][r.suppressed ] path: /index1/_search, params: {q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1} org.elasticsearch.index.IndexNotFoundException: no such index at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.infe(IndexNameExpressionResolver.java:678) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.innerResolve(IndexNameExpressionResolver.java:632) ~[elasticsearch-5.6.15.jar:5.6.15] at org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:580) ~[elasticsearch-5.6.15.jar:5.6.15] The logs above show that q=${java:os} in query conditions is executed and is changed to q=Mac OS X 10.13.4 unknown, architecture: x86_64-64, index=index1.\nRequest calling execution result when requests need to pass through the gateway:\nmedcl@Medcl:~% curl 'http://localhost:8000/index1/_search?q=%24%7Bjava%3Aos%7D' Apache Log4j 2, Boom!% The logs above show that requests are filtered out.\nYou can try other commands to check whether malicious requests are intercepted:\n#{java:vm} ~% curl 'http://localhost:9200/index/_search?q=%24%7Bjava%3Avm%7D' [2021-12-11T02:36:04,764][DEBUG][r.suppressed ] [Medcl-2.local] path: /index/_search, params: {q=OpenJDK 64-Bit Server VM (build 25.72-b15, mixed mode), index=index} ~% curl 'http://localhost:8000/index/_search?q=%24%7Bjava%3Avm%7D' Apache Log4j 2, Boom!% #{jndi:rmi://localhost:1099/api} ~% curl 'http://localhost:9200/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D' 2021-12-11 03:35:06,493 elasticsearch[YOmFJsW][search][T#3] ERROR An exception occurred processing Appender console java.lang.SecurityException: attempt to add a Permission to a readonly Permissions object ~% curl 'http://localhost:8000/index/_search?q=%24%7Bjndi%3Armi%3A%2F%2Flocalhost%3A1099%2Fapi%7D' Apache Log4j 2, Boom!%  The benefits of using INFINI Gateway is that no change needs to be made to the Elasticsearch server, especially in large-scale cluster scenarios. The flexible INFINI Gateway can significantly reduce workload, improve efficiency, shorten the security processing time, and reduce enterprise risks.\n "});index.add({'id':12,'href':'/en/docs/latest/gateway/tutorial/online_query_rewrite/','title':"Rewrite Your Elasticsearch Requests OnTheFly",'section':"Tutorials",'content':"Rewrite Your Elasticsearch Requests OnTheFly #  In some cases, you may find that the QueryDSL generated by the service code is unreasonable. The general practice is to modify the service code and publish it online. If the launch of a new version takes a long time (for example, the put-into-production window is not reached, major network operation closure is in progress, or additional code needs to be submitted to go live), a large number of tests need to be performed. However, faults in the production environment need to be rectified immediately and customers have no time to wait. What should be done in that case?\nDon\u0026rsquo;t worry. You can use INFINI Gateway to dynamically repair queries.\nExample #  See the following query example:\nGET _search { \u0026quot;size\u0026quot;: 1000000 , \u0026quot;explain\u0026quot;: true } The size parameter is set to a very large value and the problem is not found at the beginning. With more and more data generated, too much returned data is bound to cause a sharp decline in performance. In addition, enabling the explain parameter will create unnecessary performance overhead and this function is generally used only during development and debugging.\nBy adding the request_body_json_set filter to the gateway, you can dynamically replace the value of the specified request body JSON PATH. The configuration for the above example is as follows:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - explain -\u0026gt; false - size -\u0026gt; 10 - dump_request_body: - elasticsearch: elasticsearch: dev Set the explain and size parameters again. The query is rewritten in the following format before it is sent to Elasticsearch:\n{ \u0026quot;size\u0026quot;: 10, \u0026quot;explain\u0026quot;: false } The problem is successfully fixed in in-service mode.\nAnother Example #  Look at the following query example. The programmer who writes the code writes the name of the field to be queried by mistake. The name should be name but is written as name1. The size parameter is set to a very large value.\nGET medcl/_search { \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } } The system goes live but a problem arises when a query is conducted. For this problem, you can add the following filter configuration to the gateway request flow:\nflow: - name: rewrite_query filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 10 - size -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, we can replace the data of the JSON request body through its path, and add one parameter not to return the query document because only aggregated results are required.\nAnother Example #  The user query is as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{\u0026quot;term\u0026quot;:{\u0026quot;isDel\u0026quot;:0}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] }\t} } Now you want to replace the term query with the equivalent range query as follows:\n{ \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;should\u0026quot;:[{ \u0026quot;range\u0026quot;: { \u0026quot;isDel\u0026quot;: {\u0026quot;gte\u0026quot;: 0,\u0026quot;lte\u0026quot;: 0 }}},{\u0026quot;match\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;order\u0026quot;}}] }\t} } Use the following configuration:\nflow: - name: rewrite_query filter: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev In the above configuration, one request_body_json_del filter is used to delete the first element from the Should query, that is, the Term subquery to be replaced. There is only one Match query left. One Should subquery is added, and the added subscript should be 1. Set the attributes of the Range query.\nFurther Improvement #  In the above examples, queries are directly replaced. In general, you may need to make a judgment about whether to replace the query, for example, replacement may only be performed when the _ctx.request.body_json.query.bool.should.[0].term.isDel JSON field exists. The conditional judgment of the gateway is very flexible and the configuration is as follows:\nflow: - name: cache_first filter: - if: and: - has_fields: ['_ctx.request.body_json.query.bool.should.[0].term.isDel'] then: - request_body_json_del: path: - query.bool.should.[0] - request_body_json_set: path: - query.bool.should.[1].range.isDel.gte -\u0026gt; 0 - query.bool.should.[1].range.isDel.lte -\u0026gt; 0 - dump_request_body: - elasticsearch: elasticsearch: dev The feature is superb!\n"});index.add({'id':13,'href':'/en/docs/latest/console/reference/platform/monitoring/','title':"Cluster Monitoring",'section':"Platform",'content':"Cluster Monitoring #  Introduction #  When the monitoring of the registered cluster is enabled, the INFINI Console will periodically collect data from the target cluster according to the corresponding configuration. Including some metrics at the cluster, node, and index level. These metrics can then be observed in cluster monitoring to understand the running status of the target cluster.\nList of Elasticsearch API permissions required for monitoring #  _cluster/health，_cluster/stats，_cat/shards, /_nodes/\u0026lt;node_id\u0026gt;/stats _cat/indices, _stats, _cluster/state, _nodes, _alias, _cluster/settings\nEnable cluster monitoring #  When registering the cluster or modifying the cluster configuration, you can see the following interface\nYou can see that there is a Monitored switch. When this switch is turned on, it means that the current cluster is monitored. When a cluster is registered, monitoring is enabled by default. The monitoring configuration includes cluster health metrics, cluster metrics, node metrics and index metrics. And you can set whether to open and the collection time interval respectively.\n The above are the settings for a single cluster. In the configuration file console.yaml, you can set the monitoring start and stop of all clusters. By default, you can see the following configuration in the configuration file:\nmetrics: enabled: true major_ip_pattern: \u0026quot;192.*\u0026quot; queue: metrics elasticsearch: enabled: true cluster_stats: true node_stats: true index_stats: true If metrics\u0026gt;enable is set to false, then all cluster monitoring is disabled; If metrics\u0026gt;elasticsearch\u0026gt;cluster_stats\u0026gt;enabled is set to false, then all The cluster will not collect related metrics at the cluster level.\n View cluster metrics monitoring #  After monitoring is enabled, you can view the monitoring information of the cluster in the monitoring report under the platform management on the left menu of INFINI Console, as follows:\nClick the Advanced tab to view more metrics at the cluster level;\nAs shown in the figure, you can specify multiple nodes in a cluster to view node-related metrics and compare them horizontally. By default, the top 5 node metrics are displayed (top 5 nodes are calculated based on the sum of the query qps and write qps of the node in the last 15 minutes). Switching to the index tab page here can also specify several related metrics to view the index, similar to the node. Switch to the Thread Pool tab to view the related metrics of the node\u0026rsquo;s thread pool.\nView node metrics monitoring #  Click the Nodes tab to view a list of cluster nodes.\nClick the node name in the list to view the monitoring of the specified node\nHere you can view the metrics monitoring information and related fragmentation information of a single node\nView index metrics monitoring #  Click the Indexes tab to see a list of cluster indexes.\nClick the node name in the list to view the monitoring of the specified index\nHere you can view the metrics monitoring information and related fragmentation information of a single node\n"});index.add({'id':14,'href':'/en/docs/latest/console/reference/system/cluster/','title':"Cluster Management",'section':"System",'content':"Cluster Management #  Introduction #  Cluster management can quickly and easily help us manage multiple Elasticsearch clusters across versions.\nCluster list #  The registered Elasticsearch cluster can be queried in the cluster list\nCluster registration #  The first step is to fill in the cluster address, and enable TLS and authentication as needed (you need to input a user name and password after enabling authentication).\nThe second step is to confirm the information\n Modify the cluster name and cluster description as needed; Whether to enable monitoring (enabled by default), after enabling monitoring, you can view various metrics of the Elasticsearch cluster in the console monitoring function Whether to enable Discovery (recommended), after enabling, the console will automatically discover all nodes in the cluster. When the configured cluster address is unavailable, the console will try to use the automatically discovered addresses available in other nodes to interact with Elasticsearch  Update cluster configuration #  Click the Edit button in the cluster list table to input the update interface\nModify the configuration as needed, then click the save button to submit\nDelete cluster #  Click the delete button in the cluster list table to confirm the second time. After confirming the deletion, execute the delete operation.\n"});index.add({'id':15,'href':'/en/docs/latest/console/reference/alerting/message/','title':"Alerting Center",'section':"Alerting",'content':"Alerting center #  Introduction #  By default, the message center displays the alerting events that are currently occurring in the system, which is convenient for administrators to quickly preview the execution status of the system.\nMessage list #  The message list aggregates all triggered alerting events. If each alerting rule repeatedly triggers multiple alerting messages, only one will be aggregated and displayed here. Click the details to see more information.\nMessage details #  Click the Details button in the message list row and column to view the detailed content of the current alerting event message, including the basic information of the event message, the timing curve within the event trigger period, and the history of rule execution detection, etc., as shown in the following figure:\nIgnore warning messages #  If you think that the alerting event does not need to be processed or is not important, you can ignore it. After ignoring, the alerting message will not be displayed in the message list by default, but it can be queried by status filtering.\nOperation steps: Click the ignore button in the message list form to confirm the second time, fill in the ignore reason, and execute the ignore operation after submitting.\n"});index.add({'id':16,'href':'/en/docs/latest/console/reference/system/command/','title':"Common Commands",'section':"System",'content':"Common Commands #  Introduction #  Common commands are used to save frequently used Elasticsearch requests in Dev toolss, so that if you need to use them later, Just use the LOAD command in the Dev tools to load, and it can be used quickly.\nSave frequently used commands #  Open the Dev tools (Ctrl+shift+o) in the upper right corner of the console, and select the Elasticsearch request to be saved in the Dev tools (Supports selecting multiple requests at one time and saving them as common commands), after selecting, click Save As Command in the toolbar to submit.\nLoad common commands #  In the Dev tools, input LOAD + saved command name keyword will automatically prompt related saved common commands, After selecting the command to be loaded, press the Input key to automatically load the corresponding common command.\nCommon command list #  In the list of common commands, you can query the saved common commands\nClick the name column of common commands in the list to view the specific information of common commands, and you can also modify the name and tag information\nDelete common commands #  Click the Delete button in the list of frequently used commands to confirm twice, and then execute the delete operation.\n"});index.add({'id':17,'href':'/en/docs/latest/console/reference/alerting/rule/','title':"Alerting Rules",'section':"Alerting",'content':"Alerting Rules #  Introduction #  The alerting rules include the configuration of four parts: data source, metrics definition, trigger condition, and message notification\nAlerting rules list #  In the alerting rules list, you can query the alerting rules that have been added\nNew alerting rule #  Click the New button in the alerting rule list to enter the new alerting rule page\nConfigure data source #   Select a cluster (required) Select index, support input index pattern (required) Input elasticsearch query DSL query filter conditions (optional) Select time field (required) Select the statistical period (for time field aggregation, the default is one minute)  Configure alerting metrics and trigger conditions #   Input the rule name Add the grouped fields and group size as needed, you can add more than one for terms aggregation Select the metrics aggregation field and statistics type, you can configure more than one, when configuring more than one, you must configure a formula to calculate the final metrics Configure alerting trigger conditions Select execution check cycle Input the title of the alerting event (template, referenced by the title in the template variable, click here to learn about template syntax ) Input alerting event message (template, referenced by message in template variable, click here for template syntax )  Configure message notification #   Configure notification channels, which can be reconfigured, or you can use the add button to select an already created channel as a template to quickly fill in, and support adding multiple Choose whether to enable notification upgrades as needed Select silence period (how often notification messages are sent) Configure notification sending time period Click the save button to submit  Update alerting rules #  Select the alerting rule to be updated in the alerting rules list and click the Edit button to enter the update alerting rules page\nDelete alerting rules #  Click the delete button in the alerting rule list table to confirm the second time. After confirming the deletion, execute the delete operation.\nImport of common rule templates #  Some common Alerting rules are listed below, and notification channels such as DingTalk, Enterprise WeChat, and Slack are configured. You only need to replace the custom variables specified in the template, and you can quickly import the rules through the DevTools tool of the Console.\n  Cluster Health Change to Red  Index Health Change to Red  Disk utilization is Too High  CPU utilization is Too High  JVM utilization is Too High  Shard Storage \u0026gt;= 55G  Elasticsearch node left cluster  Search latency is great than 500ms  Too Many Deleted Documents  "});index.add({'id':18,'href':'/en/docs/latest/gateway/tutorial/request-logging/','title':"Elasticsearch Search Requests Analysis/Audit",'section':"Tutorials",'content':"Elasticsearch Search/Request Log Analysis/Audit #  INFINI Gateway can track and record all requests that pass through the gateway and analyze requests sent to Elasticsearch, to figure out request performance and service running status.\nSetting a Gateway Router #  To enable the query log analysis of INFINI Gateway, configure the tracing_flow parameter on the router and set a flow to log requests.\nrouter: - name: default tracing_flow: request_logging default_flow: cache_first In the above configuration, one router named default is defined, the default request flow is cache_first, and the flow for logging is request_logging.\nDefining a Log Flow #  The log processing flow request_logging is defined as follows:\nflow: - name: request_logging filter: - request_path_filter: must_not: # any match will be filtered prefix: - /favicon.ico - request_header_filter: exclude: - app: kibana # in order to filter kibana's access log, config `elasticsearch.customHeaders: { \u0026quot;app\u0026quot;: \u0026quot;kibana\u0026quot; }` to your kibana's config `/config/kibana.yml` - logging: queue_name: request_logging The above flow uses several filters:\n The request_path_filter filters out invalid /favicon.ico requests. The request_header_filter filters out requests from Kibana. The logging filter logs requests to the local disk array request_logging so that the pipeline consumes and creates indexes.  Defining a Log Pipeline #  INFINI Gateway uses a pipeline task to asynchronously consume logs and create indexes. The configuration is as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB In the above configuration, one processing pipeline named request_logging_index is defined, a consumption disk queue named request_logging, an index target cluster dev, and an index named gateway_requests are set, one work thread is used, and the batch submission size is set as 10 MB.\nDefining an Logging Cluster #  Configure an elasticsearch cluster to save request logging as follows:\nelasticsearch: - name: dev enabled: true endpoint: https://192.168.3.98:9200 # if your elasticsearch is using https, your gateway should be listen on as https as well basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions username: elastic password: pass discovery: # auto discovery elasticsearch cluster nodes enabled: true refresh: enabled: true In the above configuration, one Elasticsearch cluster named dev is defined and will be used for saving request logging.\nConfiguring an Index Template #  Configure an index template for the Elasticsearch cluster. Run the following commands on the dev cluster to create a log index template.\n Expand to View Elasticsearch Template Definition ...  PUT _template/.infini-gateway-default { \u0026quot;order\u0026quot;: 100000, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;gateway_requests\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;max_result_window\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ], \u0026quot;properties\u0026quot;: { \u0026quot;request\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;response\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;body\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } } }, \u0026quot;aliases\u0026quot;: {} }     Configuring the Index Lifecycle #   Expand to View the Index Lifecycle Definition ...  PUT _ilm/policy/30days-retention { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;0ms\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;max_size\u0026quot;: \u0026quot;50gb\u0026quot; }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } }, \u0026quot;warm\u0026quot;: { \u0026quot;actions\u0026quot;: { \u0026quot;forcemerge\u0026quot;: { \u0026quot;max_num_segments\u0026quot;: 1 }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 50 } } }, \u0026quot;cold\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;3d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;allocate\u0026quot;: { \u0026quot;number_of_replicas\u0026quot;: 1, \u0026quot;include\u0026quot;: {}, \u0026quot;exclude\u0026quot;: {}, \u0026quot;require\u0026quot;: { \u0026quot;box_type\u0026quot;: \u0026quot;warm\u0026quot; } }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 0 } } }, \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;delete_searchable_snapshot\u0026quot;: true } } } } } } PUT _template/gateway_requests-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;gateway_requests-*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;gateway_requests\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;routing\u0026quot; : { \u0026quot;allocation\u0026quot; : { \u0026quot;require\u0026quot; : { \u0026quot;box_type\u0026quot; : \u0026quot;hot\u0026quot; }, \u0026quot;total_shards_per_node\u0026quot; : \u0026quot;1\u0026quot; } }, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } DELETE gateway_requests-00001 PUT gateway_requests-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;gateway_requests\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;gateway_requests\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }     Importing the Dashboard #  Download the latest dashboard INFINI-Gateway-7.9.2-2021-01-15.ndjson.zip for Kibana 7.9 and import it into Kibana of the dev cluster as follows:\nStarting the Gateway #  Start the gateway.\n➜ ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, a17be4c, Wed Feb 3 00:12:02 2021 +0800, medcl, add extra retry for bulk_indexing [02-03 13:51:35] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [02-03 13:51:35] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [02-03 13:51:35] [INF] [runner.go:59] pipeline: request_logging_index started with 1 instances [02-03 13:51:35] [INF] [entry.go:267] entry [es_gateway] listen at: http://0.0.0.0:8000 [02-03 13:51:35] [INF] [app.go:297] gateway now started. Modifying Application Configuration #  Replace the Elasticsearch address with the gateway address for applications directed to the Elasticsearch address (such as Beats, Logstash, and Kibana). Assume that the gateway IP address is 192.168.3.98. Modify the Kibana configuration as follows:\n# The Kibana server's name. This is used for display purposes. #server.name: \u0026quot;your-hostname\u0026quot; # The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\u0026quot;https://192.168.3.98:8000\u0026quot;] elasticsearch.customHeaders: { \u0026quot;app\u0026quot;: \u0026quot;kibana\u0026quot; } # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \u0026quot;.kibana\u0026quot; # The default application to load. #kibana.defaultAppId: \u0026quot;home\u0026quot; Save the configuration and restart Kibana.\nChecking the Results #  All requests that access Elasticsearch through the gateway can be monitored.\n"});index.add({'id':19,'href':'/en/docs/latest/gateway/references/modules/floating_ip/','title':"Floating IP",'section':"Functional Component",'content':"Floating IP #  The embedded floating IP feature of INFINI Gateway can implement dual-node hot standby and failover. INFINI Gateway innately provides high availability for L4 network traffic, and no extra software and devices are required to prevent proxy service interruption caused by downtime or network failures.\nNote:\n This feature supports only Mac OS and Linux OS. The gateway must run as the user root. This feature relies on the ping and ifconfig commands of the target system. Therefore, ensure that related packages are installed by default. The network interface cards (NICs) of a group of gateways, on which floating IP is enabled, should belong to the same subnet, and devices on the Intranet can communicate with each other in broadcast mode (the actual IP address and floating IP address of a gateway need to be different only in the last bit, for example, 192.168.3.x).   Function Demonstration #    Youtube  Bilibili  What Is a Floating IP? #  INFINI Gateway achieves high availability by using a floating IP, which is also called a virtual IP or dynamic IP. Each server must have an IP address for communication and the IP address of a server is usually static and allocated in advance. If the server malfunctions, the IP address and the services deployed on the server are inaccessible. A floating IP address is usually a public and routable IP address that is not automatically allocated to a physical device. The project manager can temporarily allocate this dynamic IP address to one or more physical devices. The physical devices have automatically assigned static IP addresses for communicating with devices on the Intranet. This Intranet uses private addresses that are not routable. Services of physical devices on the Intranet can be identified and accessed by external networks only through the floating IP address.\nWhy Is a Floating IP Needed? #  One typical floating IP switching scenario is that, when a device bound with a floating IP address malfunctions, the floating IP address floats to another device on the network. The new device immediately replaces the faulty device to provide services externally. This creates high availability for network services. For service consumers, only the floating IP needs to be specified. Floating IPs are very useful. In certain scenarios, for example, only one service IP address is allowed for the client or SDK, which means that the IP address must be highly available. INFINI Gateway can effectively solve this problem. When two independent INFINI Gateway servers are used, you are advised to deploy them on independent physical servers. The two INFINI Gateways work in dual-node hot standby mode. If any of the gateways malfunction, front-end services can still be accessed.\nEnabling Floating IP #  To enable the floating IP feature of INFINI Gateway, modify the gateway.yml configuration file by adding the following configuration:\nfloating_ip: enabled: true INFINI Gateway can automatically detect NIC device information and bind the virtual IP address to the Intranet communication port. It is very intelligent and easy to use. By default, the IP address to be listened to is *.*.*.234 in the network segment, to which the machine belongs. Assume that the physical IP address of the machine is 192.168.3.35. The default floating IP address is 192.168.3.234. This default IP address is only used to facilitate configuration and quick startup. If you need to use a user-defined floating IP address, supplement complete parameters.\nRelated Parameter Settings #  The following is an example of configuration parameters about floating IP:\nfloating_ip: enabled: true ip: 192.168.3.234 netmask: 255.255.255.0 interface: en1 The parameters are described as follows:\n   Name Type Description     enabled bool Whether floating IP is enabled, which is set to false by default.   interface string NIC device name. If this parameter is not specified, the name of the first device that listens to the first non-local address is selected. If a server has multiple NIC cards, you are advised to manually set this parameter.   ip string Listened floating IP address, which is *.*.*.234 in the network segment, to which the current physical NIC belongs. You are advised to manually set the floating IP address. The floating IP address cannot conflict with an existing IP address.   netmask string Subnet mask of the floating IP address, which is the subnet mask of the NIC or 255.255.255.0 by default.    "});index.add({'id':20,'href':'/en/docs/latest/console/tutorials/start_with_specify_user/','title':"How to specify built-in account name and password to start INFINI Console",'section':"Tutorials",'content':"How to specify built-in account name and password to start INFINI Console #  Prepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features  INFINI Console built-in account #  INFINI Console has a built-in administrator account with security turned on. Without specifying an account name and password, INFINI Console defaults to the built-in account\u0026rsquo;s username and password as admin.\nSpecify the account name and password to start INFINI Console #  INFINI Console supports the use of environment variables to specify the account name and password to start. The following takes the Macos operating system as an example:\nBOOTSTRAP_USERNAME=admin BOOTSTRAP_PASSWORD=123456 ./console-mac-amd64\ndisable built-in accounts #  Due to the use of built-in accounts, there may be security risks, such as simple password settings. So when we use the built-in account After logging in to the INFINI Console to create a new administrator account, you can log in with the new administrator account, and then Disable built-in accounts in System \u0026gt; Security Settings. After disabling it, you cannot use the built-in account to log in to the INFINI Console.\nCreate admin account #  INFINI Console has a built-in administrator role Administrator, which is assigned when a new user is created, and the new user has administrator privileges.\nClick the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page. Then click the New button to enter the Create User page\n Input username root role selection Administrator Click the save button to submit Save the initial password after successful saving for backup  Disable built-in user with newly created admin #  Use the root user and password created in the previous step to log in to the INFINI Console, and then turn on the disable built-in account switch in System \u0026gt; Security Settings. When you see the following interface, the operation is successful.\n"});index.add({'id':21,'href':'/en/docs/latest/console/reference/data/indices/','title':"Index Management",'section':"Data",'content':"Index management #  Index list #  The index list includes addition, deletion, modification, and lookup operations on indexes.\nNew index #  Input the new index name and index settings to complete the addition.\nIndex details #  You can view the index health status, number of shards, number of documents, storage size and other details, as well as view and modify Mappings and Edit settings.\n"});index.add({'id':22,'href':'/en/docs/latest/console/getting-started/ilm/','title':"Index Settings",'section':"Quick Start",'content':"Index Settings #  All monitoring metrics of INFINI Console are stored in the Elasticsearch index. As time goes by, the data will increase. We can configure the life cycle of the index to adapt to our monitoring storage requirements.\nConfigure the Default Index Template #  Then you can configure the index template of the Elasticsearch cluster, and execute the following command on the system monitoring cluster to create the index template.\n Expand to see Elasticsearch template definitions ...  PUT _template/.infini { \u0026quot;order\u0026quot;: 0, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;.infini_*\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;max_result_window\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;mapping\u0026quot;: { \u0026quot;total_fields\u0026quot;: { \u0026quot;limit\u0026quot;: \u0026quot;20000\u0026quot; } }, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;suggest_text_search\u0026quot;: { \u0026quot;filter\u0026quot;: [ \u0026quot;word_delimiter\u0026quot; ], \u0026quot;tokenizer\u0026quot;: \u0026quot;classic\u0026quot; } } }, \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot;: {} }     Configure ILM for index .infini_metrics #   Expand to view settings ...  PUT _ilm/policy/infini_metrics-30days-retention { \u0026quot;policy\u0026quot;: { \u0026quot;phases\u0026quot;: { \u0026quot;hot\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;0ms\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;rollover\u0026quot;: { \u0026quot;max_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;max_size\u0026quot;: \u0026quot;50gb\u0026quot; }, \u0026quot;set_priority\u0026quot;: { \u0026quot;priority\u0026quot;: 100 } } }, \u0026quot;delete\u0026quot;: { \u0026quot;min_age\u0026quot;: \u0026quot;30d\u0026quot;, \u0026quot;actions\u0026quot;: { \u0026quot;delete\u0026quot;: { \u0026quot;delete_searchable_snapshot\u0026quot;: true } } } } } } PUT _template/.infini_metrics-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;.infini_metrics*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_metrics\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;translog.durability\u0026quot;:\u0026quot;async\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } # delete old index DELETE .infini_metrics DELETE .infini_metrics-00001 PUT .infini_metrics-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_metrics\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_metrics\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }     If the index .infini_metrics already exists before, it needs to be deleted first.\nConfigure ILM for index .infini_alert-history #  The alert module stores a large amount of index data for execution records, so you need to configure ILM as follows:\n Expand to view settings ...  PUT _template/.infini_alert-history-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;.infini_alert-history*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_alert-history\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;translog.durability\u0026quot;:\u0026quot;async\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } DELETE .infini_alert-history DELETE .infini_alert-history-00001 PUT .infini_alert-history-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_alert-history\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_alert-history\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot; : { \u0026quot;condition\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;items\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;expression\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;minimum_period_match\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;long\u0026quot; }, \u0026quot;operator\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;severity\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;values\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 } } }, \u0026quot;operator\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 } } }, \u0026quot;condition_result\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;object\u0026quot;, \u0026quot;enabled\u0026quot; : false }, \u0026quot;context\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;created\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; }, \u0026quot;expression\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;is_escalated\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;is_notified\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;message\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;objects\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;resource_id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;resource_name\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;rule_id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;rule_name\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;search_text\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;suggest_text_search\u0026quot;, \u0026quot;index_prefixes\u0026quot; : { \u0026quot;min_chars\u0026quot; : 2, \u0026quot;max_chars\u0026quot; : 5 }, \u0026quot;index_phrases\u0026quot; : true }, \u0026quot;severity\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;state\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;title\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;updated\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; } } } }     Configure ILM for index .infini_activities #   Expand to view settings ...  PUT _template/.infini_activities-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;.infini_activities*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_activities\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;translog.durability\u0026quot;:\u0026quot;async\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } DELETE .infini_activities DELETE .infini_activities-00001 PUT .infini_activities-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_activities\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_activities\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } } }     "});index.add({'id':23,'href':'/en/docs/latest/console/reference/agent/install/','title':"Installing Agent",'section':"Agent",'content':"Installing The Agent #   INFINI Agent is a submodule of INFINI Console, charge of data scraping and Elasticsearch instance manage. it\u0026rsquo;s manage by INFINI Console INFINI Agent supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the agent can be installed very rapidly.  Before You Begin #  Install and keep INFINI Console running.\nDownload #  Download the current INFINI Agent package for your platform. https://release.infinilabs.com/agent/\nContainer Deployment #  INFINI Agent also supports Docker container deployment.\nLearn More  Configuration #  Most of the configuration of INFINI Agent can be completed using agent.yml. After the configuration is modified, the agent program needs to be restarted to make the configuration take effect.\nAfter unzip the file and open agent.yml, you will see this:\n#Used to store data for INFINI Console And Agent #Elasticsearch cluster version should be v7.3+ elasticsearch: - name: default enabled: true monitored: false endpoint: http://192.168.3.4:9200 basic_auth: username: elastic password: ZBdkVQUUdF1Sir4X4BGB ... #INFINI Console Endpoint agent.manager.endpoint: http://192.168.3.4:9000 In most case, you only need to config the endpoint, but if Elasticsearch has security authentication enabled,then config the username and password.\nThe user must have access to the cluster metadata,index metadata,and all indexes with .infini prefix.\nStarting the Agent #  Run the agent program to start INFINI Agent, as follows:\n _ ___ __ __ _____ /_\\ / _ \\ /__\\/\\ \\ \\/__ \\ //_\\\\ / /_\\//_\\ / \\/ / / /\\/ / _ \\/ /_\\\\//__/ /\\ / / / \\_/ \\_/\\____/\\__/\\_\\ \\/ \\/ [AGENT] A light-weight, powerful and high-performance elasticsearch agent. [AGENT] 0.1.0#14, 2022-08-26 14:09:29, 2025-12-31 10:10:10, 4489a8dff2b68501a0dd9ae15276cf5751d50e19 [08-31 15:52:07] [INF] [app.go:164] initializing agent. [08-31 15:52:07] [INF] [app.go:165] using config: /Users/INFINI/agent/agent-0.1.0-14-mac-arm64/agent.yml. [08-31 15:52:07] [INF] [instance.go:72] workspace: /Users/INFINI/agent/agent-0.1.0-14-mac-arm64/data/agent/nodes/cc7h5qitoaj25p2g9t20 [08-31 15:52:07] [INF] [metrics.go:63] ip:192.168.3.22, host:INFINI-MacBook.local, labels:, tags: [08-31 15:52:07] [INF] [api.go:261] api listen at: http://0.0.0.0:8080 [08-31 15:52:07] [INF] [module.go:116] all modules are started [08-31 15:52:07] [INF] [manage.go:180] register agent to console [08-31 15:52:07] [INF] [actions.go:367] elasticsearch [default] is available [08-31 15:52:07] [INF] [manage.go:203] registering, waiting for review [08-31 15:52:07] [INF] [app.go:334] agent is up and running now. If the above startup information is displayed, the agent is running successfully and listening on the responding port.\nBut now agent can\u0026rsquo;t work normally util it\u0026rsquo;s being added to INFINI Console. See Agent Manage\nShutting Down the Agent #  To shut down INFINI Agent, hold down Ctrl+C. The following information will be displayed:\n^C [AGENT] got signal: interrupt, start shutting down [08-31 15:57:13] [INF] [module.go:145] all modules are stopped [08-31 15:57:13] [INF] [app.go:257] agent now terminated. [AGENT] 0.1.0, uptime: 5m6.240314s __ _ __ ____ __ _ __ __ / // |/ // __// // |/ // / / // || // _/ / // || // / /_//_/|_//_/ /_//_/|_//_/ ©INFINI.LTD, All Rights Reserved. System Service #  To run the INFINI Agent as a system service, run the following commands:\n➜ ./agent -service install Success ➜ ./agent -service start Success Uninstall service:\n➜ ./agent -service stop Success ➜ ./agent -service uninstall Success "});index.add({'id':24,'href':'/en/docs/latest/gateway/getting-started/install/','title':"Installing the Gateway",'section':"Quick Start",'content':"Installing the Gateway #  INFINI Gateway supports mainstream operating systems and platforms. The program package is small, with no extra external dependency. So, the gateway can be installed very rapidly.\nInstallation Demo #    Downloading #  Select a package for downloading in the following URL based on your operating system and platform:\n https://release.infinilabs.com/\nContainer Deployment #  INFINI Gateway also supports Docker container deployment.\nLearn More  Verifying the Installation #  After downloading and decompressing INFINI Gateway installation package, run the following command to check whether the installation package is effective:\n✗ ./bin/gateway -v gateway 1.0.0_SNAPSHOT 2021-01-03 22:45:28 6a54bb2 If the above version information is displayed, the gateway program is in good condition.\nStarting the Gateway #  Run the gateway program as an administrator to start INFINI Gateway, as follows:\n➜ sudo ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 4daf6e9, Mon Jan 11 11:40:44 2021 +0800, medcl, add response_header_filter [01-11 16:43:31] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [01-11 16:43:31] [INF] [api.go:255] api server listen at: http://0.0.0.0:2900 [01-11 16:43:31] [INF] [runner.go:59] pipeline: primary started with 1 instances [01-11 16:43:31] [INF] [runner.go:59] pipeline: nodes_index started with 1 instances [01-11 16:43:31] [INF] [entry.go:262] entry [es_gateway] listen at: https://0.0.0.0:8000 [01-11 16:43:32] [INF] [floating_ip.go:170] floating_ip listen at: 192.168.3.234, echo port: 61111 [01-11 16:43:32] [INF] [app.go:254] gateway now started. If the above startup information is displayed, the gateway is running successfully and listening on the responding port.\nAccessing the Gateway #  The back-end Elasticsearch service can be accessed using a browser or other clients through the gateway that serves as a proxy:\nShutting Down the Gateway #  To shut down INFINI Gateway, hold down Ctrl+C. The following information will be displayed:\n^C [GATEWAY] got signal: interrupt, start shutting down [01-11 16:44:41] [INF] [app.go:303] gateway now terminated. [GATEWAY] 1.0.0_SNAPSHOT, uptime: 1m10.550336s Thanks for using GATEWAY, have a good day! System Service #  To run the data platform of INFINI Gateway as a background task, run the following commands:\n➜ ./gateway -service install Success ➜ ./gateway -service start Success Unloading the service is simple. To unload the service, run the following commands:\n➜ ./gateway -service stop Success ➜ ./gateway -service uninstall Success INFINI Gateway has been completely installed. Next, configure the gateway.\nConfiguring INFINI Gateway  "});index.add({'id':25,'href':'/en/docs/latest/gateway/references/entry/','title':"Service Entry",'section':"Reference",'content':"Service Entry #  Defining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: false The network.binding parameter can be used to specify the IP address and port to be bound and listened to after the service is started. INFINI Gateway supports port reuse, that is, multiple INFINI Gateways can share the same IP address and port. In this way, server resources can be fully utilized and the configuration of different gateway processes can be modified dynamically (you can start multiple processes, and then restart the processes in sequence after modifying the configuration), without interrupting normal client requests.\nFor each request sent to the entry, requested traffic is routed by router. Rules are defined for router separately so that the rules are used in different entry settings. In entry, the router parameter can be used to specify the router rules to be used and default is defined here.\nTLS Configuration #  TLS transmission encryption can be seamlessly enabled on INFINI Gateway. You can switch to HTTPS communication mode by setting tls.enabled to true. INFINI Gateway can automatically generate certification files.\nINFINI Gateway also allows you to define the path of the certification file. The configuration is as follows:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 reuse_port: true tls: enabled: true cert_file: /etc/ssl.crt key_file: /etc/ssl.key skip_insecure_verify: false Multiple Services #  INFINI Gateway can listen on multiple service entries at the same time. The listened address, protocol, and router of each service entry can be separately defined to meet different service requirements. The following shows a configuration example.\nentry: - name: es_ingest enabled: true router: ingest_router network: binding: 0.0.0.0:8000 - name: es_search enabled: true router: search_router network: binding: 0.0.0.0:9000 The above example defines a service entry named es_ingest to listen on the address 0.0.0.0:8000, and all requests are processed through ingest_router. In the example, one es_search service is also defined, the listening port is 9000, and search_router is used for request processing to implement read/write separation of services. In addition, different service entries can be defined for different back-end Elasticsearch clusters, and the gateway can forward requests as a proxy.\nIPv6 Support #  INFINI Gateway support to binding to IPv6 address，for example:\nentry: - name: es_ingest enabled: true router: ingest_router network: # binding: \u0026quot;[ff80::4e2:7fb6:7db6:a839%en0]:8000\u0026quot; binding: \u0026quot;[::]:8000\u0026quot; Parameter Description #     Name Type Description     name string Name of a service entry   enabled bool Whether the entry is enabled   max_concurrency int Maximum concurrency connection number, which is 10000 by default.   router string Router name   network object Relevant network configuration   tls object TLS secure transmission configuration   network.host string Network address listened to by the service, for example, 192.168.3.10   network.port int Port address listened to by the service, for example, 8000   network.binding string Network binding address listened to by the service, for example, 0.0.0.0:8000   network.publish string External access address listened to by the service, for example, 192.168.3.10:8000   network.reuse_port bool Whether to reuse the network port for multi-process port sharing   network.skip_occupied_port bool Whether to automatically skip occupied ports   tls.enabled bool Whether TLS secure transmission is enabled   tls.cert_file string Path to the public key of the TLS security certificate   tls.key_file string Path to the private key of the TLS security certificate   tls.skip_insecure_verify bool Whether to ignore TLS certificate verification    "});index.add({'id':26,'href':'/en/docs/latest/console/reference/alerting/channel/','title':"Alerting Channels",'section':"Alerting",'content':"Alerting Channels #  Introduction #  The alerting channel is used to configure the channel for sending notification messages when an alerting rule is triggered. Currently, webhook is supported.\nChannes list #  In the channels list, you can query the channels that have been added\nNew alerting channel #  Click the New button on the channels list page to enter the new alerting channel page\n Input channel name (required) Select channel type (currently only webhook is supported) Input the webhook address Select the method of HTTP request, the default is POST Add HTTP request headers as needed Configure the webhook request body Click the save button to submit  Update channel configuration #  Select the channel to be updated in the channels list and click the Edit button to enter the update channel configuration page\nFor operation reference, create an alerting channel\ndelete alerting channel #  Click the delete button in the alerting channels list table to confirm the second time, and execute the delete operation after confirming the deletion.\n"});index.add({'id':27,'href':'/en/docs/latest/console/reference/data/alias/','title':"Alias Management",'section':"Data",'content':"Alias Management #  Alias list #  The alias list includes addition, deletion, modification, and search operations for aliases.\nNew alias #   Alias: Input an alias name Index: Select the target index corresponding to the alias, and use (*) to bind multiple indexes. Is Write Index: specify whether the selected index is writable. If the alias only binds one index, the index is writable by default; if multiple indexes are bound by (*), it is most necessary to specify one of the indexes as writable .  Alias and index relationship list #  Clicking the + button at the beginning of the alias list row will expand and display the index list bound to the alias, and at the same time, you can set and delete the relational binding update of the index.\n"});index.add({'id':28,'href':'/en/docs/latest/console/tutorials/create_readonly_account/','title':"How to easily create an Elasticsearch \"guest\" user",'section':"Tutorials",'content':"How to easily create an Elasticsearch \u0026ldquo;guest\u0026rdquo; user #  Introduction #  In some cases, we want to share some functions or data with customers, but do not want the data to be modified. At this point we need to create a \u0026ldquo;guest\u0026rdquo; user. This article briefly describes how to create a \u0026ldquo;guest\u0026rdquo; user using the INFINI Console.\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role readonly #  Click the New button, select the platform role, and create a new platform role readonly. The operation steps are as follows:\n Input role name readonly Expand all functional permissions Read permission is selected for all other functions except the security functions under the system settings. Security feature under System Settings is set to None permission. Click the save button to submit   Selecting the All permission of a function represents the read and write operation permission of this function, Read means only have read permission, None means no permission for this function (the function is not available in the menu after the user logs in)\n New data role es-v7171 #  Click the New button, select the data role, and create a new data role es-v7171. The operation steps are as follows:\n Input role name es-v7171 Cluster permission select cluster es-v7171 Click the save button to submit  New account guest #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page. Click the New button to create a new account guest and assign the account role readonly, es-v7171\nClick Save and submit. After the creation is successful, you can use the guest account to log in to the INFINI Console and only have read-only permissions.\n"});index.add({'id':29,'href':'/en/docs/latest/console/tutorials/role_with_different_rights/','title':"How to assign different Elasticsearch cluster access permissions to different INFINI Console accounts",'section':"Tutorials",'content':"How to assign different Elasticsearch cluster access permissions to different INFINI Console accounts #  Introduction #  This article will introduce the use of INFINI Console to assign two different Elasticsearch cluster management permissions to two different accounts\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features Register at least two Elasticsearch clusters to the INFINI Console  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role platform_role #  Click the New button, select the platform role, and create a new platform role platform_role. The operation steps are as follows:\n Input role name platform_role Expand all functional permissions Except for the security functions under the system settings, select the All permission for all other functions. Security feature under System Settings is set to None permission. Click the save button to submit   Selecting the All permission of a function represents the read and write operation permission of this function, Read means only have read permission, None means no permission for this function (the function is not available in the menu after the user logs in)\n New data role es-v7171 #  Click the New button, select the data role, and create a new data role es-v7171\nNew data role es-v630 #  Click the New button, select the data role, create a new data role es-v630, the configuration is similar to the role es-v7171\nCreate Account #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page.\nNew account zhangsan #  Click the New button to create a new account zhangsan and assign the account role platform_role, es-v717\nClick the save button to submit after the creation is successful, save the account password\nNew account wangwu #  Click the New button to create a new account wangwu, and assign the account roles platform_role, es-v630, the configuration is similar to the account zhangsan\nLogin with administrator account #  After logging in with the administrator account, view the platform overview, and you can see all 13 registered clusters\nLogin with account zhangsan #  After logging in with the account zhangsan and viewing the platform overview, you can only see the cluster es-v7171\nLogin with account wangwu #  After logging in with the account zhangsan and viewing the platform overview, you can only see the cluster es-v630\nSummary #  By creating different roles and granting different Elasticsearch cluster permissions, and then assigning roles to users, we can quickly implement Grant different Elasticsearch cluster permissions to different users.\n"});index.add({'id':30,'href':'/en/docs/latest/console/tutorials/role_with_index_limit/','title':"How to assign Elasticsearch index-level permissions to INFINI Console accounts",'section':"Tutorials",'content':"How to assign Elasticsearch index-level permissions to INFINI Console accounts #  Introduction #  This article will introduce the use of INFINI Console to limit an account to only have the management permissions of certain indexes in the Elasticsearch cluster\nPrepare #   Download and install the latest version of INFINI Console Enable INFINI Console Security Features Register at least two Elasticsearch clusters to the INFINI Console  Creating a Role #  Click System \u0026gt; Security Settings on the left menu of INFINI Console, and select the Role Tab page to enter the role management page.\nNew platform role platform_role #  Click the New button, select the platform role, and create a new platform role platform_role\nNew data role test_index_only #  Click the New button, select the data role, create a new data role test_index_only, and then configure the following:\n Select only es-v7140 for the cluster (restrict access to this role only to the Elasticsearch cluster es-v7140) Set index permissions to index only enter the index pattern test* (restrict the role to only index access permissions whose index name matches test*)  After the configuration is complete, click the Save button to submit.\nCreate Account #  Click the left menu of INFINI Console System \u0026gt; Security Settings, select the User Tab page to enter the Account Management page.\nNew account liming #  Click the New button to create a new account liming and assign the account roles platform_role, test_index_only\nClick the save button to submit after the creation is successful, save the account password\nLogin with administrator account #  After logging in with the administrator account, click the menu Data \u0026gt; Index Management, select the cluster es-v7140, and you can see:\nLogin with account liming #  After logging in with the account liming, click the menu Data \u0026gt; Index Management, select the cluster es-v7140, and then you can see:\nSummary #  By specifying the role\u0026rsquo;s Elasticsearch cluster permissions and indexing permissions, it is easy to precisely control user permissions down to the indexing level.\n"});index.add({'id':31,'href':'/en/docs/latest/gateway/getting-started/configuration/','title':"Configuring the Gateway",'section':"Quick Start",'content':"Configuration #  The configuration of INFINI Gateway can be modified in multiple ways.\nCLI Parameters #  INFINI Gateway provides the following CLI parameters:\n✗ ./bin/gateway --help Usage of ./bin/gateway: -config string the location of config file, default: gateway.yml (default \u0026quot;gateway.yml\u0026quot;) -debug run in debug mode, gateway will quit with panic error -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -v version The parameters are described as follows:\n config: Specifies the name of a configuration file. The default configuration file name is gateway.yml in the directory where the currently executed command is located. If your configuration file is stored elsewhere, you can specify the parameter to select it. daemon: Switches the gateway to the background. It needs to be used jointly with pidfile to save the process ID and facilitate subsequent process operations.  Configuration File #  Most of the configuration of INFINI Gateway can be completed using gateway.yml. After the configuration is modified, the gateway program needs to be restarted to make the configuration take effect.\nDefining an Entry #  Each gateway must expose at least one service entrance to receive operation requests of services. In INFINI Gateway, the service entrance is called an entry, which can be defined using the following parameters:\nentry: - name: es_gateway enabled: true router: default network: binding: 0.0.0.0:8000 The above configuration defines one service entry named es_gateway, the address listened to is 0.0.0.0:8000, and one router named default is used to process requests.\nDefining a Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: default default_flow: cache_first This example defines one router named default, which is also the main flow for service handling. Request forwarding, filtering, caching, and other operations are performed in this flow.\nDefining a Flow #  One request flow defines a series of work units for request handling. It adopts a typical pipeline work mode. One typical configuration example is as follows:\nflow: - name: cache_first filter: - get_cache: - elasticsearch: elasticsearch: prod - set_cache: The configuration example defines a flow named cache_first, which uses three different filters: get_cache, elasticsearch, and set_cache. These filters are executed in their configuration sequence. Note that each filter name must be appended with one colon (:). The processing results of the filters are as follows:\n get_cache: This filter is mainly used to get data from the cache. If the same request has been received before and data is cached in the cache, which is within the validity period, this filter can directly take and return the cached data immediately, without further processing. elasticsearch: This filter is used to forward requests to back-end Elasticsearch clusters and further transfer responses returned by Elasticsearch. set_cache: This filter caches execution results to the local memory. It has some parameter restrictions such as the status code and request size, and expiration time is set for the filter so that results in the cache can be used directly when the same request is received next time. It is generally used together with get_cache.  Defining a Resource #  Resources here refer to Elasticsearch back-end server resources. INFINI Gateway supports multiple Elasticsearch clusters. It can forward requests to different clusters and supports blue/green deployment and smooth evolution under canary deployment of requests. The following example shows how to define an Elasticsearch back-end resource.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass The endpoint parameter is used to set the access address for Elasticsearch. If identity authentication is enabled for Elasticsearch, you can use basic_auth to specify the username and password and the user must have the permission to obtain cluster status information. The discover parameter is used to enable automatic discovery to automatically detect the status of back-end nodes and identify new and offline nodes.\nAfter these basic configurations have been completed, INFINI Gateway can normally handle Elasticsearch requests as a proxy. For details about parameters of each component, see the Reference.\n"});index.add({'id':32,'href':'/en/docs/latest/console/reference/data/view/','title':"Data View",'section':"Data",'content':"Data View #  View list #  Creating and managing data views can help you better get data from Elasticsearch.\nCreate data view #  Step 1 Define the data view #   Input a view name Matching rules: Match the corresponding index, you can also use (*) to match multiple indexes.  Step 2 Configuration #    Select time field as time filter for view index\n  Created\n  Edit data view #  The page lists all fields that match the index, and you can set the Format, Popularity, etc. of the fields.\n"});index.add({'id':33,'href':'/en/docs/latest/gateway/tutorial/index_diff/','title':"Document-Level Index Diff Between Two Elasticsearch Clusters",'section':"Tutorials",'content':"Document-Level Index Diff Between Two Elasticsearch Clusters #  INFINI Gateway is able to compare differences between two different indexes in the same or different clusters. In scenarios in which application dual writes, CCR, or other data replication solutions are used, differences can be periodically compared to ensure data consistency.\nFunction Demonstration #    How Is This Feature Configured? #  Setting a Target Cluster #  Modify the gateway.yml configuration file by setting two cluster resources source and target and adding the following configuration:\nelasticsearch: - name: source enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest - name: target enabled: true endpoint: http://localhost:9201 basic_auth: #used to discovery full cluster nodes, or check elasticsearch's health and versions username: test password: testtest Configuring a Contrast Task #  Add a service pipeline to handle the index document pulling and contrast of two clusters as follows:\npipeline: - name: index_diff_service auto_start: true keep_running: true processor: - dag: parallel: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 - dump_hash: #dump es2's doc indices: \u0026quot;medcl-test\u0026quot; scroll_time: \u0026quot;10m\u0026quot; batch_size: 10000 slice_size: 5 elasticsearch: \u0026quot;target\u0026quot; output_queue: \u0026quot;target_docs\u0026quot; end: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' In the above configuration, dump_hash is concurrently used to pull the medcl-a index of the source cluster and fetch the medcl-b index of the target cluster, and output results to terminals in text form.\nOutputting Results to Elasticsearch #  If there are many difference results, you can save them to the Elasticsearch cluster, set the text_report parameter of the above index_diff processing unit to false, and add the following configuration:\npipeline: - name: diff_result_ingest auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;diff_result\u0026quot; elasticsearch: \u0026quot;source\u0026quot; input_queue: \u0026quot;diff_result\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB Finally, import the dashboard to Kibana to achieve the following effect:\n"});index.add({'id':34,'href':'/en/docs/latest/gateway/user-cases/stories/indexing_speedup_for_big_index_rebuild/','title':"How an Insurance Group Improved the Indexing Speed by 200x Times",'section':"User Cases",'content':"How an Insurance Group Improved the Indexing Speed by 200x Times #  Challenges #  A large insurance group places common database fields in Elasticsearch to improve the query performance for its policy query service. The cluster is deployed on 14 physical machines, with 4 Elasticsearch instances deployed on each physical machine. The whole cluster has more than 9 billion pieces of data. The storage size of index primary shards is close to 5 TB, and about 600 million pieces of incremental data are updated every day. Due to the service particularity, all the service data across the country is stored in one index, resulting in up to 210 shards in the single index. The bulk rebuilding task is executed in parallel by Spark. The average write speed is about 2,000–3,000 pieces per second. One incremental rebuilding operation may take 2–3 days. Service data updating causes a large delay and the lengthy rebuilding also affects service access during normal time periods. The technical team had tried hard to optimize Elasticsearch and also the Spark write end for several rounds, but did not get any progress in the indexing speed improvement.\nScenario #  The analysis shows that the cluster performance is good. However, after write requests in a single batch are received by Elasticsearch, they need to be encapsulated and forwarded according to the node where the primary shard is located. There are too many service index shards and each data node eventually gets a very small number of request documents. One bulk write request of the client is divided into hundreds of small bulk requests. According to the short board theory of the barrel, the processing speed of the slowest node slows down the whole bulk write operation. INFINI Gateway knows where a document should go to.\nINFINI Gateway is capable of splitting and merging requests in advance. It splits and merges requests in advance, sends the requests to local queues based on the target node, and then writes the requests to the target Elasticsearch cluster through the queue consumption program to convert random bulk requests into sequential requests that are precisely delivered. See the figure below.\nAfter receiving a request from Spark, INFINI Gateway first stores the request on the local disk to prevent data loss. Meanwhile, INFINI Gateway can locally calculate the routing information of each document and the target data nodes. The new data writing architecture is shown in the figure below.\nAfter INFINI Gateway is used to receive write requests from Spark, the write throughput of the entire cluster is significantly improved. Spark accomplishes the data writing task in less than 15 minutes, and it takes only 20 minutes for the gateway to receive requests and write them into Elasticsearch. The CPU resources of the server are fully utilized and all the CPU resources of each node are used.\nUser Benefits #   The Indexing Speed Is Improved by 20,000%\n After INFINI Gateway is used as the intermediate acceleration layer, the index rebuilding cycle of the group\u0026rsquo;s policy service is reduced from 2–3 days to about 20 minutes, the 600 million pieces of daily incremental data can also be rebuilt very quickly, and the peak index write QPS can exceed 300,000. In a word, INFINI Gateway greatly shortens the index rebuilding cycle, reduces data latency, enhances the consistency of online data, and ensures the normal use of the query service.\n"});index.add({'id':35,'href':'/en/docs/latest/gateway/references/modules/force_merge/','title':"Index Segment Merging",'section':"Functional Component",'content':"Active Merging of Index Segments #  INFINI Gateway has an index segment merging service, which can actively merge index segment files to improve query speed. The index segment merging service supports sequential processing of multiple indexes and tracks the status of the merging task, thereby preventing cluster slowdown caused by concurrent operations of massive index segment merging tasks.\nEnabling the Service #  Modify the gateway.yml configuration file by adding the following configuration:\nforce_merge: enabled: false elasticsearch: dev min_num_segments: 20 max_num_segments: 1 indices: - index_name The parameters are described as follows:\n   Name Type Description     enabled bool Whether the module is enabled, which is set to false by default.   elasticsearch string ID of an Elasticsearch cluster, on which index segment merging is performed   min_num_segments int Minimum number of shards in an index for active shard merging. The value is based on indexes.   max_num_segments int The maximum number of segment files that can be generated after segment files in a shard are merged   indices array List of indexes that need shard merging   discovery object Auto-discovery of index-related settings   discovery.min_idle_time string Minimum time span for judging whether segment merging conditions are met. The default value is 1d.   discovery.interval string Interval for detecting whether segment merging is required   discovery.rules array Index matching rules used in automatic index detection   discovery.rules.index_pattern string Pattern of indexes that need index segment file merging   discovery.rules.timestamp_fields array List of fields representing the index timestamp    "});index.add({'id':36,'href':'/en/docs/latest/gateway/user-cases/stories/a_cross_region_cluster_access_locality/','title':"Nearest Cluster Access Across Two Cloud Providers",'section':"User Cases",'content':"Nearest Cluster Access Across Two Cloud Providers #  Service Requirements #  To ensure the high availability of the Elasticsearch service, Zuoyebang deploys a single Elasticsearch cluster on both Baidu Cloud and Huawei Cloud and requires that service requests be sent to the nearest cloud first.\nDeployment of a Single Elasticsearch Cluster on Dual Clouds #  The Elasticsearch cluster uses an architecture with master nodes separated from data nodes. Currently, the main cloud is used to accommodate two master nodes and the other cloud is used to accommodate another master node. The main consideration is that infrastructure failures are mostly dedicated line failures, and the overall breakdown of a provider\u0026rsquo;s cloud rarely occurs. Therefore, the main cloud is configured. When a dedicated line failure occurs, the Elasticsearch cluster on the main cloud is read/write and service traffic can be switched to the main cloud.\nThe configuration is as follows:\nFirst, complete the following settings on the master nodes:\ncluster.routing.allocation.awareness.attributes: zone_id cluster.routing.allocation.awareness.force.zone_id.values: zone_baidu,zone_huawei Then, perform the following settings on data nodes on Baidu Cloud:\nnode.attr.zone_id: zone_baidu Perform the following settings on data nodes on Huawei Cloud:\nnode.attr.zone_id: zone_huawei Indexes are created using one copy, which can ensure that the same copy of data exists on Baidu Cloud and Huawei Cloud.\nThe service access mode is shown as follows:\n Baidu Cloud service -\u0026gt; Baidu lb -\u0026gt; INFINI Gateway (Baidu Cloud) -\u0026gt; Elasticsearch (data nodes on Baidu Cloud) Huawei Cloud service -\u0026gt; Huawei lb -\u0026gt; INFINI Gateway (Huawei Cloud) -\u0026gt; Elasticsearch (data nodes on Huawei Cloud)  Configuring INFINI Gateway #  Elasticsearch uses the Preference parameter to set the request access priority. Set the default Preference parameter for requests on INFINI Gateway inside the two clouds so that requests inside each cloud are sent to data nodes in the local cloud first, thereby implementing nearest access of requests.\nThe specific configuration on INFINI Gateway inside Baidu Cloud is as follows (the configuration on INFINI Gateway inside Huawei Cloud is basically the same and is not provided here):\npath.data: data path.logs: log entry: - name: es-test enabled: true router: default network: binding: 0.0.0.0:9200 reuse_port: true router: - name: default default_flow: es-test flow: - name: es-test filter: - set_request_query_args: args: - preference -\u0026gt; _prefer_nodes:node-id-of-data-baidu01,node-id-of-data-baidu02 #Set _prefer_nodes of Preference to all Baidu data nodes(use `node_id` of these nodes) so that Baidu Cloud service accesses the nodes of Baidu Cloud first, thereby avoiding cross-cloud access to the maximum extent and enabling the service to run more smoothly. when: contains: _ctx.request.path: /_search - elasticsearch: elasticsearch: default refresh: enabled: true interval: 10s roles: include: - data #Set it to data so that requests are sent only to the data node. tags: include: - zone_id: zone_baidu #Requests are forwarded only to nodes in Baidu Cloud. elasticsearch: - name: default enabled: true endpoint: http://10.10.10.10:9200 allow_access_when_master_not_found: true discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: elastic Summary and Benefits #  Retrospect of Failures Before INFINI Gateway Is Introduced #  When Baidu Cloud service accesses the Elasticsearch cluster, it pulls daily incremental data from the Hive cluster and synchronizes it to Elasticsearch. Some tasks may fail and data is synchronized again. As a result, some data is pulled from the Elasticsearch node inside Huawei Cloud to the Hive cluster of Baidu Cloud. The huge amount of data triggers an alarm about cross-cloud dedicated line traffic monitoring. Online services, MySQL, Redis, and Elasticsearch use the same dedicated line. The impact of the failures is huge. The temporary solution is to add the Preference parameter to the service modification statement so that the services only pull local cloud data, reducing the occupancy of the dedicated line. The service transformation and maintenance costs are high. In addition, DBA has worries that there are omissions in service transformation, the Preference parameter is ignored for new services, and later adjustment costs are high. These are always risk points.\nBenefits of INFINI Gateway #  After INFINI Gateway is added to the original architecture, services can preferentially access the local cloud with the service code not modified. In this way, CPU resources of the server are fully utilized and all CPU resources of each node are used.\n Author: Zhao Qing, former DBA of NetEase, mainly involved in the O\u0026amp;M of Oracle, MySQL, Redis, Elasticsearch, Tidb, OB, and other components, as well as O\u0026amp;M automation, platform-based application, and intelligence. Now he is working in Zuoyebang.\n "});index.add({'id':37,'href':'/en/docs/latest/gateway/references/router/','title':"Service Router",'section':"Reference",'content':"Service Router #  INFINI Gateway judges the flow direction based on routers. A typical example of router configuration is as follows:\nrouter: - name: my_router default_flow: default_flow tracing_flow: request_logging rules: - method: - PUT - POST pattern: - \u0026quot;/_bulk\u0026quot; - \u0026quot;/{index_name}/_bulk\u0026quot; flow: - bulk_process_flow Router involves several important terms:\n Flow: Handling flow of a request. Flows can be defined in three places in a router. default_flow: Default handling flow, which is the main flow of service handling. Request forwarding, filtering, and caching are performed in this flow. tracing_flow: Flow used to track the request status. It is independent of the default_flow. This flow is used to log requests and collect statistics. rules: Requests are distributed to specific handling flows according to matching rules. Regular expressions can be used to match the methods and paths of requests.  Parameter Description #     Name Type Description     name string Route name   default_flow string Name of the default request handling flow   tracing_flow string Name of the flow used to trace a request   rules array List of routing rules to be applied in the array sequence   rules.method string Method type of a request. The GET, HEAD, POST, PUT, PATCH, DELETE, CONNECT, OPTIONS, and TRACE types are supported and * indicates any type.   rules.pattern string URL path matching rule of a request. Patterns are supported and overlapping matches are not allowed.   rules.flow string Flow to be executed after rule matching. Multiple flows can be combined and they are executed sequentially.   permitted_client_ip_list string array Specified IP list will be allowed access to the gateway service, in order to permit specify user or application.   denied_client_ip_list string array Specified IP list will not allowed access to the gateway service, in order to prevent specify user or application.    Pattern Syntax #     Syntax Description Example     {Variable name} Variable with a name /{name}   {Variable name:regexp} Restricts the matching rule of the variable by using a regular expression. /{name:[a-zA-Z]}   {Variable name:*} Any path after matching. It can be applied only to the end of a pattern. /{any:*}    Examples:\nPattern: /user/{user} /user/gordon match /user/you match /user/gordon/profile no match /user/ no match Pattern with suffix: /user/{user}_admin /user/gordon_admin match /user/you_admin match /user/you no match /user/gordon/profile no match /user/gordon_admin/profile no match /user/ no match Pattern: /src/{filepath:*} /src/ match /src/somefile.go match /src/subdir/somefile.go match Notes:\n A pattern must begin with /. Any match is only used as the last rule.  Permit IPs #  If you only want some specific IP to access the gateway, you can configure it in the route section, and the request will be directly rejected during the link establishment process. In the following example, 133.37.55.22 will be allowed to access the gateway, and the rest of the IP will be denied.\nrouter: - name: my_router default_flow: async_bulk ip_access_control: enabled: true client_ip: permitted: - 133.37.55.22 Block IPs #  If you want to block specify know ip to access gateway, you can configure the ip list in the router section, the requests will be denied during the TCP connection establishment。 for below example，the ip 133.37.55.22 will be blocked:\nrouter: - name: my_router default_flow: async_bulk ip_access_control: enabled: true client_ip: denied: - 133.37.55.22 "});index.add({'id':38,'href':'/en/docs/latest/console/reference/agent/docker/','title':"Container Deployment",'section':"Agent",'content':"Container Deployment #  INFINI Agent supports container deployment.\nDownload Image #  The images of INFINI Agent are published at the official repository of Docker. The URL is as follows: https://hub.docker.com/r/infinilabs/agent\nRun the following command:\ndocker pull infinilabs/agent:latest Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Agent is very small, with a size less than 25 MB. So, the downloading is very fast.\n✗ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/agent latest c7bd9ad063d9 4 days ago 13.8MB Configuration #  Create a configuration file agent.yml to perform basic configuration as follows:\napi: enabled: true network: binding: 0.0.0.0:8080 metrics: enabled: true queue: metrics network: enabled: true summary: true details: true memory: metrics: - swap - memory disk: metrics: - ioqs - usage cpu: metrics: - idle - system - user - iowait - load elasticsearch: enabled: true agent_mode: true node_stats: true index_stats: true cluster_stats: true elasticsearch: - name: default enabled: true endpoint: http://192.168.3.4:9200 monitored: false discovery: enabled: true pipeline: - name: metrics_ingest auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;.infini_metrics\u0026quot; elasticsearch: \u0026quot;default\u0026quot; input_queue: \u0026quot;metrics\u0026quot; output_queue: name: \u0026quot;metrics_requests\u0026quot; label: tag: \u0026quot;metrics\u0026quot; worker_size: 1 bulk_size_in_mb: 10 - name: consume-metrics_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ \u0026quot;default\u0026quot; ] agent: major_ip_pattern: \u0026quot;192.*\u0026quot; labels: env: dev tags: - linux - x86 - es7 - v7.5 path.data: data path.logs: log agent.manager.endpoint: http://192.168.3.4:9000 Note: In the above configuration, replace the Elasticsearch configuration with the actual server connection address and authentication information.\nStarting #  Run the following command:\ndocker run -p 8080:8080 -v=`pwd`/agent.yml:/agent.yml infinilabs/agent:latest Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-agent: image: infinilabs/agent:latest ports: - 8080:8080 container_name: \u0026quot;infini-agent\u0026quot; volumes: - ./agent.yml:/agent.yml volumes: dist: Run the following command to start INFINI Agent.\n➜ docker-compose up Recreating infini-agent ... done Attaching to infini-agent infini-agent | _ ___ __ __ _____ infini-agent | /_\\ / _ \\ /__\\/\\ \\ \\/__ \\ infini-agent | //_\\\\ / /_\\//_\\ / \\/ / / /\\/ infini-agent | / _ \\/ /_\\\\//__/ /\\ / / / infini-agent | \\_/ \\_/\\____/\\__/\\_\\ \\/ \\/ infini-agent | infini-agent | [AGENT] A light-weight, powerful and high-performance elasticsearch agent. infini-agent | [AGENT] 0.1.0_SNAPSHOT#15, 2022-08-26 15:05:43, 2025-12-31 10:10:10, 164bd8a0d74cfd0ba5607352e125d72b46a1079e infini-agent | [08-31 09:11:45] [INF] [app.go:164] initializing agent. infini-agent | [08-31 09:11:45] [INF] [app.go:165] using config: /agent.yml. infini-agent | [08-31 09:11:45] [INF] [instance.go:72] workspace: /data/agent/nodes/cc7ibke5epac7314bf9g infini-agent | [08-31 09:11:45] [INF] [metrics.go:63] ip:172.18.0.2, host:bd9f43490911, labels:, tags: infini-agent | [08-31 09:11:45] [INF] [api.go:261] api listen at: http://0.0.0.0:8080 infini-agent | [08-31 09:11:45] [INF] [actions.go:367] elasticsearch [default] is available infini-agent | [08-31 09:11:45] [INF] [module.go:116] all modules are started infini-agent | [08-31 09:11:45] [INF] [manage.go:180] register agent to console infini-agent | [08-31 09:11:45] [INF] [app.go:334] agent is up and running now. "});index.add({'id':39,'href':'/en/docs/latest/gateway/getting-started/docker/','title':"Container Deployment",'section':"Quick Start",'content':"Container Deployment #  INFINI Gateway supports container deployment.\nInstallation Demo #    Downloading an Image #  The images of INFINI Gateway are published at the official repository of Docker. The URL is as follows:\n https://hub.docker.com/r/infinilabs/gateway\nUse the following command to obtain the latest container image:\ndocker pull infinilabs/gateway:latest Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Gateway is very small, with a size less than 25 MB. So, the downloading is very fast.\n✗ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/gateway latest fdae74b64e1a 47 minutes ago 23.5MB Creating Configuration #  Create a configuration file gateway.yml to perform basic configuration as follows:\npath.data: data path.logs: log entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 200000 network: binding: 0.0.0.0:8000 flow: - name: simple_flow filter: - elasticsearch: elasticsearch: dev router: - name: my_router default_flow: simple_flow elasticsearch: - name: dev enabled: true endpoint: http://localhost:9200 basic_auth: username: test password: testtest Note: In the above configuration, replace the Elasticsearch configuration with the actual server connection address and authentication information.\nStarting the Gateway #  Use the following command to start the INFINI Gateway container:\ndocker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest Verifying the Gateway #  If the gateway runs properly, the following information is displayed:\n➜ /tmp docker run -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, b61758c, Mon Dec 28 14:32:02 2020 +0800, medcl, no panic by default [12-30 05:26:41] [INF] [instance.go:24] workspace: data/gateway/nodes/0 [12-30 05:26:41] [INF] [runner.go:59] pipeline: primary started with 1 instances [12-30 05:26:41] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 [12-30 05:26:41] [INF] [app.go:247] gateway now started. [12-30 05:26:45] [INF] [reverseproxy.go:196] elasticsearch [prod] endpoints: [] =\u0026gt; [192.168.3.201:9200] If you want the container to run in the background, append the parameter -d as follows:\ndocker run -d -p 2900:2900 -p 8000:8000 -v=`pwd`/gateway.yml:/gateway.yml infinilabs/gateway:latest Access the URL http://localhost:8000/ from the CLI or browser. The Elasticsearch can be accessed normally. See the following information.\n➜ /tmp curl -v http://localhost:8000/ * Trying ::1... * TCP_NODELAY set * Connected to localhost (::1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Wed, 30 Dec 2020 05:12:39 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.201:9200 \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node1\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } * Connection #0 to host localhost left intact * Closing connection 0 Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-gateway: image: infinilabs/gateway:latest ports: - 2900:2900 - 8000:8000 container_name: \u0026quot;infini-gateway\u0026quot; volumes: - ../gateway.yml:/gateway.yml volumes: dist: In the directory where the configuration file resides, run the following command to start INFINI Gateway.\n➜ docker-compose up Starting infini-gateway ... done Attaching to infini-gateway infini-gateway | ___ _ _____ __ __ __ _ infini-gateway | / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ infini-gateway | / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ infini-gateway | / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ infini-gateway | \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ infini-gateway | infini-gateway | [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. infini-gateway | [GATEWAY] 1.0.0_SNAPSHOT, b61758c, Mon Dec 28 14:32:02 2020 +0800, medcl, no panic by default infini-gateway | [12-30 13:24:16] [INF] [instance.go:24] workspace: data/gateway/nodes/0 infini-gateway | [12-30 13:24:16] [INF] [api.go:244] api server listen at: http://0.0.0.0:2900 infini-gateway | [12-30 13:24:16] [INF] [runner.go:59] pipeline: primary started with 1 instances infini-gateway | [12-30 13:24:16] [INF] [entry.go:257] entry [es_gateway] listen at: http://0.0.0.0:8000 infini-gateway | [12-30 13:24:16] [INF] [app.go:247] gateway now started. "});index.add({'id':40,'href':'/en/docs/latest/console/getting-started/docker/','title':"Docker",'section':"Quick Start",'content':"Container Deployment #  INFINI Console supports container deployment.\nDownloading an Image #  The images of INFINI Console are published at the official repository of Docker. The URL is as follows:\n https://hub.docker.com/r/infinilabs/console\nUse the following command to obtain the latest container image:\ndocker pull infinilabs/console:latest Verifying the Image #  After downloading the image locally, you will notice that the container image of INFINI Console is very small, with a size less than 30 MB. So, the downloading is very fast.\n✗ docker images REPOSITORY TAG IMAGE ID CREATED SIZE infinilabs/console latest 8c27cd334e4c 47 minutes ago 26.4MB Create configuration #  Now you need to create a configuration file console.yml for basic configuration, as follows:\n# for this System Cluster, please use Elasticsearch v7.3+ elasticsearch: - name: default enabled: true monitored: false endpoint: http://192.168.3.188:9299 basic_auth: username: elastic password: ZBdkVQUUdF1Sir4X4BGB discovery: enabled: true web: enabled: true embedding_api: true auth: enabled: true ui: enabled: true path: .public vfs: true local: true network: binding: 0.0.0.0:9000 skip_occupied_port: true gzip: enabled: true elastic: elasticsearch: default enabled: true remote_configs: true health_check: enabled: true interval: 30s availability_check: enabled: true interval: 60s metadata_refresh: enabled: true interval: 30s cluster_settings_check: enabled: true interval: 20s store: enabled: false orm: enabled: true init_template: true template_name: \u0026quot;.infini\u0026quot; index_prefix: \u0026quot;.infini_\u0026quot; metrics: enabled: true major_ip_pattern: \u0026quot;192.*\u0026quot; queue: metrics elasticsearch: enabled: true cluster_stats: true node_stats: true index_stats: true pipeline: - name: indexing_merge auto_start: true keep_running: true processor: - indexing_merge: input_queue: \u0026quot;metrics\u0026quot; elasticsearch: \u0026quot;default\u0026quot; index_name: \u0026quot;.infini_metrics\u0026quot; output_queue: name: \u0026quot;metrics_requests\u0026quot; label: tag: \u0026quot;metrics\u0026quot; worker_size: 1 bulk_size_in_mb: 10 - name: consume-metrics_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ \u0026quot;default\u0026quot; ] - name: metadata_ingest auto_start: true keep_running: true processor: - metadata: bulk_size_in_mb: 10 bulk_max_docs_count: 5000 fetch_max_messages: 1000 elasticsearch: \u0026quot;default\u0026quot; queues: type: metadata category: elasticsearch consumer: group: metadata when: cluster_available: [ \u0026quot;default\u0026quot; ] - name: activity_ingest auto_start: true keep_running: true processor: - activity: bulk_size_in_mb: 10 bulk_max_docs_count: 5000 fetch_max_messages: 1000 elasticsearch: \u0026quot;default\u0026quot; queues: category: elasticsearch activity: true consumer: group: activity when: cluster_available: [ \u0026quot;default\u0026quot; ] Note: Please change the relevant configuration of Elasticsearch in the above configuration to the actual server connection address and authentication information, which requires version v7.3 or above.\nStarting the Console #  Use the following command to start the INFINI Console container:\ndocker run -p 9000:9000 -v=`pwd`/console.yml:/console.yml infinilabs/console:latest Docker Compose #  You can also use docker compose to manage container instances. Create one docker-compose.yml file as follows:\nversion: \u0026quot;3.5\u0026quot; services: infini-console: image: infinilabs/console:latest ports: - 9000:9000 container_name: \u0026quot;infini-console\u0026quot; volumes: - ../console.yml:/console.yml volumes: dist: In the directory where the configuration file resides, run the following command to start INFINI Console.\n➜ docker-compose up "});index.add({'id':41,'href':'/en/docs/latest/console/reference/system/security/role/','title':"Role",'section':"Security",'content':"Role Management #  Introduction #  Role Management includes CURD operations for role. And INFINI Console has a builtin role named Administrator, it has all privileges , includes Platform and Data. The data role can help us control privileges of elasticsearch, includes elasticsearch api privileges which can be configured in the file config/permission.json of your setup path.\nCreate Platform Role #   Input role name, role name should be unique. Select feature privileges, can not be empty. Input a description if needed  All privilege represents both read and write permission, Read privilege represents only read permission, and None privilege represents no permission\nCreate Data Role #   Input role name, role name should be unique. Select one or more cluster, * represents all clusters. Config cluster api privileges, * represents all privileges. Config index api privileges, * represents all privileges. Input a description if needed  Search Role #  Input a keyword and click the search button to query roles.\nUpdate Platform Role #  Modify the role as needed, and then click the Save button to submit.\nUpdate Data Role #  Modify the role as needed, and then click the Save button to submit.\n"});index.add({'id':42,'href':'/en/docs/latest/console/reference/system/security/user/','title':"User",'section':"Security",'content':"User Management #  Introduction #  User Management includes CURD operations and reset password for user.\nCreate User #   User name is required, and it should be unique. Nick name, phone, email, is optional. Select one or more role. Tags is optional, it helps you group users.  Search User #  Input keywords and click the search button to query users.\nUpdate User #  Modify as needed, and then click the Save button to submit.\nReset User password #  Input the new password and then click the save button to reset the password.\n"});index.add({'id':43,'href':'/en/docs/latest/console/screenshots/','title':"Screenshots",'section':"INFINI Console",'content':"Screenshots #  Cluster Overview #  Cluster Activities #  Alerting #  Cluster Monitoring #  Data Management #  Developer Tool #  Cluster Management #  Security #  "});index.add({'id':44,'href':'/en/docs/latest/console/reference/data/discover/','title':"Discover",'section':"Data",'content':"Discover #  Introduction #  In Discover, you can search and query the data under the index or view according to conditions such as time and fields. The data display methods include regular mode and Insight mode.\nSearch toolbar #  Index (View) #  Search Statement #  Time Range #  Field Filter #  Save Search #  Saved Search List #  Insight Mode Switch #  Insight Configuration #  Normal Mode #  Flexibly add fields to data content with multi-function charts in normal mode\nEdit, delete, etc. document data\nInsight Mode #  In Insight mode, visual charts will be pushed to display data according to data characteristics\nCharts can be added via push list\nEdit(Remove) chart widget\n"});index.add({'id':45,'href':'/en/docs/latest/console/reference/system/security/','title':"Security",'section':"System",'content':"Security #  Introduction #  INFINI Console Security provides the following security benefits and capabilities:\n Grant different platform privileges to different users Grant different elasticsearch cluster privileges to users, include indices level and API level  INFINI Console Security has two role type\n Platform Role，used to control platform privileges Data Role, used to control data privileges  INFINI Console Security is enabled by default，and we can disable it by configure section web\u0026gt;auth\u0026gt;enabled to false in config file cosnole.yml as follows：\nweb: enabled: true embedding_api: true auth: enabled: false ui: enabled: true path: .public vfs: true local: true network: binding: 0.0.0.0:9000 skip_occupied_port: true gzip: enabled: true  Password is required to login INFINI Console when Security is enabled 。INFINI Console has a builtin account with both username and password are admin 。\n "});index.add({'id':46,'href':'/en/docs/latest/gateway/getting-started/optimization/','title':"System Optimization",'section':"Quick Start",'content':"System Optimization #  The operating system of the server where INFINI Gateway is installed needs to be optimized to ensure that INFINI Gateway runs in the best possible state. The following uses Linux as an example.\nSystem Parameters #  sudo tee /etc/security/limits.d/21-infini.conf \u0026lt;\u0026lt;-'EOF' * soft nofile 1048576 * hard nofile 1048576 * soft memlock unlimited * hard memlock unlimited root soft nofile 1048576 root hard nofile 1048576 root soft memlock unlimited root hard memlock unlimited EOF Kernel Optimization #  cat \u0026lt;\u0026lt; SETTINGS | sudo tee /etc/sysctl.d/70-infini.conf fs.file-max=10485760 fs.nr_open=10485760 vm.max_map_count=262144 net.core.somaxconn=65535 net.core.netdev_max_backlog=65535 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.core.rmem_max=4194304 net.core.wmem_max=4194304 net.ipv4.ip_forward = 1 net.ipv4.ip_nonlocal_bind=1 net.ipv4.ip_local_port_range = 1024 65535 net.ipv4.conf.default.accept_redirects = 0 net.ipv4.conf.default.rp_filter = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 net.ipv4.tcp_tw_reuse=1 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_max_tw_buckets = 300000 net.ipv4.tcp_timestamps=1 net.ipv4.tcp_syncookies=1 net.ipv4.tcp_max_syn_backlog=65535 net.ipv4.tcp_synack_retries=0 net.ipv4.tcp_keepalive_intvl = 30 net.ipv4.tcp_keepalive_time = 900 net.ipv4.tcp_keepalive_probes = 3 net.ipv4.tcp_fin_timeout = 10 net.ipv4.tcp_max_orphans = 131072 net.ipv4.tcp_rmem = 4096 4096 16777216 net.ipv4.tcp_wmem = 4096 4096 16777216 net.ipv4.tcp_mem = 786432 3145728 4194304 SETTINGS Run the following command to check whether configuration parameters are valid.\nsysctl -p Restart the operating system to make the configuration take effect.\n"});index.add({'id':47,'href':'/en/docs/latest/console/user-cases/','title':"User Cases",'section':"INFINI Console",'content':"User Cases #  Who Is Using ? #  If you are using INFINI Console and feel it good enough to tell others you are using it, please leave a message in Github Discussion to let us know. Thank you for your support and encouragement.\nUsers in China #            "});index.add({'id':48,'href':'/en/docs/latest/gateway/user-cases/','title':"User Cases",'section':"INFINI Gateway",'content':"User Cases #  Who Is Using INFINI Gateway? #  If you are using INFINI Gateway and feel it good enough to tell others you are using it, please leave a message in Github Discussion to let us know. Thank you for your support and encouragement.\nCustomers in China #                              "});index.add({'id':49,'href':'/en/docs/latest/gateway/references/flow/','title':"Handling Flow",'section':"Reference",'content':"Handling Flow #  Flow Definition #  Requests received by each gateway are handled through a series of processes and then results are returned to the client. A process is called a flow in INFINI Gateway. See the following example.\nflow: - name: hello_world filter: - echo: message: \u0026quot;hello gateway\\n\u0026quot; repeat: 1 - name: not_found filter: - echo: message: '404 not found\\n' repeat: 1 The above example defines two flows: hello_world and not_found. Each flow uses a filter named echo to output a string. A series of filters can be defined in each flow and they are executed in the defined sequence.\nSyntax Description #  INFINI Gateway defines a flow in the stipulated format and supports flexible conditional parameters for logical judgment. The specific format is defined as follows:\nflow: - name: \u0026lt;flow_name\u0026gt; filter: - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: when: \u0026lt;condition\u0026gt; \u0026lt;parameters\u0026gt; ... In the format defined above, filter_name indicates the name of a filter, which is used to execute a specific task. condition below when is used to define specific conditional parameters for executing the task, and the filter task is skipped when the conditions are not met. In parameters, parameters related to the filter are set, and the parameters are separated by the line feed character.\nConditional Judgment #  Complex logical judgments can be defined in a flow of INFINI Gateway so that a filter can be executed only when certain conditions are met. See the following example.\nfilter: - if: \u0026lt;condition\u0026gt; then: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... else: - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; - \u0026lt;filter_name\u0026gt;: \u0026lt;parameters\u0026gt; ... Parameter Description #     Name Type Description     then array A series of filters to be executed only when conditions defined in condition are met.   else array A set of filters to be executed only when the conditions are not met. You do not have to set it.    You can use if to make conditional judgment and logical selection in the case of multiple filters and use when to determine whether to execute a single filter.\nCondition Type #  For various condition defined in a flow, you can use the current request context to judge whether a specific condition is met so as to achieve logical processing. The conditions support the combination of Boolean expressions (AND, NOT, and OR). The complete list of condition types is as follows:\n equals contains prefix suffix regexp range network has_fields in queue_has_lag consumer_has_lag cluster_available or and not  equals #  The equals condition is used to judge whether the content of a field is the specified value. It is used for the exact match of characters and digits.\nThe following example determines whether the request method is of the GET type and _ctx is a specific keyword for accessing the request context:\nequals: _ctx.request.method: GET contains #  The contains condition is used to judge whether the content of a field contains a specific character value. Only support string field.\nThe following example judges whether the returned response body contains an error keyword:\ncontains: _ctx.response.body: \u0026quot;error\u0026quot; prefix #  Use the prefix condition to determine whether the contents of a field begin with a specific character value, Only support string field.\nThe following example determines that the returned request path starts with a specific index name:\nprefix: _ctx.request.path: \u0026quot;/filebeat\u0026quot; suffix #  Use the suffix condition to determine whether the content of a field ends with a specific character value. Only support string field.\nThe following example determines whether the request is a search request:\nsuffix: _ctx.request.path: \u0026quot;/_search\u0026quot; regexp #  The regexp condition is used to judge whether the content of a field meets the matching rules of a regular expression. Only support string field.\nThe following example judges whether the request URI is a query request:\nregexp: _ctx.request.uri: \u0026quot;.*/_search\u0026quot; range #  The range condition is used to judge whether the value of a field meets a specific range. It supports the lt, lte, gt, and gte types and only numeric fields are supported.\nThe following example judges the range of the status code:\nrange: _ctx.response.code: gte: 400 The following combination example judges the range of the response byte size:\nrange: _ctx.request.body_length.gte: 100 _ctx.request.body_length.lt: 5000 network #  If the value of a field is an IP address, you can use the network condition to judge whether the field meets a specific network range, whether it supports standard IPv4 or IPv6, whether it supports the classless inter-domain routing (CIDR) expression, or whether it uses an alias in the following range:\n   Name Description     loopback Matches the local loopback network address. Range: 127.0.0.0/8 or ::1/128.   unicast Matches global unicast addresses defined in RFC 1122, RFC 4632, and RFC 4291, except the IPv4 broadcast address (255.255.255.255) but including private address ranges.   multicast Matches the broadcast address.   interface_local_multicast Matches the local multicast address of an IPv6 interface.   link_local_unicast Matches the link-local unicast address.   link_local_multicast Matches the link-local broadcast address.   private Matches the private address range defined in RFC 1918 (IPv4) and RFC 4193 (IPv6).   public Matches public addresses other than the local address, unspecified address, IPv4 broadcast address, link-local unicast address, link-local multicast address, interface local multicast address, or private address.   unspecified Matches an unspecified address (IPv4 address 0.0.0.0 or IPv6 address ::).    The following example matches the local network address:\nnetwork: _ctx.request.client_ip: private The following example specifies a subnet:\nnetwork: _ctx.request.client_ip: '192.168.3.0/24' An array is supported and it is judged that the condition is met when any value in the array is met.\nnetwork: _ctx.request.client_ip: ['192.168.3.0/24', '10.1.0.0/8', loopback] has_fields #  You can use the has_fields condition to judge whether a field exists. It supports the use of one or more character fields. See the following example:\nhas_fields: ['_ctx.request.user'] in #  You can use the in condition to judge whether a field has any value in a specified array. It supports a single field and the character and numeric types.\nThe following example judges the returned status code.\nin: _ctx.response.status: [ 403,404,200,201 ] queue_has_lag #  The queue_has_lag condition is used to judge whether one or more local disk queues are stacked with messages.\nqueue_has_lag: [ \u0026quot;prod\u0026quot;, \u0026quot;prod-500\u0026quot; ] If you want to set the depth of a queue to be greater than a specified depth, add \u0026gt;queue depth to the end of the queue name. See the following example:\nqueue_has_lag: [ \u0026quot;prod\u0026gt;10\u0026quot;, \u0026quot;prod-500\u0026gt;10\u0026quot; ] The above example shows that the condition is met only when the queue depth exceeds 10.\nconsumer_has_lag #  The consumer_has_lag condition is used to judge whether delay and message stacking occur in the consumer of a queue.\nconsumer_has_lag: queue: \u0026quot;primary-partial-success_bulk_requests\u0026quot; group: \u0026quot;my-group\u0026quot; name: \u0026quot;my-consumer-1\u0026quot; cluster_available #  The cluster_available condition is used to judge the service availability of one or more Elasticsearch clusters. See the following example:\ncluster_available: [\u0026quot;prod\u0026quot;] or #  The or condition is used to combine multiple optional conditions in the following format:\nor: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nor: - equals: _ctx.response.code: 304 - equals: _ctx.response.code: 404 and #  The and condition is used to combine multiple necessary conditions in the following format:\nand: - \u0026lt;condition1\u0026gt; - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; ... See the following example:\nand: - equals: _ctx.response.code: 200 - equals: _ctx.status: OK You can combine the and and or conditions flexibly. See the following example:\nor: - \u0026lt;condition1\u0026gt; - and: - \u0026lt;condition2\u0026gt; - \u0026lt;condition3\u0026gt; not #  If you want to negate a condition, use the not condition in the following format:\nnot: \u0026lt;condition\u0026gt; See the following example:\nnot: equals: _ctx.status: OK "});index.add({'id':50,'href':'/en/docs/latest/console/reference/system/security/settings/','title':"Settings",'section':"Security",'content':"Settings #  Disable built-in users #  When the security is turned on, start the console without specifying the user and password, the system will have a The default built-in user admin. The built-in user can be disabled after adding a new user with administrator privileges.\n Can\u0026rsquo;t disable yourself using built-in user  "});index.add({'id':51,'href':'/en/docs/latest/gateway/references/elasticsearch/','title':"Elasticsearch",'section':"Reference",'content':"Elasticsearch #  Defining a Resource #  INFINI Gateway supports multi-cluster access and different versions. Each cluster serves as one Elasticsearch back-end resource and can be subsequently used by INFINI Gateway in multiple locations. See the following example.\nelasticsearch: - name: local enabled: true endpoint: https://127.0.0.1:9200 - name: dev enabled: true endpoint: https://192.168.3.98:9200 basic_auth: username: elastic password: pass - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true interval: 10s basic_auth: username: elastic password: pass The above example defines a local development test cluster named local and a development cluster named dev. Authentication is enabled in the development cluster, in which corresponding usernames and passwords are also defined. In addition, one production cluster named prod is defined, and the auto node topology discovery and update of the cluster are enabled through the discovery parameter.\nParameter Description #     Name Type Description     name string Name of an Elasticsearch cluster   project string project name   location.provider string the service provider region info of the cluster   location.region string the region info of the cluster   location.dc string the data center info of the cluster   location.rack string the rack info of the cluster   labels map cluster labels   tags array cluster tags   enabled bool Whether the cluster is enabled   endpoint string Elasticsearch access address, for example, http://localhost:9200   endpoints array List of Elasticsearch access addresses. Multiple entry addresses are supported for redundancy.   schema string Protocol type: http or https   host string Elasticsearch host, in the format of localhost:9200. Either the host or endpoint configuration mode can be used.   hosts array Elasticsearch host list. Multiple entry addresses are supported for redundancy.   request_timeout int Request timeout duration, in seconds, default 30   request_compress bool Whether to enable Gzip compression   basic_auth object Authentication information   basic_auth.username string Username   basic_auth.password string Password   discovery object Cluster discovery settings   discovery.enabled bool Whether to enable cluster topology discovery   discovery.refresh object Cluster topology update settings   discovery.refresh.enabled bool Whether to enable auto cluster topology update   discovery.refresh.interval string Interval of auto cluster topology update   traffic_control object Node-level overall traffic control of the cluster   traffic_control.max_bytes_per_node int Maximum allowable number of request bytes per second   traffic_control.max_qps_per_node int Maximum allowable number of requests per second, regardless of read or write requests   traffic_control.max_connection_per_node int Maximum allowable number of connections per node   traffic_control.max_wait_time_in_ms int In case of throttled, the maximum allowable waiting time in ms, the default is 10000   allow_access_when_master_not_found bool Still allow access to visit this elasticsearch when it is in error master_not_discovered_exception , default value is false    "});index.add({'id':52,'href':'/en/docs/latest/gateway/references/context/','title':"Request Context",'section':"Reference",'content':"Request Context #  What Is Context #  Context is the entry for INFINI Gateway to access relevant information in the current running environment, such as the request source and configuration. You can use the _ctx keyword to access relevant fields, for example, _ctx.request.uri, which indicates the requested URL.\nEmbedded Request Context #  The embedded _ctx context objects of an HTTP request mainly include the following:\n   Name Type Description     id uint64 Unique ID of the request   tls bool Whether the request is a TLS request   remote_ip string Source IP of the client   remote_addr string Source IP address of the client, including port   local_ip string Gateway local IP address   local_addr string Gateway local IP address, including port   elapsed int64 Time that the request has been executed (ms)   request.* object Request description   response.* object Response description    request #  The request object has the following attributes:\n   Name Type Description     to_string string Complete HTTP request in text form   host string Accessed destination host name/domain name   method string Request type   uri string Complete URL of request   path string Request path   query_args map URL request parameter   username string Name of the user who initiates the request   password string Password of the user   header map Header parameter   body string Request body   body_json object JSON request body object   body_length int Request body length    If the request body data submitted by the client is in JSON format, you can use body_json to access the data. See the following example.\ncurl -u tesla:password -XGET \u0026quot;http://localhost:8000/medcl/_search?pretty\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;:{ \u0026quot;bool\u0026quot;:{ \u0026quot;must\u0026quot;:[{\u0026quot;match\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;A\u0026quot;}},{\u0026quot;match\u0026quot;:{\u0026quot;age\u0026quot;:18}}] }\t}, \u0026quot;size\u0026quot;:900, \u0026quot;aggs\u0026quot;: { \u0026quot;total_num\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;name1\u0026quot;, \u0026quot;size\u0026quot;: 1000000 } } } }' In JSON data, . is used to identify the path. If the data is an array, you can use [Subscript] to access a specified element, for example, you can use a dump filter for debugging as follows:\n - name: cache_first filter: - dump: context: - _ctx.request.body_json.size - _ctx.request.body_json.aggs.total_num.terms.field - _ctx.request.body_json.query.bool.must.[1].match.age The output is as follows:\n_ctx.request.body_json.size : 900 _ctx.request.body_json.aggs.total_num.terms.field : name1 _ctx.request.body_json.query.bool.must.[1].match.age : 18 response #  The response object has the following attributes:\n   Name Type Description     to_string string Complete HTTP response in text form   status int Request status code   header map Header parameter   content_type string Response body type   body string Response body   body_length int Response body length    "});index.add({'id':53,'href':'/en/docs/latest/gateway/getting-started/benchmark/','title':"Benchmark Testing",'section':"Quick Start",'content':"Benchmark Testing #  You are advised to use the Elasticsearch-dedicated benchmark tool Loadgen to test the gateway performance.\nHighlights of Loadgen:\n Robust performance Lightweight and dependency-free Random selection of template-based parameters High concurrency Balanced traffic control at the benchmark end   Download URL: http://release.infinilabs.com/loadgen/\n Loadgen #  Loadgen is easy to use. After the tool is downloaded and decompressed, two files are obtained: one executable program and one configuration file loadgen.yml. An example of the configuration file is as follows:\nvariables: - name: ip type: file path: test/ip.txt - name: user type: file path: test/user.txt - name: id type: sequence - name: uuid type: uuid - name: now_local type: now_local - name: now_utc type: now_utc - name: now_unix type: now_unix requests: - request: method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search body: '{ \u0026quot;query\u0026quot;: {\u0026quot;match\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;$[[user]]\u0026quot; }}}' Use of Variables #  In the above configuration, variables is used to define variable parameters and variables are identified by name. In a constructed request, $[[Variable name]] can be used to access the value of the variable. Supported variable types are as follows:\n   Type Description     file External variable parameter of the file type   sequence Variable of the auto incremental numeric type   range Variable of the range numbers, support parameters from and to to define the range   uuid Variable of the UUID character type   now_local Current time and local time zone   now_utc Current time and UTC time zone   now_unix Current time and Unix timestamp   now_with_format Current time，support parameter format to customize the output format， eg: 2006-01-02T15:04:05-0700    Variable parameters of the file type are loaded from an external text file. One variable parameter occupies one line. When one variable of the file type is accessed, one variable value is taken randomly. An example of the variable format is as follows:\n➜ loadgen git:(master) ✗ cat test/user.txt medcl elastic Tips about how to generate a random string of fixed length, such as 1024 per line:\nLC_CTYPE=C tr -dc A-Za-z0-9_\\!\\@\\#\\$\\%\\^\\\u0026amp;\\*\\(\\)-+= \u0026lt; /dev/random | head -c 1024 \u0026gt;\u0026gt; 1k.txt Request Definition #  The requests node is used to set requests to be executed by Loadgen in sequence. Loadgen supports fixed-parameter requests and requests constructed using template-based variable parameters. The following is an example of a common query request.\nrequests: - request: method: GET basic_auth: username: elastic password: pass url: http://localhost:8000/medcl/_search?q=name:$[[user]] In the above query, Loadgen conducts queries based on the medcl index and executes one query based on the name field. The value of each request is from the random variable user.\nCLI Parameters #  Loadgen cyclically executes requests defined in the configuration file. By default, Loadgen runs for 5s and then automatically exits. If you want to prolong the running time or increase the concurrency, you can set the tool\u0026rsquo;s startup parameters. The help commands are as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen --help Usage of ./bin/loadgen: -c int Number of concurrent threads (default 1) -compress Compress requests with gzip -config string the location of config file, default: loadgen.yml (default \u0026quot;loadgen.yml\u0026quot;) -d int Duration of tests in seconds (default 5) -debug run in debug mode, loadgen will quit with panic error -l int Limit total requests (default -1) -log string the log level,options:trace,debug,info,warn,error (default \u0026quot;info\u0026quot;) -r int Max requests per second (fixed QPS) (default -1) -v\tversion Benchmark Test #  Run Loadgen to perform the benchmark test as follows:\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -compress __ ___ _ ___ ___ __ __ / / /___\\/_\\ / \\/ _ \\ /__\\/\\ \\ \\ / / // ///_\\\\ / /\\ / /_\\//_\\ / \\/ / / /__/ \\_// _ \\/ /_// /_\\\\//__/ /\\ / \\____|___/\\_/ \\_/___,'\\____/\\__/\\_\\ \\/ [LOADGEN] A http load generator and testing suit. [LOADGEN] 1.0.0_SNAPSHOT, 83f2cb9, Sun Jul 4 13:52:42 2021 +0800, medcl, support single item in dict files [07-19 16:15:00] [INF] [instance.go:24] workspace: data/loadgen/nodes/0 [07-19 16:15:00] [INF] [loader.go:312] warmup started [07-19 16:15:00] [INF] [app.go:306] loadgen now started. [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:00] [INF] [loader.go:316] [GET] http://localhost:8000/medcl/_search?q=name:medcl [07-19 16:15:00] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:1,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}} [07-19 16:15:01] [INF] [loader.go:316] [POST] http://localhost:8000/_bulk [07-19 16:15:01] [INF] [loader.go:317] status: 200,\u0026lt;nil\u0026gt;,{\u0026quot;took\u0026quot;:120,\u0026quot;errors\u0026quot;:false,\u0026quot;items\u0026quot;:[{\u0026quot;index\u0026quot;:{\u0026quot;_index\u0026quot;:\u0026quot;medcl-y4\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c3qj9123r0okahraiej0\u0026quot;,\u0026quot;_version\u0026quot;:1,\u0026quot;result\u0026quot;:\u0026quot;created\u0026quot;,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:2,\u0026quot;successful\u0026quot;:1,\u0026quot;failed\u0026quot;:0},\u0026quot;_seq_no\u0026quot;:5735852,\u0026quot;_primary_term\u0026quot;:3,\u0026quot;status\u0026quot;:201}}]} [07-19 16:15:01] [INF] [loader.go:325] warmup finished 5253 requests in 32.756483336s, 524.61KB sent, 2.49MB received [Loadgen Client Metrics] Requests/sec:\t175.10 Request Traffic/sec:\t17.49KB Total Transfer/sec:\t102.34KB Avg Req Time:\t5.711022ms Fastest Request:\t440.448µs Slowest Request:\t3.624302658s Number of Errors:\t0 Number of Invalid:\t0 Status 200:\t5253 [Estimated Server Metrics] Requests/sec:\t160.37 Transfer/sec:\t93.73KB Avg Req Time:\t623.576686ms Loadgen executes all requests once to warm up before the formal benchmark test. If an error occurs, a prompt is displayed, asking you whether to continue. The warm-up request results are also output to the terminal. After execution, an execution summary is output.\n The final results of Loadgen are the cumulative statistics after all requests are executed, and they may be inaccurate. You are advised to start the Kibana dashboard to check all operating indicators of Elasticsearch in real time.\n Simulating Bulk Ingestion #  It is very easy to use Loadgen to simulate bulk ingestion. Configure one index operation in the request body and then use the body_repeat_times parameter to randomly replicate several parameterized requests to complete the preparation of a batch of requests. See the following example.\n - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/_bulk body_repeat_times: 1000 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-y4\\\u0026quot;,\\\u0026quot;_type\\\u0026quot;:\\\u0026quot;doc\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot;,\\\u0026quot;field1\\\u0026quot; : \\\u0026quot;$[[user]]\\\u0026quot;,\\\u0026quot;ip\\\u0026quot; : \\\u0026quot;$[[ip]]\\\u0026quot;,\\\u0026quot;now_local\\\u0026quot; : \\\u0026quot;$[[now_local]]\\\u0026quot;,\\\u0026quot;now_unix\\\u0026quot; : \\\u0026quot;$[[now_unix]]\\\u0026quot; }\\n\u0026quot; Limiting the Client Workload #  You can use Loadgen and set the CLI parameter -r to restrict the number of requests that can be sent by the client per second, so as to evaluate the response time and load of Elasticsearch under fixed pressure. See the following example.\n➜ loadgen git:(master) ✗ ./bin/loadgen -d 30 -c 100 -r 100  Note: The client throughput limit may not be accurate enough in the case of massive concurrencies.\n Limiting the Total Number of Requests #  You can set the -l parameter to control the total number of requests that can be sent by the client, so as to generate a fixed number of documents. Modify the configuration as follows:\nrequests: - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-test\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot;,\\\u0026quot;field1\\\u0026quot; : \\\u0026quot;$[[user]]\\\u0026quot;,\\\u0026quot;ip\\\u0026quot; : \\\u0026quot;$[[ip]]\\\u0026quot; }\\n\u0026quot; Configured parameters use the content of only one document for each request. Then, the system executes Loadgen.\n./bin/loadgen -config loadgen-gw.yml -d 600 -c 100 -l 50000 After execution, 50000 records are added for the Elasticsearch index medcl-test.\nUsing Auto Incremental IDs to Ensure the Document Sequence #  If the IDs of generated documents need to increase regularly to facilitate comparison, you can use the auto incremental IDs of the sequence type as the primary key and avoid using random numbers in the content. See the following example.\nrequests: - request: method: POST basic_auth: username: test password: testtest url: http://localhost:8000/medcl-test/doc2/_bulk body_repeat_times: 1 body: \u0026quot;{ \\\u0026quot;index\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;medcl-test\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[id]]\\\u0026quot; }\\n\u0026quot; Reuse variables in Request Context #  In a request, we might want use the same variable value, such as the routing parameter to control the shard destination, also store the field in the JSON document. You can use runtime_variables to set request-level variables, or runtime_body_line_variables to define request-body-level variables. If the request body set body_repeat_times, each line will be different, as shown in the following example:\nvariables: - name: id type: sequence - name: uuid type: uuid - name: now_local type: now_local - name: now_utc type: now_utc - name: now_unix type: now_unix - name: suffix type: range from: 10 to: 15 requests: - request: method: POST runtime_variables: batch_no: id runtime_body_line_variables: routing_no: uuid basic_auth: username: ingest password: password #url: http://localhost:8000/_search?q=$[[id]] url: http://192.168.3.188:9206/_bulk body_repeat_times: 10 body: \u0026quot;{ \\\u0026quot;create\\\u0026quot; : { \\\u0026quot;_index\\\u0026quot; : \\\u0026quot;test-$[[suffix]]\\\u0026quot;,\\\u0026quot;_type\\\u0026quot;:\\\u0026quot;doc\\\u0026quot;, \\\u0026quot;_id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot; , \\\u0026quot;routing\\\u0026quot; : \\\u0026quot;$[[routing_no]]\\\u0026quot; } }\\n{ \\\u0026quot;id\\\u0026quot; : \\\u0026quot;$[[uuid]]\\\u0026quot;,\\\u0026quot;routing_no\\\u0026quot; : \\\u0026quot;$[[routing_no]]\\\u0026quot;,\\\u0026quot;batch_number\\\u0026quot; : \\\u0026quot;$[[batch_no]]\\\u0026quot;, \\\u0026quot;random_no\\\u0026quot; : \\\u0026quot;$[[suffix]]\\\u0026quot;,\\\u0026quot;ip\\\u0026quot; : \\\u0026quot;$[[ip]]\\\u0026quot;,\\\u0026quot;now_local\\\u0026quot; : \\\u0026quot;$[[now_local]]\\\u0026quot;,\\\u0026quot;now_unix\\\u0026quot; : \\\u0026quot;$[[now_unix]]\\\u0026quot; }\\n\u0026quot; We defined the batch_no variable to represent the same batch number in a batch of documents, and the routing_no variable to represent the routing value at each document level.\n"});index.add({'id':54,'href':'/en/docs/latest/console/troubleshooting/','title':"FAQs",'section':"INFINI Console",'content':"FAQs and Troubleshooting #  FAQs about INFINI Console and handling methods are provided here. You are welcome to submit your problems here.\nCommon Faults #  Nginx is added in front of Elasticsearch, and the console prompts a 400 error #  Similar error logs are as follows:\n[11-25 18:26:58] [TRC] [v0.go:390] search response: {\u0026quot;query\u0026quot;:{\u0026quot;match\u0026quot;:{\u0026quot;status\u0026quot;: \u0026quot;RUNNING\u0026quot;}}},\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;title\u0026gt;Error 400 (Bad Request)!!1\u0026lt;/title\u0026gt; \u0026lt;p\u0026gt;\u0026lt;b\u0026gt;400.\u0026lt;/b\u0026gt; \u0026lt;ins\u0026gt;That’s an error.\u0026lt;/ins\u0026gt; \u0026lt;p\u0026gt;Your client has issued a malformed or illegal request. \u0026lt;ins\u0026gt;That’s all we know.\u0026lt;/ins\u0026gt; [11-25 18:26:58] [ERR] [init.go:87] json: invalid character '\u0026lt;' looking for beginning of value: \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=en\u0026gt; Fault Description #  Nginx does not support passing request body for GET request type\nSolution #  Recommended to upgrade to the latest version.\nMonitoring data is not displayed after the cluster is registered #  as shown below:\nFault Description #  INFINI Console needs to use some features of Elasticsearch 7.0 and above\nSolution #  Upgrade the Elasticsearch cluster version of the INFINI Console storage data to v7.0+\nStartup Error #  [03-23 08:38:20] [ERR] [metadata.go:529] {\u0026quot;error\u0026quot;:{\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;illegal_argument_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;Can't merge a non object mapping [payload.node_state.settings.http.type] with an object mapping [payload.node_state.settings.http.type]\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;illegal_argument_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;Can't merge a non object mapping [payload.node_state.settings.http.type] with an object mapping [payload.node_state.settings.http.type]\u0026quot;},\u0026quot;status\u0026quot;:400} or\n[04-16 09:45:06] [ERR] [schema.go:144] error on update mapping: {\u0026quot;root_cause\u0026quot;:[{\u0026quot;type\u0026quot;:\u0026quot;mapper_parsing_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;Failed to parse mapping [_doc]: analyzer [suggest_text_search] has not been configured in mappings\u0026quot;}],\u0026quot;type\u0026quot;:\u0026quot;mapper_parsing_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;Failed to parse mapping [_doc]: analyzer [suggest_text_search] has not been configured in mappings\u0026quot;,\u0026quot;caused_by\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;illegal_argument_exception\u0026quot;,\u0026quot;reason\u0026quot;:\u0026quot;analyzer [suggest_text_search] has not been configured in mappings\u0026quot;}} Fault Description #  Version v0.3 modified the Template and Mapping, if the index already exists and the Mapping is not of the expected object type, then the error will be show up.\nFor upgrade, please refer to the upgrade instructions.\nSolution #   stop console delete template .infini  DELETE _template/.infini  delete index .infini_node and .infini_index  DELETE .infini_node DELETE .infini_index  start console  "});index.add({'id':55,'href':'/en/docs/latest/gateway/troubleshooting/','title':"FAQs",'section':"INFINI Gateway",'content':"FAQs and Troubleshooting #  FAQs about INFINI Gateway and handling methods are provided here. You are welcome to submit your problems here.\nFAQs #  The Auth section in Elasticsearch configuration #  Q：I see that when configuring Elasticsearch, I need to specify user information, what is use for? A：INFINI Gateway is a transparent gateway, the parameters that are passed before the gateway placed, just keep them same, such as identity information or any other parameters that need to be passed. The auth information in the gateway mainly used to obtain the internal running status or metadata info of the cluster, some asynchronous operations that require the gateway to carry out also need to use the auth information, such as persist metrics or logs to that elasticsearch cluster.\nThe Write Speed Is Not Improved #  Q: Why is the write speed not improved after I use bulk_reshuffle of the INFINI gateway?\nA: If your cluster has a small number of nodes, for example, if it contains less than 10 data nodes or if the index throughput is lower than 150k event/s, you may not need to use this feature or you do not need to focus on the write performance because the cluster size is too small and forwarding and request distribution have a minimal impact on Elasticsearch. Therefore, the performance does not differ greatly regardless of whether the gateway is used. But there are other benefits of using bulk_reshuffle, for example, the impact of faults occurring on the back-end Elasticsearch can be decoupled if data is sent to the gateway first.\nElasticsearch 401 Error #  Q：I see some error when switch to INFINI Gateway: missing authentication credentials for REST request [/] A：The INFINI gateway is a transparent gateway. The auth information configured in the gateway only used for the communication between the gateway and Elasticsearch. Clients still need to pass appropriate auth information to access Elasticsearch resources.\nCommon Faults #  Port Reuse Is Not Supported #  Error prompt: The OS doesn\u0026rsquo;t support SO_REUSEPORT: cannot enable SO_REUSEPORT: protocol not available\nFault description: Port reuse is enabled on INFINI Gateway by default. It is used for multi-process port sharing. Patches need to be installed in the Linux kernel of the old version so that the port reuse becomes available.\nSolution: Modify the network monitoring configuration by changing reuse_port to false to disable port reuse.\n**. network: binding: 0.0.0.0:xx reuse_port: false An Elasticsearch User Does Not Have Sufficient Permissions #  Error prompt: [03-10 14:57:43] [ERR] [app.go:325] shutdown: json: cannot unmarshal object into Go value of type []adapter.CatIndexResponse\nFault description: After discovery is enabled for Elasticsearch on INFINI Gateway, this error is generated if the user permission is insufficient. The cause is that relevant Elasticsearch APIs need to be accessed to acquire cluster information.\nSolution: Grant the monitor and view_index_metadata permissions of all indexes to relevant Elasticsearch users.\n"});index.add({'id':56,'href':'/en/docs/latest/console/tutorials/cluster_health_change/','title':"How to Monitor Elasticsearch Cluster Health",'section':"Tutorials",'content':"How to Monitor Elasticsearch Cluster Health #  Introduction #  In many cases, the cluster health status of the Elasticsearch cluster will turn red for some reason. At this time, at least one primary shard in the Elasticsearch cluster is unallocated or lost. So it is necessary to monitor the health status of the Elasticsearch cluster. This article will introduce how to use the INFINI Console alerting feature to monitor the health of an Elasticsearch cluster.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Select the alerting object .infini_metrics (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter condition (Elasticsearch query DSL) Here we need to filter the data whose monitoring metrics category is cluster_health and the health status is red. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;match\u0026quot;: { \u0026quot;payload.elasticsearch.cluster_health.status\u0026quot;: \u0026quot;red\u0026quot; } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;cluster_health\u0026quot; } } } ] } }  Select time field and stat period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so they need to be grouped according to the cluster ID, Here we choose metadata.labels.cluster_id Configure the alerting metrics, select the aggregation field payload.elasticsearch.cluster_health.status, and the statistical method count Configure the alerting condition, configure the continue for one period and the aggregation result is greater than or equal to 1, that is, the Critical alerting is triggered Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here   Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nSimulate trigger alerting rule #  Open the INFINI Console Dev tools (Ctrl+Shift+O) and enter the command as shown below:\nReceive alert notification message #  After waiting for about a minute, you will receive a DingTalk alerting notification as follows:\nYou can see that the alerting notification message shows the ID of the Elasticsearch cluster whose health status has turned red. Click the link below the message to view the alerting details as follows:\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the health status of the Elasticsearch cluster. After configuring alerting rule,As soon as any Elasticsearch cluster status turns red, an alert is triggered and an alert message is sent.\n"});index.add({'id':57,'href':'/en/docs/latest/console/tutorials/cluster_node_disk_usage/','title':"How to monitor Elasticsearch cluster node disk usage",'section':"Tutorials",'content':"How to monitor Elasticsearch cluster node disk usage #  Introduction #  When the system disk usage is too high, data cannot be written into the Elasticsearch cluster, which is likely to result in data loss. Therefore, monitor the Elasticsearch cluster. Node disk usage is necessary. This article will show you how to monitor your Elasticsearch cluster using INFINI Console alerts Node disk usage.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats, the DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.fs.total.free_in_bytes, and the statistics method avg. Then add another alerting metrics, select the aggregation field payload.elasticsearch.node_stats.fs.total.total_in_bytes, and the statistical method avg. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as ((b-a)/b)*100, which means to use the total Disk space subtract remaining disk space to get disk used space, Then divide disk used space by total disk space and multiply by 100 to get disk usage Configure the alerting conditions, here configure three alerting conditions, configure the P2(Medium) alerting when the disk usage is greater than 80 for persisting for one period; Configure the continue for one period when the disk usage is greater than 90, trigger the P1(High) alerting; Configure persisting for a period when the disk usage rate is greater than 95, trigger the P0(Critical) alerting; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID: {{index.group_values ​​0}}; NodeID: {{index.group_values ​​1}}; Disk Usage:{{.result_value | to_fixed 2}}%; Free Storage: {{.relation_values.a | format_bytes 2}}; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and remaining disk space with high disk usage.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the disk usage of Elasticsearch cluster nodes. After configuring alerting rule, Once the disk usage of any Elasticsearch node exceeds the set threshold, an alert will be triggered and an alert message will be sent.\n"});index.add({'id':58,'href':'/en/docs/latest/console/tutorials/cluster_slow_request/','title':"How to monitor slow query requests in Elasticsearch",'section':"Tutorials",'content':"How to monitor slow query requests in Elasticsearch #  Introduction #  Many times, the Elasticsearch cluster will experience peak data writing or query traffic. At this time, the Elasticsearch cluster will be under a lot of pressure. Through the monitoring and alertinging of the delay of the Elasticsearch index query. This allows us to locate which indexes are the most stressed on the Elasticsearch cluster. This article will introduce how to use the INFINI Console alerting function to monitor the slow query request index in Elasticsearch.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to index_stats, and the index name cannot be _all, the DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;index_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ], \u0026quot;must_not\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.labels.index_name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;_all\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the index name, Here we choose metadata.labels.cluster_id and metadata.labels.index_name Configure the alerting metrics, select the aggregation field payload.elasticsearch.index_stats.total.search.query_time_in_millis, and use the statistical method to derive derivative. Then add another alerting metrics, select the aggregation field payload.elasticsearch.index_stats.total.search.query_total, and the statistical method derivative. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a/b to calculate the delay, Configure the alerting conditions, three alerting conditions are configured here, and when the continue for one cycle delay is greater than 100, the P3(Low) alerting is triggered; Configure the Continue for one cycle delay when the delay is greater than 500, trigger the P1(High) alerting; Configure continue for one cycle when the delay is greater than 1000, trigger the P0(Critical) alerting; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; Index name:{{index.group_values ​​1}}; Current value:{{.result_value | to_fixed 2}}ms; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alerting notification message shows the Elasticsearch cluster ID, index name, and delay size of which query delay is too high.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the slow index of the Elasticsearch cluster. After configuring alerting rule, Once any Elasticsearch index query latency is too high, an alert will be triggered and an alert message will be sent.\n"});index.add({'id':59,'href':'/en/docs/latest/console/tutorials/cluster_node_cpu_usage/','title':"How to monitor the CPU usage of Elasticsearch cluster nodes",'section':"Tutorials",'content':"How to monitor the CPU usage of Elasticsearch cluster nodes #  Introduction #  This article will introduce how to use the INFINI Console to monitor the disk usage of Elasticsearch cluster nodes and alert them.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click Alerting \u0026gt; Rules on the left menu to enter the alerting management page, and then click the New button to enter the alerting rule creation page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter condition (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats and the metadata category to elasticsearch. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id. Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.process.cpu.percent, and the statistics method avg. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a. Then set the value type of the variable a to the ratio Ratio. Configure the alerting conditions, configure three alerting conditions here, and configure the P2(Medium) alerting when the CPU usage is greater than 80 for continuous one cycle; Configure the continue for one cycle when the CPU usage is greater than 90, trigger the P1(High) alerting; Configure the continuous period to trigger the P0(Critical) alerting when the CPU usage is greater than 95; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; NodeID:{{index.group_values ​​1}}; CPU:{{.result_value | to_fixed 2}}%; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and current CPU usage triggered by the current rule.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alerting function, you can easily monitor the CPU usage of Elasticsearch cluster nodes. After configuring the alerting rule, once the CPU usage of any Elasticsearch node exceeds the set threshold, an alert will be triggered and an alert message will be sent.\n"});index.add({'id':60,'href':'/en/docs/latest/console/tutorials/cluster_node_jvm_usage/','title':"How to monitor JVM usage of Elasticsearch cluster nodes",'section':"Tutorials",'content':"How to monitor JVM usage of Elasticsearch cluster nodes #  Introduction #  This article will introduce how to use the INFINI Console to monitor the JVM usage of Elasticsearch cluster nodes and generate alerts.\nPrepare #   Download and install the latest version of INFINI Console Register Elasticsearch cluster using INFINI Console  Create alerting rule #  Open INFINI Console in the browser, click on the left menu \u0026ldquo;Alerting\u0026rdquo; \u0026gt; Rules to enter the alerting management page, and then click New button to enter the Create Alerting Rule page. Follow these steps to create an alerting rule:\n Select the cluster (here you need to select the Elasticsearch cluster where the INFINI Console stores data, that is, the Elasticsearch cluster configured in the configuration file console.yml, if it is not registered to the INFINI Console, please register first) Input the alerting object .infini_metrics* (select the index under the Elasticsearch cluster, or enter the index pattern, because the monitoring data collected by the INFINI Console is stored in the index .infini_metrics) Input filter criteria (Elasticsearch query DSL) Here we need to filter the monitoring metrics category to node_stats and the metadata category to elasticsearch. The DSL is as follows:  { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: [ { \u0026quot;term\u0026quot;: { \u0026quot;metadata.name\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;node_stats\u0026quot; } } }, { \u0026quot;term\u0026quot;: { \u0026quot;metadata.category\u0026quot;: { \u0026quot;value\u0026quot;: \u0026quot;elasticsearch\u0026quot; } } } ] } }  Select time field timestamp and statistical period for date histogram aggregation   Input the rule name Group settings (optional, multiple can be configured), set when statistical metrics need to be grouped, because all registered to INFINI Console The Elasticsearch cluster monitoring metrics are stored in the index .infini_metrics, so you need to group according to the cluster ID first, and then group according to the node ID, Here we choose metadata.labels.cluster_id and metadata.labels.node_id. Configure the alerting metrics, select the aggregation field payload.elasticsearch.node_stats.jvm.mem.heap_used_percent, and the statistics method p90. Configure the metrics formula (when more than one alerting metrics is configured, you need to set a formula to calculate the target metrics), where the formula fx is configured as a. Then set the value type of the variable a to the ratio Ratio. Configure the alerting conditions, configure three alerting conditions here, configure the P2(Medium) alerting when the JVM usage rate is greater than 50 for continuous one cycle; Configure continues one cycle when the JVM usage rate is greater than 90, trigger the P1(High) alerting; Configure the P0(Critical) alerting to be triggered when the JVM usage rate is greater than 95 for one cycle; Set the execution period, here is configured to execute a check every minute Set the event title, the event title is a template, you can use template variables, template syntax and template variable usage reference here Set the event content, the event content is a template, you can use template variables, template syntax and template variable usage reference here  Priority:{{.priority}} Timestamp:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}} RuleID:{{.rule_id}} EventID:{{.event_id}} {{range.results}} ClusterID:{{index.group_values ​​0}}; NodeID:{{index.group_values ​​1}}; JVM used percent: {{.result_value | to_fixed 2}}%; {{end}}  Turn on the configure alerting channel switch, and select add in the upper right corner to quickly select an alerting channel template to fill. For how to create an alerting channel template, please refer to here Set the silence period to 1 hour, that is, after the alerting rule is triggered, the notification message will only be sent once within an hour Set the receiving period, the default is 00:00-23:59, that is, you can receive notification messages throughout the day  After the settings are complete, click the Save button to submit.\nReceive alert notification message #  Wait for a while, and receive the DingTalk alerting message notification as follows:\nYou can see that the alert notification message displays the Elasticsearch cluster ID, node ID, and current JVM usage rate triggered by the current rule.\nView the alerting message center #  In addition to receiving external notification messages, the INFINI Console Alert Message Center also generates an alert message. Click menu Alerting \u0026gt; Alerting Center to enter\nSummary #  By using the INFINI Console alert function, you can easily monitor the JVM usage of Elasticsearch cluster nodes. After configuring alerting rule, Once any Elasticsearch node JVM usage exceeds a set threshold, an alert will be triggered and an alert message will be sent.\n"});index.add({'id':61,'href':'/en/docs/latest/gateway/references/filters/','title':"Online Filter",'section':"Reference",'content':"Request Filter #  What Is a Filter #  A filter is a series of processing units defined in a flow for requests received by the gateway. Each filter processes one task and the filters can be flexibly combined. Filters process requests online.\nFilter List #  Request Filtering #    context_filter  request_method_filter  request_header_filter  request_path_filter  request_user_filter  request_host_filter  request_client_ip_filter  request_api_key_filter  response_status_filter  response_header_filter  Request Forwarding #    ratio  clone  switch  flow  redirect  Request Mutation #    javascript  sample  request_body_json_del  request_body_json_set  context_regex_replace  request_body_regex_replace  response_body_regex_replace  response_header_format  set_context  set_basic_auth  set_hostname  set_request_header  set_request_query_args  set_response_header  set_response  Traffic Control and Throttling #    context_limiter  request_path_limiter  request_host_limiter  request_user_limiter  request_api_key_limiter  request_client_ip_limiter  retry_limiter  sleep  Log Monitoring #    logging  Elasticsearch #    date_range_precision_tuning  bulk_reshuffle  elasticsearch_health_check  bulk_response_process  bulk_request_mutate  Authentication #    basic_auth  ldap_auth  Output #    queue  elasticsearch  cache  translog  redis_pubsub  drop  http  Debugging and Development #    echo  dump  record  "});index.add({'id':62,'href':'/en/docs/latest/console/release-notes/','title':"Release Notes",'section':"INFINI Console",'content':"Release Notes #  Information about release notes of INFINI Console is provided here.\n0.7.0 #  Breaking changes #  Features #   Added Data Migration。 Added Initialization Guide. Added System Service Health Monitoring。 Added License Authorization.  Bug fix #   Fixed the problem that Discover did not search for the first time.  0.6.0 #  Breaking changes #  Features #   Added Overview Hosts. Added Monitor Hosts. Added log viewing in node overview (agent installation required). Insight Config Modal add Search Config.  Bug fix #   Fixed the problem that page Discover became white screen when used field filter. Fixed the problem that sort failure after adding fields to page Discover\u0026rsquo;s table. Fixed the problem that the cluster registration was unsuccessful due to the incompatibility of js in low-level browsers. Fixed the incompatibility issue of Elasticsearch 8.x deleting documents. Fixed the issue of exception handling when creating a new index was unsuccessful. Fixed the issue with null pointer references in metadata configuration. Fixed the issue where loading common commands failed in the dev tools.  Improvements #   Local list search lookup supports wildcard filtering. Support configuration page title suffix. Optimized the display of required fields in alerting rules. Set Discover TimeRange Auto Fit to 15 minutes. Optimize Discover to save the search, the field filters and Insight widget configuration will be saved. Optimized cluster list: add link jump; Support cluster list status field sorting.  0.5.0 #  Breaking changes #  Features #   Added IO metrics at the elasticsearch node level (only supports the Linux version of Elasticsearch cluster). Added agent management. Add docker image based on Centos. Add INFINI Insight chart type (number, pie, area).  Bug fix #   Fixed an issue with duplicate requests after the Gateway instance list was refreshed. Fixed an issue where docker image timezone loading failed. Fixed the issue that the metrics queue does not consume when the Elasticsearch cluster of storing data is unavailable. Fixed the problem that the development tool cannot forward the request to the Https based cluster. Fixed INFINI Insight all chart fetch data again after edits one of them. Fixed query still has the old state when jumping from the index link of other pages to discover.  Improvements #   Optimized the log output when refresh cluster state. Optimized the pop-up prompt that frequently jumps to the login page when unauthorized. Optimize the time selection UI of the discover search bar, make the space more compact and switch more convenien  0.4.0 #  Breaking changes #  Features #   Discover adds an Insight module, which pushes charts according to the data characteristics under the index, and visualizes the metrics data. Discover adds the functions of saving searches and replaying searches. Add alias management.  Bug fix #   Fixed the bug that the Dev tools returned an error when sending a request response in v0.3.1 without security enabled. Fixed the bug that the AWS Elasticsearch cloud environment had no node http.public_address, which caused an error in collecting monitoring data. Fixed the bug that when the Elasticsearch cluster for storing data is unavailable, the collected metric data are not consumed(Updating the settings of elastic\u0026gt;store defaults to false in console.yml).  Improvements #   Optimized Console storage data Elasticsearch version check prompt.  0.3.1 #  Bug fix #   The kv module should be initialized before elastic module The account profile api should get builtin username dynamically Fixed an issue where the index in the overview was not displayed correctly Fixed node health status in the overview was not displayed correctly Fixed the bug that the new channel could not get the type when the rule was submit  0.3.0 #  Breaking changes #  Features #   Support basic authentication Added platform overview Added cluster activities Added index management Added data view management Added data discover (Support both index and view) Support gzip compression and it is enabled by default Support rbac authorization Added alerting management (Support webhook channel) Added time-zone quick selector  Bug fix #   Fixed bug: discover multi fields selected Fixed bug: the count of nodes and shards value incorrect in cluster overview Fixed bug: overview search request params field from do not counting from 0 Fixed bug: login page tab not centered Fixed bug: Re-login redirect jump parameter problem caused by session expiration Fixed bug: Overview Statistic component mask state value incorrect Fixed bug: repeat http request pending state Fixed bug: console copy as curl without an endpoint  Improvements #   Rewritten monitoring UI Optimize cluster metrics line chart Optimize health status component Add filter component to quick filter clisters,nodes,indices Add local sort for table column of clisters,nodes,indices Add isTLS form field for Gateway register Index list and node list Support real-time and non-real-time data switching viewing The interval for collecting elasticsearch cluster state is configurable Optimized requests to elasticsearch Add Console version info Add client http request timeout auto abort Dev tool support search Proper Handle metrics collecting while cluster in partial failure  0.2.0 #  Breaking changes #  Features #   Collect Elasticsearch cluster_health metrics Added thread pool related metrics Optimize the grouping of metrics Index .infini_metrics support ilm configuration Added hot key(Ctrl+Shift+O) to dev tools English version support  Bug fix #   Fixed the \u0026ldquo;required authentication credentials\u0026rdquo; issue in the test connection cluster time Fixed the problem that the validation failed when the cluster address is a domain name and contains special characters Fixed the issue that monitoring data is not displayed on 32-bit operating systems Fixed the problem that the Dev tools was initialized blank when the storage ES address changed Fixed the problem that the pagination of cluster list page cannot work  Improvements #   Cluster view Added metrics of counting cluster master, data, and coordinating nodes Cluster view Added metric of cluster health Node view Add JVM grouping, display related information of JVM memory Node view added JVM GC frequency and GC delay metrics Use POST instead of GET when request body is not nil Node view added cache hit rate and other related metrics Node View added metric of the number of open files Show the last time of the metrics was collected When the cluster is unavailable  0.1.0 #   Elasticsearch clusters management Basic monitoring supported for Elasticsearch cluster Dev tools support elasticsearch  "});index.add({'id':63,'href':'/en/docs/latest/gateway/release-notes/','title':"Release Notes",'section':"INFINI Gateway",'content':"Release Notes #  Information about release notes of INFINI Gateway is provided here.\n1.9.0 #  Breaking changes #   Refactoring config for ip access control Disable metadata refresh and node availability check by default Update default config path from configs to config  Features #   Support listen on IPv6 address Add general health api Add request_ip to context Add badger filter plugin Allow to split produce and consume messages from s3 Add bulk_request_throttle filter Support access request context and more output options in echo filter Add body_json to response context  Bug fix #   Fix user was removed in logging filter Fix incorrect message size issue, reload when files changed in disk_queue Fix issue that index_diff could not finished automatically Fix hostname was not well updated in filter set_request_header or set_hostname  Improvements #   Remove newline in indexing_merge and json_indexing processor Improve instance check, add config to disable Add option skip_insecure_verify to s3 module Improve instance check, enable config to disable Update the way to get ctx process info, optimize memory usage Improve indexing performance for bulk_indexing processor Refactoring disk_queue, speedup message consumption Enable segment compress for disk_queue by default Skip download s3 files when s3 was not enabled Add option to log warning messages for throttle filters Optimize hash performance for getting primary shardID and partitionID Add cache for get index routing table Optimize performance for bulk response processing Refactoring bulk_processor, pass meta info to payload func Don\u0026rsquo;t call payload func for delete action  1.8.1 #  Bug fix #   Remove newline in document for processor es_scroll and dump_hash  1.8.0 #  Breaking changes #   Remove config compress_on_message_payload from disk_queue Rename parameter consumer to name in consumer_has_lag condition Remove redundancy prefix name of the disk_queue files  Features #   Add segment level based disk_queue file compression  Bug fix #   Fix nil host in bulk_indexing processor Fix nil body in bulk_response_process filter Fix sliced consume in bulk_indexing processor  Improvements #   Handle bulk stats to bulk_response_process and used in logging filter  1.7.0 #  Breaking changes #  Features #   Add prometheus format to stats API Add redirect filter Add context_flow to flow filter Add permitted_client_ip_list to router Add Centos based docker image  Bug fix #   Fix date_range_precision filter failed to parse on specify field Fix disk usage status in windows platform  Improvements #   Merge events during config change, prevent unnecessary reload Handle templates when loading config Add cache to ldap_auth filter  1.6.0 #  Breaking changes #   Update disk_queue folder structure, use UUID as folder name instead of the queue name Parameter mode was removed from bulk_reshuffle filter, only async was supported Rename filter bulk_response_validate to bulk_response_process  Features #   Add metadata to queue Support subscribe queue by specify labels Support concurrent worker control for bulk_indexing processor Auto detect new queues for bulk_indexing processor Allow to consume queue messages over disk queue Auto sync disk_queue files to remote s3 in background Add api to operate gateway entry Support plugin auto discovery Add API to operate gateway entities Filter bulk_request_mutate support remove _type in bulk requests for es v8.0+ Add elasticsearch adapter for version 8.0+ Add http filter for general reverse proxy usage, like proxy Kibana Add consumer_has_lag condition to check queue status Add record filter to play requests easier Add zstd compress to disk_queue, disabled by default Add disorder_bulk_indexing processor Add javascript filter Add prefix and suffix to when conditions Add indexing_merge processor  Bug fix #   Fix date_range_precision_tuning filter for complex range query Fix node availability initially check Fix basic_auth filter not asking user to input auth info in browser Fix null id not fixed in filter bulk_request_mutate and bulk_reshuffle Fix switch filter not forwarding when remove_prefix was disabled Fix buffer was not proper reset in flow_runner processor Fix entry not loading the pre-defined TLS certificates Fix set_basic_auth not proper reset previous user information Fix elapsed in request logging not correct Fix switch filter, use strings.TrimPrefix instead of strings.TrimLeft Fix the last query_string args can\u0026rsquo;t be deleted, parameter no_cache in get_cache filter fixed Fix s3 downloaded file corrupted  Improvements #   Handle http public address, remove prefix if that exists Refactor bulk_reshuffle filter and bulk_indexing processor Should not fetch nodes info when elasticsearch discovery disabled Seamless consume queue message across files Persist consumer offset to local store Add API to reset consumer offset Refactoring ORM framework Expose error of mapping put Refactoring pipeline framework Improve multi-instance check, multi-instance disabled by default Add CPU and memory metrics to stats api Seamless fetch queue files from s3 server Proper handle 409 version conflicts in bulk requests Allow memory queue to retry 1s when it is full Proper handle the cluster available check Proper handle the cluster partial failure check Exit bulk worker while no new messages returned  1.5.0 #  Breaking changes #  Features #   Add API to scroll messages from disk queue Prevent out of space, disk usage reserved for disk_queue Add context_filter and context_limiter for general purpose Add bulk_request_mutate filter Add basic_auth filter Add set_context filter Add context_regex_replace filter Add to_string property to request and response context  Bug fix #   Fix bulk response validate incorrectly caused by jsonParser Fix nil exception in request_path_limiter caused by refactoring Fix big size document out of order caused by bulk buffer  Improvements #   Fix TCP not keepalived in some case Add closing progress bar to pipeline module Add retry_delay_in_ms config to pipeline module Handle partial failure in bulk requests Optimize scroll performance of dump_hash processor Improve API directory  1.4.0 #  Breaking changes #   Rename flow config filter_v2 to filter, only support new syntax Rename pipeline config pipelines_v2 to pipeline, processors to processor, only support new syntax Rename filter request_logging to logging Merge dump filters to dump filter Response headers renamed, dashboard may broken Remove filter request_body_truncate and response_body_truncate  Features #   Add option to disable file logging output Add option compress to queue_consumer processor  Bug fix #   Fix invalid host header setting in elasticsearch reverse proxy Fix cluster available health check Fix gzip encoding issue for requests forwarding  Improvements #   Support string type in in condition  1.3.0 #  Breaking changes #   Switch to use pipelines_v2 syntax only Rename filter disk_enqueue to queue Rename processor disk_queue_consumer to queue_consumer Rename filter redis to redis_pubsub  Features #   Refactoring pipeline framework, support DAG based task schedule Add dump_hash and index_diffs processor Add redis output and redis queue adapter Add set_request_query_args filter Add ldap_auth filter Add retry_limiter filter Add request_body_json_set and request_body_json_del filter Add stats filter Add health_check config to elastic module Add API to pipeline framework, support _start and _stop pipelines  Bug fix #   Fix data race issue in bulk_reshuffle Fix fix_null_id always executed in bulk_reshuffle Auto handle big sized documents in bulk requests  Improvements #   Refactoring flow runner to service pipeline Optimize CPU and Memory usage Optimize index diff service, speedup and cross version compatibility Set the default max file size of queue files to 1 GB Proper handle elasticsearch failure during startup Support custom depth check to queue_has_lag condition Support multi hosts for elasticsearch configuration Add parameter auto_start to prevent pipeline running on start Add keep_running parameter to pipeline config Safety shutdown pipeline and entry service Support more complex routing pattern rules  1.2.0 #  Features #   Support alias in bulk_reshuffle filter. Support truncate in request_logging filter. Handle 429 retry in json_indexing service. Add forcemerge service. Add response_body_regex_replace filter. Add request_body_regex_replace filter. Add sleep filter. Add option to log slow requests only. Add cluster and bulk status to request logging. Add filter_v2 and support _ctx to access request context. Add dump_context filter. Add translog filter, support rotation and compression. Add set_response filter. Add set_request_header filter. Add set_hostname filter. Add set_basic_auth filter. Add set_response_header filter. Add elasticsearch_health_check filter. Add drop filter.  Bug fix #   Fix truncate body filter, correctly resize the body bytes. Fix cache filter. Fix floating_ip module. Fix dirty write in diskqueue. Fix compression enabled requests. Fix date_range_precision_tuning filter. Fix invalid indices status on closed indices #23. Fix document hash for elasticsearch 6.x. Fix floating_ip feature run with daemon mode. Fix async bulk to work with beats.  Improvements #   Optimize memory usage, fix memory leak.  Acknowledgement #  Thanks to the following enterprises and teams #   China Everbright Bank, China Citic Bank, BSG, Yogoo  Thanks to the following individual contributors #   MaQianghua, YangFan, Tanzi, FangLi  1.1.0 #   Request Logging and Dashboard. Support ARM Platform [armv5\\v6\\v7\\v8(arm64)]. Fix Elasticsearch Nodes Auto Discovery. Add Request Header Filter. Add Request Method Filter. Add Sample Filter. Request Logging Performance Optimized (100x speedup). Add Request Path Filter. Add Debug Filter. Add User Info to Logging Message. Support Routing Partial Traffic to Specify Processing Flow (by Ratio). Support Traffic Clone, Support Dual-Write or 1:N Write. Elasticsearch topology auto discovery, support filter by nodes,tags,roles. Backend failure auto detection, auto retry and select another available endpoint. Floating IP feature ready to use. Add bulk_reshuffle filter.  1.0.0 #   Rewritten for performance Index level request throttle Request caching Kibana MAGIC speedup Upstream auto discovery Weighted upstream selections Max connection limit per upstream  "});index.add({'id':64,'href':'/en/docs/latest/console/upgrade/','title':"Upgrade",'section':"INFINI Console",'content':"Upgrade #  Here are instructions for the version upgrade of the INFINI Console.\nFrom 0.2 to 0.3 #  Update template .infini #  PUT _template/.infini { \u0026quot;order\u0026quot;: 0, \u0026quot;index_patterns\u0026quot;: [ \u0026quot;.infini_*\u0026quot; ], \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;max_result_window\u0026quot;: \u0026quot;10000000\u0026quot;, \u0026quot;mapping\u0026quot;: { \u0026quot;total_fields\u0026quot;: { \u0026quot;limit\u0026quot;: \u0026quot;20000\u0026quot; } }, \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;suggest_text_search\u0026quot;: { \u0026quot;filter\u0026quot;: [ \u0026quot;word_delimiter\u0026quot; ], \u0026quot;tokenizer\u0026quot;: \u0026quot;classic\u0026quot; } } }, \u0026quot;number_of_shards\u0026quot;: \u0026quot;1\u0026quot; } }, \u0026quot;mappings\u0026quot;: { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot;: {} } Close index .infini_cluster #  POST .infini_cluster/_close Update index settings of .infini_cluster #  PUT .infini_cluster/_settings { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;suggest_text_search\u0026quot;: { \u0026quot;filter\u0026quot;: [ \u0026quot;word_delimiter\u0026quot; ], \u0026quot;tokenizer\u0026quot;: \u0026quot;classic\u0026quot; } } } } Update index mappings of .infini_cluster #  PUT .infini_cluster/_mapping { \u0026quot;dynamic_templates\u0026quot;: [ { \u0026quot;strings\u0026quot;: { \u0026quot;match_mapping_type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;mapping\u0026quot;: { \u0026quot;ignore_above\u0026quot;: 256, \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } } ], \u0026quot;properties\u0026quot;: { \u0026quot;basic_auth\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;password\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;username\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } }, \u0026quot;created\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; }, \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;discovery\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;refresh\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;object\u0026quot; } } }, \u0026quot;enabled\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;boolean\u0026quot; }, \u0026quot;endpoint\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;endpoints\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;host\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;hosts\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;labels\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;health_status\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot;: 256 } } }, \u0026quot;location\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;dc\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;provider\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;rack\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;region\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } }, \u0026quot;monitored\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;boolean\u0026quot; }, \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } }, \u0026quot;order\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;integer\u0026quot; }, \u0026quot;owner\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;department\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;id\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;name\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] } } }, \u0026quot;project\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;schema\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;search_text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;suggest_text_search\u0026quot;, \u0026quot;index_prefixes\u0026quot;: { \u0026quot;min_chars\u0026quot;: 2, \u0026quot;max_chars\u0026quot;: 5 }, \u0026quot;index_phrases\u0026quot;: true }, \u0026quot;tags\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;traffic_control\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;max_bytes_per_node\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;max_connection_per_node\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;max_qps_per_node\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; }, \u0026quot;max_wait_time_in_ms\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot; } } }, \u0026quot;updated\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;date\u0026quot; }, \u0026quot;version\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot;: [ \u0026quot;search_text\u0026quot; ] } } } Open index .infini_cluster #  POST .infini_cluster/_open Update console.yml #  The v0.3 version adds a pipeline configuration to the v0.2 version：\n- name: metadata_ingest auto_start: true keep_running: true processor: - metadata: bulk_size_in_mb: 10 bulk_max_docs_count: 5000 fetch_max_messages: 1000 elasticsearch: \u0026quot;default\u0026quot; queues: type: metadata category: elasticsearch when: cluster_available: [ \u0026quot;default\u0026quot; ] - name: activity_ingest auto_start: true keep_running: true processor: - activity: bulk_size_in_mb: 10 bulk_max_docs_count: 5000 fetch_max_messages: 1000 elasticsearch: \u0026quot;default\u0026quot; queues: category: elasticsearch activity: true consumer: group: activity when: cluster_available: [ \u0026quot;default\u0026quot; ]  stop console add the above configuration under the pipeline module in the console.yml configuration file start console  Configure ILM for index .infini_alert-history #  The alert module has been added in v0.3. The alert module stores a large amount of index data for execution records, so you need to configure ILM as follows:\nPUT _template/.infini_alert-history-rollover { \u0026quot;order\u0026quot; : 100000, \u0026quot;index_patterns\u0026quot; : [ \u0026quot;.infini_alert-history*\u0026quot; ], \u0026quot;settings\u0026quot; : { \u0026quot;index\u0026quot; : { \u0026quot;format\u0026quot; : \u0026quot;7\u0026quot;, \u0026quot;lifecycle\u0026quot; : { \u0026quot;name\u0026quot; : \u0026quot;infini_metrics-30days-retention\u0026quot;, \u0026quot;rollover_alias\u0026quot; : \u0026quot;.infini_alert-history\u0026quot; }, \u0026quot;codec\u0026quot; : \u0026quot;best_compression\u0026quot;, \u0026quot;number_of_shards\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;translog.durability\u0026quot;:\u0026quot;async\u0026quot; } }, \u0026quot;mappings\u0026quot; : { \u0026quot;dynamic_templates\u0026quot; : [ { \u0026quot;strings\u0026quot; : { \u0026quot;mapping\u0026quot; : { \u0026quot;ignore_above\u0026quot; : 256, \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;match_mapping_type\u0026quot; : \u0026quot;string\u0026quot; } } ] }, \u0026quot;aliases\u0026quot; : { } } DELETE .infini_alert-history DELETE .infini_alert-history-00001 PUT .infini_alert-history-00001 { \u0026quot;settings\u0026quot;: { \u0026quot;index.lifecycle.rollover_alias\u0026quot;:\u0026quot;.infini_alert-history\u0026quot; , \u0026quot;refresh_interval\u0026quot;: \u0026quot;5s\u0026quot; }, \u0026quot;aliases\u0026quot;:{ \u0026quot;.infini_alert-history\u0026quot;:{ \u0026quot;is_write_index\u0026quot;:true } }, \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot; : { \u0026quot;condition\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;items\u0026quot; : { \u0026quot;properties\u0026quot; : { \u0026quot;expression\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;minimum_period_match\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;long\u0026quot; }, \u0026quot;operator\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;severity\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;values\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 } } }, \u0026quot;operator\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 } } }, \u0026quot;condition_result\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;object\u0026quot;, \u0026quot;enabled\u0026quot; : false }, \u0026quot;context\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;created\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; }, \u0026quot;expression\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;is_escalated\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;is_notified\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;boolean\u0026quot; }, \u0026quot;message\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;objects\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;copy_to\u0026quot; : [ \u0026quot;search_text\u0026quot; ] }, \u0026quot;resource_id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;resource_name\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;rule_id\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;rule_name\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;search_text\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;suggest_text_search\u0026quot;, \u0026quot;index_prefixes\u0026quot; : { \u0026quot;min_chars\u0026quot; : 2, \u0026quot;max_chars\u0026quot; : 5 }, \u0026quot;index_phrases\u0026quot; : true }, \u0026quot;severity\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;state\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot;, \u0026quot;ignore_above\u0026quot; : 256 }, \u0026quot;title\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;keyword\u0026quot; }, \u0026quot;updated\u0026quot; : { \u0026quot;type\u0026quot; : \u0026quot;date\u0026quot; } } } } Upgrade FAQs #  Fault Description: #  Duplicate index data\nSolution: #   stop console delete index .infini_index start console  Fault Description: #  Duplicate node data\nSolution: #   stop console delete index .infini_node start console  Fault Description: #  The data node has monitoring data, Non-data nodes (master, client, etc.) have no monitoring data, as shown in the following figure：\nSolution: #  Please upgrade the INFINI Console to the latest version.\nFault Description: #  The page appears blank and JS reports an error.\nSolution: #  Please upgrade the INFINI Console to the latest version. Also provide the specific error information to us.\nFeedback #  If you have any other questions or suggestions, please submit them to us through the feedback on the right or here, to help us improving the products, thank you for your support!\n"});index.add({'id':65,'href':'/en/docs/latest/gateway/references/processors/','title':"Offline Processor",'section':"Reference",'content':"Pipeline #  What Is Pipeline? #  A pipeline is a function combination used for processing tasks offline. It uses the pipeline design pattern, just as online request filters do. A processor is the basic unit of a pipeline. Each processing component focuses on one task and the components can be flexibly assembled, and plugged and removed as required.\nPipeline Definition #  A typical pipeline service is defined as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 #in MB In the above configuration, a processing pipeline named request_logging_index is defined, and the processor parameter defines several processing units for the pipeline, which are executed in sequence.\nParameter Description #  Parameters related to pipeline definition are described as follows:\n   Name Type Description     name string Name of a pipeline, which must be unique   auto_start bool Whether the pipeline automatically starts with the gateway startup, that is, whether the task is executed immediately   keep_running bool Whether the gateway starts executing the task again after completing the execution   retry_delay_in_ms int Minimum waiting time for the task re-execution, which is set to 5000 milliseconds by default   processor array List of processors to be executed by the pipeline in sequence    Processor List #  Task Scheduling #    dag  Index Writing #    bulk_indexing  json_indexing  queue_consumer  Index Contrast #    dump_hash  index_diff  Request Processing #    flow_runner  Request Replay #    replay  "});index.add({'id':66,'href':'/en/docs/latest/console/reference/alerting/variables/','title':"Template variables",'section':"Alerting",'content':"Template variables #  Introduction #  When custom alerting triggers event content, in addition to the fixed copy written by yourself, the event title and event content also support template syntax. The rendering of the text can be achieved using fields in the event.\nVariables #  The syntax for rendering fields is {{ .fieldname }}, and the variable fields that can be used for template content rendering are as follows:\n   Field Name Type Descriction eg     rule_id string rule uuid c9f663tath2e5a0vksjg   rule_name string rule name High CPU usage   resource_id string resource uuid c9f663tath2e5a0vksjg   resource_name string resource name es-v716   event_id string identifier for check details c9f663tath2e5a0vksjx   timestamp number Millisecond timestamp 1654595042399   first_group_value string The first value of group_values in results c9aikmhpdamkiurn1vq0   first_threshold string The first value of threshold in results 90   priority string The highest priority in results critical   title string event title Node ({{.first_group_value}}) disk used \u0026gt;= 90%   message string event content EventID：{{.event_id}}; Cluster：{{.resource_name}}   results array result of groups    ┗ threshold array  [\u0026ldquo;90\u0026rdquo;]   ┗ priority string  high   ┗ group_values array  [\u0026ldquo;cluster-xxx\u0026rdquo;, \u0026ldquo;node-xxx\u0026rdquo;]   ┗ issue_timestamp number Millisecond timestamp 1654595042399   ┗ result_value float  91.2   ┗ relation_values map  {a:100, b:91.2}    Variable usage example #  Example 1:\n{\u0026quot;content\u0026quot;:\u0026quot;【Alerting】Event ID: {{.event_id}}, Cluster：{{.resource_name}}\u0026quot;} Example 2(array traversal):\n{{range .results}} Cluster ID: {{index .group_values 0}} {{end}} Template functions #  In addition to directly displaying the field value in the alerting event, it also supports the use of template functions to further process the field value to optimize the output.\nFunctions support extra parameters. When no parameters are required or passed, the following syntax can be used directly:\n{{ \u0026lt;field\u0026gt; | \u0026lt;function\u0026gt; }}\nSpecific examples are as follows:\nFunctions take no parameters:\nAlerting event trigger time:{{ .timestamp | datetime }} Functions take parameters:\nAlerting event trigger time:{{ .timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot; }} Use multiple functions in combination:\n{{.result_value | format_bytes 2 ｜ to_upper}} The complete list of template functions is as follows:\n   Functions params Descriction     to_fixed fixed number of decimal places The float type value retains N decimal places\nExample:{{.result_value | to_fixed 2}}\nOutput:10.35   format_bytes fixed number of decimal places Byte type numeric formatting\nExample:{{.result_value | format_bytes 2}}\nOutput:10.35gb   date  Convert timestamp to UTC date\nExample:{{.timestamp | date}}\nOutput:2022-05-01   date_in_zone Time zone Convert timestamp to current zone date\nExample:{{.timestamp | date_in_zone \u0026quot;Asia/Shanghai\u0026quot;}}\nOutput:2022-05-01   datetime  Convert timestamp to UTC time\nExample:{{.timestamp | datetime}}\nOutput:2022-05-01 10:10:10   datetime_in_zone Time zone Convert timestamp to current zone time\nExample:{{.timestamp | datetime_in_zone \u0026quot;Asia/Shanghai\u0026quot;}}\nOutput:2022-05-01 10:10:10   to_lower  Convert characters to lowercase\nExample:{{.resource_name | to_lower }}\nOutput:cluster   to_upper  Convert characters to uppercase\nExample:{{.resource_name | to_upper }}\nOutput:CLUSTER   add number Example: a+b\n{{.result_value | add 1 }}\nOutput：2   sub number Example: a - b\n{{sub .result_value 1 }}\nOutput：0   mul number Example: a * b * c\n{{mul .result_value 3 2 }}\nOutput：6   div number Example: a/b\n{{div .result_value 2 }}\nOutput：0.5    Common Template Syntax #  Array traversal：\n{{range .results}} priority: {{.priority}} {{end}} Get values by array subscript:\nExample: group_values = [\u0026ldquo;value1\u0026rdquo;,\u0026ldquo;value2\u0026rdquo;,\u0026ldquo;value3\u0026rdquo;]\n{{index .group_values 0}} # output: value1 {{index .group_values 2}} # output: value3 if conditional branch：\n{{if pipeline}} T1 {{else}} T0 {{end}} Example:\n{{if eq .priority \u0026quot;critical\u0026quot;}} \u0026quot;#C91010\u0026quot; {{else if eq .priority \u0026quot;high\u0026quot;}} \u0026quot;#EB4C21\u0026quot; {{else}} \u0026quot;#FFB449\u0026quot; {{end}} There is also a set of binary comparison operators defined as functions:\neq Returns the boolean truth of arg1 == arg2 ne Returns the boolean truth of arg1 != arg2 lt Returns the boolean truth of arg1 \u0026lt; arg2 le Returns the boolean truth of arg1 \u0026lt;= arg2 gt Returns the boolean truth of arg1 \u0026gt; arg2 ge Returns the boolean truth of arg1 \u0026gt;= arg2   A more complete example for Slack message ...  { \u0026quot;blocks\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;【test201】Alerting:\\n\u0026lt;http://localhost:8000/#/alerting/alert/{{.event_id}}|{{.title}}\u0026gt; \u0026lt;@username\u0026gt;\u0026quot; } }, { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;text\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Timestamp:* {{.issue_timestamp | datetime}}\u0026quot; } } ], \u0026quot;attachments\u0026quot;: [ {{range .results}} { \u0026quot;color\u0026quot;: {{if eq .priority \u0026quot;critical\u0026quot;}} \u0026quot;#C91010\u0026quot; {{else if eq .priority \u0026quot;high\u0026quot;}} \u0026quot;#EB4C21\u0026quot; {{else}} \u0026quot;#FFB449\u0026quot; {{end}}, \u0026quot;blocks\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;section\u0026quot;, \u0026quot;fields\u0026quot;: [ { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Cluster:* {{index .group_values 0}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Node:* {{index .group_values 1}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Threshold:* {{index .threshold 0}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Priority:* {{.priority}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Monitoring value:* {{.result_value}}\u0026quot; }, { \u0026quot;type\u0026quot;: \u0026quot;mrkdwn\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;*Disk usage:* {{.relation_values.a | format_bytes 2 | to_upper}}\u0026quot; } ] } ] }, {{end}} ] } More template syntaxClick me\n   "});index.add({'id':67,'href':'/en/docs/latest/gateway/tutorial/proxy_kibana/','title':"Adding a TLS and Basic Security for Kibana",'section':"Tutorials",'content':"Adding a TLS and Basic Security for Kibana #  If you have multiple Kibana versions or your Kibana version is out of date, or if you do not set TLS or identity, then anyone can directly access Kibana. You can use the INFINI Gateway to quickly fix this issue.\nUsing the HTTP Filter to Forward Requests #   - http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; Adding Authentication #   - basic_auth: valid_users: medcl: passwd Replacing Static Resources in the Router #   - method: - GET pattern: - \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: - replace_logo_flow Enabling TLS #   - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true Complete Configuration #  entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true flow: - name: logout_flow filter: - set_response: status: 401 body: \u0026quot;Success logout!\u0026quot; - drop: - name: replace_logo_flow filter: - redirect: uri: https://elasticsearch.cn/uploads/event/20211120/458c74ca3169260dbb2308dd06ef930a.png - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: \u0026quot;http\u0026quot; #https or http host: \u0026quot;192.168.3.188:5602\u0026quot; router: - name: my_router default_flow: default_flow rules: - method: - GET - POST pattern: - \u0026quot;/_logout\u0026quot; flow: - logout_flow - method: - GET pattern: - \u0026quot;/plugins/kibanaReact/assets/illustration_integrations_lightmode.svg\u0026quot; flow: - replace_logo_flow Effect #  To access Kibana through INFINI Gateway, you need to log in as follows:\nAfter login, you will find that resources in Kibana are also replaced. See the figure below.\nProspect #  We can explore other benefits of by using INFINI Gateway, for example, we can use the INFINI Gateway to replace the static assets, like logo, js, and CSS style in Kibana, or use the combination of js and CSS to dynamically add navigation and pages or advanced visualization.\n"});index.add({'id':68,'href':'/en/docs/latest/gateway/tutorial/proxy_elasticsearch/','title':"Enable HTTPS/TLS + Basic Auth for Elasticsearch easily",'section':"Tutorials",'content':"Enable HTTPS/TLS + Basic Auth for Elasticsearch easily #  If you have multiple Elasticsearch versions or your version is out of date, or if you do not set TLS or identity, then anyone can directly access Elasticsearch. You can use INFINI Gateway to quickly fix this issue.\nDefine an Elasticsearch resource #  Let\u0026rsquo;s define the Elasticsearch resources, config as bellow：\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The prod refer to http://192.168.3.201:9200\nAnd then, we will need to use a filter to forward requests to that Elasticsearch，which name is prod：\n - elasticsearch: elasticsearch: prod For more options of this elasticsearch filter, please refer to documentation： elasticsearch filter\nAdd basic_auth filter #  In order to perform access control of elasticsearch, we are using a basic_auth filter for example:\n - basic_auth: valid_users: medcl: passwd The only valid user defined in above configuration.\nEnable TLS #  Enable auth, but do not enable the TLS, it is useless, because HTTP is a clear text transmission protocol, which can easily leak the passwords, enable the TLS is quite simple, jut define a entry as below:\n - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true You can try visit https://localhost:8000 to access the prod Elasticsearch cluster now。\nNote that the listening address here is \u0026lsquo;0.0.0.0\u0026rsquo;, which means that the IP on all the network cards on the machine are listening. For security reasons, you may need to change to listen only on local addresses or specified NIC IP addresses.\nCompatible with HTTP access #  If there are legacy systems that cannot switch to HTTPS, we can leverage gateway to provide plain HTTP access too:\n - name: my_unsecure_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8001 tls: enabled: false By visit http://localhost:8001 you can access the prod cluster too。\nFull configuration #  elasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 entry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 tls: enabled: true - name: my_unsecure_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8001 tls: enabled: false flow: - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - elasticsearch: elasticsearch: prod router: - name: my_router default_flow: default_flow Showcase #  You will a valid user to access Elasticsearch now：\n"});index.add({'id':69,'href':'/en/docs/latest/gateway/tutorial/fix_count_in_search_response/','title':"Handle Count Structure of Different Elasticsearch Versions",'section':"Tutorials",'content':"Handle Count Structure of Different Elasticsearch Versions #  To optimize performance in Elasticsearch 7.0 and later versions, search result matches are not accurately counted and the search result response body is adjusted. This will inevitably cause incompatibility with existing code. How can the problem be fixed quickly?\nStructure Diff #  The search structure difference is as follows:\nThe search structure used by Elasticsearch before version 7.0 is as follows. total shows a specific value.\n{ \u0026quot;took\u0026quot;: 53, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 0, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] } } The search structure used by Elasticsearch 7.0 and later versions is as follows. total shows a group of description scope objects.\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;successful\u0026quot;: 1, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: 1, \u0026quot;hits\u0026quot;: [] } } Parameters Provided by Elasticsearch #  Elasticsearch 7.0 provides a parameter to accurately control the count. In other words, rest_total_hits_as_int=true can be added to the query request URL parameter so that the old structure is used. It is disabled by default.\nDocument URL: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html\nHowever, you need to modify the program to add this parameter, and you may need to adjust the back-end code, front-end paging, and presentation. The modification workload may not be small.\nUsing INFINI Gateway for Quick Fixing #  If you do not want to modify the program, you can use INFINI Gateway to quickly repair the query and add query parameters to a search query. In addition, INFINI Gateway can be used to limit the request sources for which query parameters are to be added. For example, request sources can be adjusted only for specific service calling parties. The following uses the curl command as an example to add query parameters only to queries from the curl debugging.\nentry: - name: es_entrypoint enabled: true router: default network: binding: 0.0.0.0:8000 router: - name: default default_flow: main_flow flow: - name: main_flow filter: - set_request_query_args: args: - rest_total_hits_as_int -\u0026gt; true when: and: - contains: _ctx.request.path: \u0026quot;_search\u0026quot; - equals: _ctx.request.header.User-Agent: \u0026quot;curl/7.54.0\u0026quot; - record: stdout: true - elasticsearch: elasticsearch: es-server - dump: response_body: true elasticsearch: - name: es-server enabled: true endpoints: - http://192.168.3.188:9206 The final effect is as follows:\nFigure 1 shows the search result returned after the gateway is accessed through a browser. Figure 2 shows the search result returned by the curl command. The User-Agent header information can match the curl command and only parameters are added to the search conditions to avoid affecting other requests.\n"});index.add({'id':70,'href':'/en/docs/latest/gateway/tutorial/es-hadoop_integration/','title':"Integrate with Elasticsearch-Hadoop",'section':"Tutorials",'content':"Integrate with Elasticsearch-Hadoop #  Elasticsearch-Hadoop utilizes a seed node to access all back-end Elasticsearch nodes by default. The hotspots and requests may be improperly allocated. To improve the resource utilization of back-end Elasticsearch nodes, you can implement precision routing for the access to Elasticsearch nodes through INFINI Gateway.\nWrite Acceleration #  If you import data by using Elasticsearch-Hadoop, you can modify the following parameters of Elasticsearch-Hadoop to access INFINI Gateway, so as to improve the write throughput:\n   Name Type Description     es.nodes string List of addresses used to access the gateway, for example, localhost:8000,localhost:8001   es.nodes.discovery bool When it is set to false, the sniff mode is not adopted and only the configured back-end nodes are accessed.   es.nodes.wan.only bool When it is set to true, it indicates the proxy mode, in which data is forcibly sent through the gateway address.   es.batch.size.entries int Batch document quantity. Set the parameter to a larger value to improve throughput, for example, 5000.   es.batch.size.bytes string Batch transmission size. Set the parameter to a larger value to improve throughput, for example, 20mb.   es.batch.write.refresh bool Set it to false to prevent active refresh and improve throughput.    Related Link #    Elasticsearch-Hadoop Configuration Parameter Document  "});index.add({'id':71,'href':'/en/docs/latest/gateway/tutorial/prometheus_integration/','title':"Integration with Prometheus",'section':"Tutorials",'content':"Integration with Prometheus #  Infini Gateway supports outputting metrics in Prometheus format, which is convenient for integration with Prometheus.\nStats API #  Access Gateway\u0026rsquo;s API endpoint, with URL parameter as below:\nhttp://localhost:2900/stats?format=prometheus ➜ ~ curl http://localhost:2900/stats\\?format\\=prometheus buffer_fasthttp_resbody_buffer_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 1 buffer_stats_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 7 buffer_stats_max_count{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 system_cpu{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 buffer_bulk_request_docs_acquired{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 1 buffer_fasthttp_resbody_buffer_inuse{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 stats_gateway_request_bytes{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 0 system_mem{type=\u0026quot;gateway\u0026quot;, ip=\u0026quot;192.168.3.23\u0026quot;, name=\u0026quot;Orchid\u0026quot;, id=\u0026quot;cbvjphrq50kcnsu2a8v0\u0026quot;} 31473664 ... By providing parameter format=prometheus, the Prometheus format will be outputted.\nConfigure Prometheus #  Edit config file: prometheus.yml\n# my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: # - alertmanager:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: # - \u0026quot;first_rules.yml\u0026quot; # - \u0026quot;second_rules.yml\u0026quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: - job_name: \u0026quot;prometheus\u0026quot; scrape_interval: 5s # metrics_path defaults to '/metrics' metrics_path: /stats params: format: ['prometheus'] # scheme defaults to 'http'. static_configs: - targets: [\u0026quot;localhost:2900\u0026quot;] labels: group: 'infini' Start Prometheus #  The metrics success scraped.\nThen you can continuously check the running status of the gateway.\n"});index.add({'id':72,'href':'/en/docs/latest/console/resources/','title':"Other Resources",'section':"INFINI Console",'content':"Other Resources #  Useful external resources related to INFINI Console are provided here.\nArticles #  Videos #  "});index.add({'id':73,'href':'/en/docs/latest/gateway/resources/','title':"Other Resources",'section':"INFINI Gateway",'content':"Other Resources #  Useful external resources related to INFINI Gateway are provided here.\nProject #    INFINI Gateway on Kubernetes HA (HA Solution)  Articles #  Videos #  "});index.add({'id':74,'href':'/en/docs/latest/gateway/tutorial/routing_to_cluser_by_index/','title':"Unified access indexes from different clusters in Kibana",'section':"Tutorials",'content':"Unified access indices from different clusters in Kibana #  Now there is such a demand, customers need to divide the data according to the business dimension, the index is split into three different clusters, to split the large cluster into multiple small clusters have many benefits, such as reduced coupling, bringing benefits to cluster availability and stability, but also to avoid the impact of a single business hotspot to affect other services, although splitting the cluster is a very common way to play, but the management is not so convenient, especially when querying data, it may be need to access the three sets of clusters separately APIs, even to switch between three different sets of Kibana to access the cluster\u0026rsquo;s data, is there a way to seamlessly unite them together?\nA gateway! #  The answer is naturally yes, by switching the Elasticsearch address of Kibana to the address of the INFINI Gateway, we can intelligently route requests according to the index, that is, when accessing different business indexes, they will be intelligently routed to different clusters, as shown in the following figure:\nAbove, we have three different indexes：\n apm-* erp-* mall-*  Each corresponds to three different sets of Elasticsearch clusters:\n ES1-APM ES2-ERP ES3-MALL  Now let\u0026rsquo;s see how to configure the INFINI Gateway to meet this business requirements:\nConfigure clusters #  First configure the connection information for the three clusters.\nelasticsearch: - name: es1-apm enabled: true endpoints: - http://192.168.3.188:9206 - name: es2-erp enabled: true endpoints: - http://192.168.3.188:9207 - name: es3-mall enabled: true endpoints: - http://192.168.3.188:9208 Configure Flow #  then, we define three flows that are used to access three different Elasticsearch clusters, as shown below:\nflow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1-apm - name: es2-flow filter: - elasticsearch: elasticsearch: es2-erp - name: es3-flow filter: - elasticsearch: elasticsearch: es3-mall Then define a flow for path rule and forwarding, as follows:\n - name: default-flow filter: - switch: remove_prefix: false path_rules: - prefix: \u0026quot;apm-\u0026quot; flow: es1-flow - prefix: \u0026quot;erp-\u0026quot; flow: es2-flow - prefix: \u0026quot;mall-\u0026quot; flow: es3-flow - flow: #default flow flows: - es1-flow Match different indexes based on the index prefix in the request path and forward to different flows.\nConfigure Router #  Next, we define the routing information as follows:\nrouter: - name: my_router default_flow: default-flow Point to the default flow defined above to unify the processing of requests.\nConfigure Entrypoint #  Finally, we define a service that listening on port 8000 to provide unified access to Kibana, as follows:\nentry: - name: es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 Full Configuration #  The final complete configuration is as follows:\npath.data: data path.logs: log entry: - name: es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 flow: - name: default-flow filter: - switch: remove_prefix: false path_rules: - prefix: \u0026quot;apm-\u0026quot; flow: es1-flow - prefix: \u0026quot;erp-\u0026quot; flow: es2-flow - prefix: \u0026quot;mall-\u0026quot; flow: es3-flow - flow: #default flow flows: - es1-flow - name: es1-flow filter: - elasticsearch: elasticsearch: es1-apm - name: es2-flow filter: - elasticsearch: elasticsearch: es2-erp - name: es3-flow filter: - elasticsearch: elasticsearch: es3-mall router: - name: my_router default_flow: default-flow elasticsearch: - name: es1-apm enabled: true endpoints: - http://192.168.3.188:9206 - name: es2-erp enabled: true endpoints: - http://192.168.3.188:9207 - name: es3-mall enabled: true endpoints: - http://192.168.3.188:9208 Start Gateway #  Start the gateway as follows:\n➜ gateway git:(master) ✗ ./bin/gateway -config sample-configs/elasticsearch-route-by-index.yml ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 2022-04-20 08:23:56, 2023-12-31 10:10:10, 51650a5c3d6aaa436f3c8a8828ea74894c3524b9 [04-21 13:41:21] [INF] [app.go:174] initializing gateway. [04-21 13:41:21] [INF] [app.go:175] using config: /Users/medcl/go/src/infini.sh/gateway/sample-configs/elasticsearch-route-by-index.yml. [04-21 13:41:21] [INF] [instance.go:72] workspace: /Users/medcl/go/src/infini.sh/gateway/data/gateway/nodes/c9bpg0ai4h931o4ngs3g [04-21 13:41:21] [INF] [app.go:283] gateway is up and running now. [04-21 13:41:21] [INF] [api.go:262] api listen at: http://0.0.0.0:2900 [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es1-apm] hosts: [] =\u0026gt; [192.168.3.188:9206] [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es2-erp] hosts: [] =\u0026gt; [192.168.3.188:9207] [04-21 13:41:21] [INF] [reverseproxy.go:255] elasticsearch [es3-mall] hosts: [] =\u0026gt; [192.168.3.188:9208] [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es2-erp] is available [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es1-apm] is available [04-21 13:41:21] [INF] [entry.go:312] entry [es_entry] listen at: http://0.0.0.0:8000 [04-21 13:41:21] [INF] [module.go:116] all modules are started [04-21 13:41:21] [INF] [actions.go:349] elasticsearch [es3-mall] is available [04-21 13:41:55] [INF] [reverseproxy.go:255] elasticsearch [es1-apm] hosts: [] =\u0026gt; [192.168.3.188:9206] After the gateway successfully started, you can access the target Elasticsearch cluster through the gateway\u0026rsquo;s IP+ port 8000.\nTesting #  Let\u0026rsquo;s start with the API access test, as follows:\n➜ ~ curl http://localhost:8000/apm-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /apm-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 05:45:44 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 162 \u0026lt; X-elastic-product: Elasticsearch \u0026lt; X-Backend-Cluster: es1-apm \u0026lt; X-Backend-Server: 192.168.3.188:9206 \u0026lt; X-Filters: filters-\u0026gt;elasticsearch \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:142,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% You can see that apm-2022 points to the backend ES1-APM cluster.\nTo continue testing, access to the ERP index as follows:\n➜ ~ curl http://localhost:8000/erp-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /erp-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 06:24:46 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 161 \u0026lt; X-Backend-Cluster: es2-erp \u0026lt; X-Backend-Server: 192.168.3.188:9207 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:12,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:{\u0026quot;value\u0026quot;:0,\u0026quot;relation\u0026quot;:\u0026quot;eq\u0026quot;},\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% Great!\nLet\u0026rsquo;s continue testing, access to the mall index as follows:\n➜ ~ curl http://localhost:8000/mall-2022/_search -v * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET /mall-2022/_search HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Date: Thu, 21 Apr 2022 06:25:08 GMT \u0026lt; content-type: application/json; charset=UTF-8 \u0026lt; Content-Length: 134 \u0026lt; X-Backend-Cluster: es3-mall \u0026lt; X-Backend-Server: 192.168.3.188:9208 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;took\u0026quot;:8,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:5,\u0026quot;successful\u0026quot;:5,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:0,\u0026quot;max_score\u0026quot;:null,\u0026quot;hits\u0026quot;:[]}}% Perfect!\nAnother Option #  Besides using the switch filter, it is possible to use the path rules of the router itself, as shown in the following example configuration:\nflow: - name: default_flow filter: - echo: message: \u0026quot;hello world\u0026quot; - name: mall_flow filter: - echo: message: \u0026quot;hello mall indices\u0026quot; - name: apm_flow filter: - echo: message: \u0026quot;hello apm indices\u0026quot; - name: erp_flow filter: - echo: message: \u0026quot;hello erp indices\u0026quot; router: - name: my_router default_flow: default_flow rules: - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/apm-{suffix:.*}/\u0026quot; - \u0026quot;/apm-{suffix:.*}/{any:.*}\u0026quot; flow: - apm_flow - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/erp-{suffix:.*}/\u0026quot; - \u0026quot;/erp-{suffix:.*}/{any:.*}\u0026quot; flow: - erp_flow - method: - \u0026quot;*\u0026quot; pattern: - \u0026quot;/mall-{suffix:.*}/\u0026quot; - \u0026quot;/mall-{suffix:.*}/{any:.*}\u0026quot; flow: - mall_flow INFINI Gateway has many powerful features, and there are many ways to achieve your need, go explore it by yourself.\nModify Kibana Configuration #  Modify the Kibana configuration file kibana.yml, replace the address of Elasticsearch with the gateway address (HTTP: 192.168.3.200/8000), as shown below:\nelasticsearch.hosts: [\u0026quot;http://192.168.3.200:8000\u0026quot;] Restart the Kibana。\nVisit Kibana #  As you can see, in the Kibana developer tool we can already perform read and write operations from three different clusters as if it were one cluster.\nConclusion #  Through the INFINI Gateway, we can be very flexible for online traffic editing, dynamic combine requests of different cluster operations together on the fly.\n"});index.add({'id':75,'href':'/en/docs/latest/gateway/tutorial/path_rewrite_by_javascript/','title':"Use JavaScript for complex query rewriting",'section':"Tutorials",'content':"Use JavaScript for complex query rewriting #  Here is a use case：\n How does the gateway support cross-cluster search? I want to achieve: the input search request is lp:9200/index1/_search these indices are on three clusters, so need search across these clusters, how to use the gateways to switch to lp:9200/cluster01:index1,cluster02,index1,cluster03:index1/_search? we don\u0026rsquo;t want to change the application side, there are more than 100 indices, the index name not strictly named as index1, may be multiple indices together。\n Though INFINI Gateway provide a filter content_regex_replace can implement regular expression replacement, but in this case the variable need to replace with multi parameters. It is more complex, there is no direct way to implement by regexp match and replace, so how do we do that?\nJavascript filter #  The answer is yes, we do have a way, in the above case, in theory we only need to match the index name index1 and replace 3 times by adding prefix cluster01:, cluster02: and cluster03:,\nBy using INFINI Gateway\u0026rsquo;s JavaScript filter, we can implement this easily.\nActually no matter how complex the business logic is, it can be implemented through the scripts, not one line of script, then two lines.\nDefine the scripts #  Let\u0026rsquo;s create a script file under the scripts subdirectory of the gateway data directory, as follows:\n➜ gateway ✗ tree data data └── gateway └── nodes └── c9bpg0ai4h931o4ngs3g ├── kvdb ├── queue ├── scripts │ └── index_path_rewrite.js └── stats The content of this script is as follows:\nfunction process(context) { var originalPath = context.Get(\u0026quot;_ctx.request.path\u0026quot;); var matches = originalPath.match(/\\/?(.*?)\\/_search/) var indexNames = []; if(matches \u0026amp;\u0026amp; matches.length \u0026gt; 1) { indexNames = matches[1].split(\u0026quot;,\u0026quot;) } var resultNames = [] var clusterNames = [\u0026quot;cluster01\u0026quot;, \u0026quot;cluster02\u0026quot;] if(indexNames.length \u0026gt; 0) { for(var i=0; i\u0026lt;indexNames.length; i++){ if(indexNames[i].length \u0026gt; 0) { for(var j=0; j\u0026lt;clusterNames.length; j++){ resultNames.push(clusterNames[j]+\u0026quot;:\u0026quot;+indexNames[i]) } } } } if (resultNames.length\u0026gt;0){ var newPath=\u0026quot;/\u0026quot;+resultNames.join(\u0026quot;,\u0026quot;)+\u0026quot;/_search\u0026quot;; context.Put(\u0026quot;_ctx.request.path\u0026quot;,newPath); } } Like normal JavaScript, define a specific function process to handle context information inside the request, _ctx.request.path is a variable of the gateway\u0026rsquo;s built-in context to get the path of the request, and then use function context.Get(\u0026quot;_ctx.request.path\u0026quot;) to access this field inside the script.\nIn the script we used general regular expression for matching and characters process, did some character stitching, got a new path variable newPath , and finally used context.Put(\u0026quot;_ctx.request.path\u0026quot;,newPath) to update the request path back to context.\nFor more information about fields of request context please visit: Request Context\nGateway Configuration #  Next, create a gateway configuration and reference the script using a javascript filter as follows\nentry: - name: my_es_entry enabled: true router: my_router max_concurrency: 10000 network: binding: 0.0.0.0:8000 flow: - name: default_flow filter: - dump: context: - _ctx.request.path - javascript: file: index_path_rewrite.js - dump: context: - _ctx.request.path - elasticsearch: elasticsearch: dev router: - name: my_router default_flow: default_flow elasticsearch: - name: dev enabled: true schema: http hosts: - 192.168.3.188:9206 In the above example, a javascript filter with file specified as index_path_rewrite.js, and two dump filters are used for debugging, also used one elasticsearch filter to forward requests to ElasticSearch for queries.\nStart Gateway #  Let\u0026rsquo;s start the gateway to have a test:\n➜ gateway ✗ ./bin/gateway ___ _ _____ __ __ __ _ / _ \\ /_\\ /__ \\/__\\/ / /\\ \\ \\/_\\ /\\_/\\ / /_\\///_\\\\ / /\\/_\\ \\ \\/ \\/ //_\\\\\\_ _/ / /_\\\\/ _ \\/ / //__ \\ /\\ / _ \\/ \\ \\____/\\_/ \\_/\\/ \\__/ \\/ \\/\\_/ \\_/\\_/ [GATEWAY] A light-weight, powerful and high-performance elasticsearch gateway. [GATEWAY] 1.0.0_SNAPSHOT, 2022-04-18 07:11:09, 2023-12-31 10:10:10, 8062c4bc6e57a3fefcce71c0628d2d4141e46953 [04-19 11:41:29] [INF] [app.go:174] initializing gateway. [04-19 11:41:29] [INF] [app.go:175] using config: /Users/medcl/go/src/infini.sh/gateway/gateway.yml. [04-19 11:41:29] [INF] [instance.go:72] workspace: /Users/medcl/go/src/infini.sh/gateway/data/gateway/nodes/c9bpg0ai4h931o4ngs3g [04-19 11:41:29] [INF] [app.go:283] gateway is up and running now. [04-19 11:41:30] [INF] [api.go:262] api listen at: http://0.0.0.0:2900 [04-19 11:41:30] [INF] [entry.go:312] entry [my_es_entry] listen at: http://0.0.0.0:8000 [04-19 11:41:30] [INF] [module.go:116] all modules are started [04-19 11:41:30] [INF] [actions.go:349] elasticsearch [dev] is available Testing #  Run the following query to verify the query results, as shown below:\ncurl localhost:8000/abc,efg/_search You can see debugging information output by the gateway through the dump filter\n---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search ---- DUMPING CONTEXT ---- _ctx.request.path : /cluster01:abc,cluster02:abc,cluster01:efg,cluster02:efg/_search The query criteria have been rewritten according to our requirements,Nice!\nRewrite the DSL #  All right, we did change the request url, is that also possible to change the request body, like the search QueryDSL?\nLet\u0026rsquo;s do this:\nfunction process(context) { var originalDSL = context.Get(\u0026quot;_ctx.request.body\u0026quot;); if (originalDSL.length \u0026gt;0){ var jsonObj=JSON.parse(originalDSL); jsonObj.size=123; jsonObj.aggs= { \u0026quot;test1\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;abc\u0026quot;, \u0026quot;size\u0026quot;: 10 } } } context.Put(\u0026quot;_ctx.request.body\u0026quot;,JSON.stringify(jsonObj)); } } Testing:\n curl -XPOST localhost:8000/abc,efg/_search -d'{\u0026quot;query\u0026quot;:{}}' Output:\n---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search _ctx.request.body : {\u0026quot;query\u0026quot;:{}} [04-19 18:14:24] [INF] [reverseproxy.go:255] elasticsearch [dev] hosts: [] =\u0026gt; [192.168.3.188:9206] ---- DUMPING CONTEXT ---- _ctx.request.path : /abc,efg/_search _ctx.request.body : {\u0026quot;query\u0026quot;:{},\u0026quot;size\u0026quot;:123,\u0026quot;aggs\u0026quot;:{\u0026quot;test1\u0026quot;:{\u0026quot;terms\u0026quot;:{\u0026quot;field\u0026quot;:\u0026quot;abc\u0026quot;,\u0026quot;size\u0026quot;:10}}}} Look, we just unlock the new world, agree?\nConclusion #  By using the Javascript filter in INFINI Gateway, it can be very flexible and easily to perform the complex logical operations and rewrite the Elasticsearch QueryDSL to meet your business needs.\n"});index.add({'id':76,'href':'/en/docs/latest/gateway/references/config/','title':"Other Configuration",'section':"Reference",'content':"Other Configuration #  System Configuration #  System configuration is used to set the configurations of INFINI Gateway.\n   Name Type Description     path.data string Data directory, which is data by default.   path.logs string Log directory, which is log by default.   path.configs string Configuration directory, which is config by default.   log.level string Log level, which is info by default.   log.debug bool Whether to enable the debugging mode, which is false by default. After the debugging mode is enabled, if an exception occurs, the program exits directly and prints the complete stack, which is only used for debugging and locating faults. Do not enable it in the production environment because data may be lost.   log.disable_file_output bool Whether to disable the local log file output, which is set to false by default. Enable this parameter if local log output is not required in the container environment.   allow_multi_instance bool Whether to start multiple gateway instances on a single machine, which is set to false by default.   skip_instance_detect bool Whether to skip the detect of running instances, which is set to false by default   max_num_of_instances int Maximum number of gateway instances, which is set to 5 by default.   configs.auto_reload bool Whether dynamic loading configured in path.configs is supported    Templated Configuration #  Example：\nconfigs.template: - name: \u0026quot;es_gw1\u0026quot; path: ./sample-configs/config_template.tpl variable: name: \u0026quot;es_gw1\u0026quot; binding_host: \u0026quot;0.0.0.0:8000\u0026quot; tls_on_entry: true elasticsearch_endpoint: \u0026quot;http://localhost:9200\u0026quot;    Name Type Description     configs.template array Configurations   configs.template[].name string name of the config   configs.template[].path string path to the template file   configs.template[].variable map redefine variable value of template, variable used in template: $[[variable_name]]    Configuring the Local Disk Queue #  Example:\ndisk_queue: upload_to_s3: true s3: server: my_blob_store location: cn-beijing-001 bucket: infini-store max_bytes_per_file: 102400    Name Type Description     disk_queue.min_msg_size int Minimum limit on the bytes of a single message sent to a queue. The default value is 1.   disk_queue.max_msg_size int Maximum limit on the bytes of a single message sent to a queue. The default value is 104857600, that is, 100 MB.   disk_queue.sync_every_records int Number of records, after which one disk synchronization is performed. The default value is 1000.   disk_queue.sync_timeout_in_ms int Interval for performing one disk synchronization operation. The default value is 1000 ms.   disk_queue.max_bytes_per_file int Maximum size of a single file in the local disk queue. When the size of a file exceeds this value, a new file is automatically generated. The default value is 104857600, that is, 100 MB.   disk_queue.max_used_bytes int Maximum allowable storage space in the local disk queue.   disk_queue.warning_free_bytes int Idle storage space when the disk usage reaches the alarm threshold. The default value is 10737418240, that is, 10 GB.   disk_queue.reserved_free_bytes int Protection value of the idle storage space of the disk. The disk becomes read-only when this value is reached. The default value is 5368709120, that is, 5 GB.   disk_queue.upload_to_s3 bool Whether to upload disk queue files to S3. The default value is false.   disk_queue.s3.async bool Whether to asynchronously upload files to S3   disk_queue.s3.server string ID of the S3 server   disk_queue.s3.location string Location of the S3 server   disk_queue.s3.bucket string Bucket of the S3 server   disk_queue.retention.max_num_of_local_files int Maximum number of latest files sorted by creation time that can be kept on the local disk after files are uploaded to the S3 server. The default value is 10.   disk_queue.compress.segment.enabled bool Whether to enable compress on segment file. The default value is false.    Configuring the S3 Server #  Example:\ns3: my_blob_store: endpoint: \u0026quot;192.168.3.188:9000\u0026quot; access_key: \u0026quot;admin\u0026quot; access_secret: \u0026quot;gogoaminio\u0026quot;    Name Type Description     s3.[id].endpoint string Address of the S3 server   s3.[id].access_key string Key of the S3 server   s3.[id].access_secret string Secret key of the S3 server   s3.[id].token string Token information about the S3 server   s3.[id].ssl bool Whether TLS is enabled on the S3 server   s3.[id].skip_insecure_verify bool Whether to skip TLS verification    "});index.add({'id':77,'href':'/en/docs/latest/gateway/references/filters/basic_auth/','title':"basic_auth",'section':"Online Filter",'content':"basic_auth #  Description #  The basic_auth filter is used to verify authentication information of requests. It is applicable to simple authentication.\nConfiguration Example #  A simple example is as follows:\nflow: - name: basic_auth filter: - basic_auth: valid_users: medcl: passwd medcl1: abc ... Parameter Description #     Name Type Description     valid_users map Username and password    "});index.add({'id':78,'href':'/en/docs/latest/gateway/references/processors/bulk_indexing/','title':"bulk_indexing",'section':"Offline Processor",'content':"bulk_indexing #  Description #  The bulk_indexing processor is used to asynchronously consume bulk requests in queues.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - bulk_indexing: bulk_size_in_mb: 1 queue_selector.labels: type: bulk_reshuffle level: cluster Parameter Description #     Name Type Description     elasticsearch string The default Elasticsearch cluster ID, which will be used if elasticsearch is not specified in the queue Labels   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   max_connection_per_node int Maximum number of connections allowed by the target node. The default value is 1.   max_worker_size int The maximum size of workers allowed to run at the same time, default 10   bulk.batch_size_in_kb int Size of a bulk request, in KB.   bulk.batch_size_in_mb int Size of a bulk request, in MB.   bulk.batch_size_in_docs int Num of docs in bulk request, default 1000   bulk.compress bool Whether to enable request compression.   bulk.retry_delay_in_seconds int Waiting time for request retry.   bulk.reject_retry_delay_in_seconds int Waiting time for request rejection.   bulk.max_retry_times int Maximum retry count.   bulk.failure_queue string Queue for storing requests that fail because of a back-end failure.   bulk.invalid_queue string Queue for storing requests, for which 4xx is returned because of invalid requests.   bulk.dead_letter_queue string Request queue, for which the maximum retry count is exceeded.   bulk.safety_parse bool Whether to enable secure parsing, that is, no buffer is used and memory usage is higher. The default value is true.   bulk.doc_buffer_size bool Maximum document buffer size for the processing of a single request. You are advised to set it to be greater than the maximum size of a single document. The default value is 256*1024.   queue_selector.labels map A group of queues filtered by label, in which data needs to be consumed. alias queues   queue_selector.ids array Specifies the UUID of the queue to consume, an array of string   queue_selector.keys array Specifies the unique Key path of the queue to consume, string array   waiting_after array Whether to wait for the specified queue to finish consumption before starting consumption, UUID of the queue, string array   detect_active_queue bool Whether to automatically detect new queues that meet the conditions, default true   detect_interval bool The time interval for automatically detecting new queues that meet the conditions, in milliseconds, default 5000   num_of_slices int Threads consuming a single queue in parallel, maximum slice size at runtime   slices array Allowed slice numbers, int array   skip_info_missing bool Whether to ignore queue data consumption when conditions are not met, for example, the node, index, or shard information does not exist, that is, whether to consume queue data after information is obtained. The default value is false. Otherwise, one Elasticsearch node is selected to send requests.   skip_empty_queue bool Whether to skip consumption of empty queue, default true   consumer.source string consumer source   consumer.id string consumer UUID   consumer.name string consumer name   consumer.group string consumer group name   consumer.fetch_min_bytes int Minimum size in bytes to pull messages, default 1   consumer.fetch_max_bytes int The maximum byte size of the pull message, the default is 10485760, which is 10MB   consumer.fetch_max_messages int Pull the maximum number of messages, default 1   consumer.fetch_max_wait_ms int Pull maximum waiting time, in milliseconds, default 10000    "});index.add({'id':79,'href':'/en/docs/latest/gateway/references/filters/bulk_request_mutate/','title':"bulk_request_mutate",'section':"Online Filter",'content':"bulk_request_mutate #  Description #  The bulk_request_mutate filter is used to mutate bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_request_mutate filter: - bulk_request_mutate: fix_null_id: true generate_enhanced_id: true # fix_null_type: true # default_type: m-type # default_index: m-index # index_rename: # \u0026quot;*\u0026quot;: index-new # index1: index-new # index2: index-new # index3: index3-new # index4: index3-new # medcl-dr3: index3-new # type_rename: # \u0026quot;*\u0026quot;: type-new # type1: type-new # type2: type-new # doc: type-new # doc1: type-new ... Parameter Description #     Name Type Description     fix_null_type bool Whether to fix a request that does not carry _type. It is used in collaboration with the default_type parameter.   fix_null_id bool Whether to fix a request that does not carry _id and generate a random ID, for example, c616rhkgq9s7q1h89ig0   remove_type bool Whether to remove the _type parameter. Elasticsearch versions higher than 8.0 do not support the _type parameter.   generate_enhanced_id bool Whether to generate an enhanced ID, such as c616rhkgq9s7q1h89ig0-1635937734071093-10.   default_index string Default index name, which is used if no index name is specified in metadata   default_type string Default document type, which is used if no document type is specified in metadata   index_rename map Index name used for renaming. You can use * to overwrite all index names.   type_rename map Type used for renaming. You can use * to overwrite all type names.   pipeline string pipeline parameter of a specified bulk request   remove_pipeline bool Whether to remove the pipeline parameter from the bulk request   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.    "});index.add({'id':80,'href':'/en/docs/latest/gateway/references/filters/bulk_reshuffle/','title':"bulk_reshuffle",'section':"Online Filter",'content':"bulk_reshuffle #  Description #  The bulk_reshuffle filter is used to parse batch requests of Elasticsearch based on document, sort out documents as needed, and archive and store them in queues. After documents are stored, the filter can rapidly return service requests, thereby decoupling front-end writing from back-end Elasticsearch clusters. The bulk_reshuffle filter needs to be used in combination with offline pipeline consumption tasks.\nWhen passing through queues generated by the bulk_reshuffle filter, metadata carries \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; and Elasticsearch cluster information such as \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, by default. You can call APIs on the gateway to check metadata defined in queues. See the following example.\ncurl http://localhost:2900/queue/stats { \u0026quot;queue\u0026quot;: { \u0026quot;disk\u0026quot;: { \u0026quot;async_bulk-cluster##dev\u0026quot;: { \u0026quot;depth\u0026quot;: 0, \u0026quot;metadata\u0026quot;: { \u0026quot;source\u0026quot;: \u0026quot;dynamic\u0026quot;, \u0026quot;id\u0026quot;: \u0026quot;c71f7pqi4h92kki4qrvg\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;async_bulk-cluster##dev\u0026quot;, \u0026quot;label\u0026quot;: { \u0026quot;elasticsearch\u0026quot;: \u0026quot;dev\u0026quot;, \u0026quot;level\u0026quot;: \u0026quot;cluster\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bulk_reshuffle\u0026quot; } } } } } } Node-Level Asynchronous Submission #  INFINI Gateway is capable of locally calculating the target storage location of a back-end Elasticsearch cluster corresponding to each index document so as to precisely locate requests. A batch of bulk requests may contain the data of multiple back-end nodes. The bulk_reshuffle filter is used to shuffle normal bulk requests and reassemble them based on target nodes or shards. The purpose is to prevent Elasticsearch nodes from distributing received requests, so as to reduce the traffic and load between Elasticsearch clusters. The filter also prevents a single node from becoming a bottleneck and ensures balanced processing of all data nodes, thereby improving the overall index throughput of clusters.\nDefining a Flow #  A simple example is as follows:\nflow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: node #cluster,node,shard,partition - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s The above configuration indicates that bulk requests will be split and reassembled based on the target nodes corresponding to index documents. Data is sent to local disk queues first and then consumed and submitted through separate tasks to the target Elasticsearch nodes.\nThe benefit of this filter is that a failure occurring on the back-end Elasticsearch cluster will not affect indexing operations because requests are stored in disk queues of the gateway and the front-end indexing is decoupled from back-end clusters. Therefore, when the back-end Elasticsearch cluster encounters a failure, restarts, or initiates version upgrade, normal index operations will not be affected.  Configuring a Consumption Pipeline #  After the gateway sends requests to the disk, a consumption queue pipeline needs to be configured as follows to submit data:\npipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: bulk_size_in_mb: 10 #in MB queues: type: bulk_reshuffle level: node One pipeline task named bulk_request_ingest is used and the filter conditions for queues of to-be-subscribed targets are type: bulk_reshuffle and level: node. You can also set the batch size for bulk submission. In this way, node-level requests received by INFINI Gateway will be automatically sent to the corresponding Elasticsearch node.\nShard-Level Asynchronous Submission #  Shard-level asynchronous submission is suitable for scenarios in which the data amount of a single index is large and needs to be processed independently. An index is split into shards and then bulk requests are submitted in the form of shards, which further improves the processing efficiency of back-end Elasticsearch nodes.\nThe configuration is as follows:\nDefining a Flow #  flow: - name: online_indexing_merge filter: - bulk_reshuffle: elasticsearch: prod level: shard - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Set the assembly and disassembly level to the shard type.\nDefining a Pipeline #  pipeline: - name: bulk_request_ingest auto_start: true processor: - bulk_indexing: queues: type: bulk_reshuffle level: shard bulk_size_in_mb: 1 #in MB Compared with the preceding node-level configuration, the level parameter is modified to listen to shard-type disk queues. If there are many indexes, excess local disk queues will cause extra overhead. You are advised to enable this mode only for specific indexes whose throughput needs to be optimized.\nParameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster instance.   level string Shuffle level of a request, that is, cluster level. The default value is cluster. It can be set to cluster, node, index, or shard.   partition_size int Maximum partition size. Partitioning is performed by document _id on the basis of level.   fix_null_id bool Whether to automatically generate a random UUID if no document ID is specified in the bulk index request document. It is applicable to data of the log type. The default value is true.   index_stats_analysis bool Whether to record index name statistics to request logs. The default value is true.   action_stats_analysis bool Whether to record bulk request statistics to request logs. The default value is true.   doc_buffer_size int Buffer size of a processing document. If a single index document is very large, the value needs to be greater than the document size. The default value is 262144, or, 256 KB.   shards array Index shards that can be processed. The value is a character array, for example, \u0026quot;0\u0026quot;. All shards are processed by default, and you can set specific shards to be processed.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.    "});index.add({'id':81,'href':'/en/docs/latest/gateway/references/filters/bulk_response_process/','title':"bulk_response_process",'section':"Online Filter",'content':"bulk_response_process #  Description #  The bulk_response_process filter is used to process bulk requests of Elasticsearch.\nConfiguration Example #  A simple example is as follows:\nflow: - name: bulk_response_process filter: - bulk_response_process: success_queue: \u0026quot;success_queue\u0026quot; tag_on_success: [\u0026quot;commit_message_allowed\u0026quot;] Parameter Description #     Name Type Description     invalid_queue string Name of the queue that saves an invalid request. It is mandatory.   failure_queue string Name of the queue that saves a failed request. It is mandatory.   save_partial_success_requests bool Whether to save partially successful requests in bulk requests. The default value is false.   success_queue string Queue that saves partially successful requests in the bulk requests   doc_buffer_size int Size of the buffer for processing a single document. The default value is 25 KB, or, 262144.   continue_on_error bool Whether to continue to execute subsequent filters after an error occurs on a bulk request. The default value is false.   message_truncate_size int Truncation length of a bulk request error log. The default value is 1024.   safety_parse bool Whether to use a secure bulk metadata parsing method. The default value is true.   doc_buffer_size int Buffer size when an insecure bulk metadata parsing method is adopted. The default value is 256 * 1024.   tag_on_success array Specified tag to be attached to request context after all bulk requests are processed.   tag_on_error array Specified tag to be attached to request context after an error occurs on a request.   tag_on_partial array Specified tag to be attached to request context after requests in a bulk request are partially executed successfully.   tag_on_failure array Specified tag to be attached to request context after some requests in a bulk request fail (retry is supported).   tag_on_invalid array Specified tag to be attached to request context after an invalid request error occurs.    "});index.add({'id':82,'href':'/en/docs/latest/gateway/references/filters/cache/','title':"cache",'section':"Online Filter",'content':"cache #  Description #  The cache filter is composed of the get_cache and set_cache filters, which need to be used in combination. The cache filter is used to cache accelerated queries, prevent repeated requests, and reduce the query pressure of back-end clusters.\nget_cache Filter #  The get_cache filter is used to acquire previous messages from the cache and return them to the client, without needing to access the back-end Elasticsearch. It is intended to cache hotspot data.\nA configuration example is as follows:\nflow: - name: get_cache filter: - get_cache: pass_patterns: [\u0026quot;_cat\u0026quot;,\u0026quot;scroll\u0026quot;, \u0026quot;scroll_id\u0026quot;,\u0026quot;_refresh\u0026quot;,\u0026quot;_cluster\u0026quot;,\u0026quot;_ccr\u0026quot;,\u0026quot;_count\u0026quot;,\u0026quot;_flush\u0026quot;,\u0026quot;_ilm\u0026quot;,\u0026quot;_ingest\u0026quot;,\u0026quot;_license\u0026quot;,\u0026quot;_migration\u0026quot;,\u0026quot;_ml\u0026quot;,\u0026quot;_rollup\u0026quot;,\u0026quot;_data_stream\u0026quot;,\u0026quot;_open\u0026quot;, \u0026quot;_close\u0026quot;] Parameter Description #     Name Type Description     pass_patterns string Rule for ignoring the cache for a request. The cache is skipped when the URL contains any defined keyword.    set_cache Filter #  The set_cache filter is used to cache results returned through back-end query. Expiration time can be set for the cache.\nA configuration example is as follows:\nflow: - name: get_cache filter: - set_cache: min_response_size: 100 max_response_size: 1024000 cache_ttl: 30s max_cache_items: 100000 Parameter Description #     Name Type Description     cache_type string Cache type. It can be set to ristretto, ccache, or redis, and the default value is ristretto.   cache_ttl string Expiration time of the cache. The default value is 10s.   async_search_cache_ttl string Expiration time of the cache for storing asynchronous request results. The default value is 10m.   min_response_size int Minimum message body size that meets cache requirements. The default value is -1, indicating an unlimited value.   max_response_size int Maximum message body size that meets cache requirements. The default value is the maximum value of the int parameter.   max_cached_item int Maximum number of messages that can be cached. The default value is 1000000. The value is valid when the cache type is ccache.   max_cached_size int Maximum cache memory overhead. The default value is 1000000000, that is, 1 GB. The value is valid when the cache type is ristretto.   validated_status_code array Request status code that is allowed to be cached. The default value is 200,201,404,403,413,400,301.    Other Parameters #  If you want to ignore caching, you can define no_cache in the URL parameters to cause the gateway to ignore caching. For example:\ncurl http://localhost:8000/_search?no_cache=true "});index.add({'id':83,'href':'/en/docs/latest/gateway/references/filters/clone/','title':"clone",'section':"Online Filter",'content':"clone #  Description #  The clone filter is used to clone and forward traffic to another handling flow. It can implement dual-write, multi-write, multi-DC synchronization, cluster upgrade, version switching, and other requirements.\nConfiguration Example #  A simple example is as follows:\nflow: - name: double_write filter: - clone: flows: - write_to_region_a - write_to_region_b #last one's response will be output to client - name: write_to_region_a filter: - elasticsearch: elasticsearch: es1 - name: write_to_region_b filter: - elasticsearch: elasticsearch: es2 The above example copies Elasticsearch requests to two different remote clusters.\nParameter Description #     Name Type Description     flows array Multiple traffic handling flows, which are executed one after another. The result of the last flow is output to the client.   continue bool Whether to continue the previous flow after traffic is migrated. The gateway returns immediately after it is set to false. The default value is false.    "});index.add({'id':84,'href':'/en/docs/latest/gateway/references/filters/context_filter/','title':"context_filter",'section':"Online Filter",'content':"context_filter #  Description #  The context_filter is used to filter traffic by request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_filter: context: _ctx.request.path message: \u0026quot;request not allowed.\u0026quot; status: 403 must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     context string Context variable   exclude array List of variables used to refuse requests to pass through   include array List of variables used to allow requests to pass through   must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n"});index.add({'id':85,'href':'/en/docs/latest/gateway/references/filters/context_limiter/','title':"context_limiter",'section':"Online Filter",'content':"context_limiter #  Description #  The context_limiter filter is used to control the traffic based on request context.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: default_flow filter: - context_limiter: max_requests: 1 action: drop context: - _ctx.request.path - _ctx.request.header.Host - _ctx.request.header.Env The above configuration combines three context variables (_ctx.request.path, _ctx.request.header.Host, and _ctx.request.header.Env) into a bucket for traffic control. The allowable maximum queries per second (QPS) is 1 per second. Subsequent requests out of the traffic control range are directly denied.\nParameter Description #     Name Type Description     context array Context variables, which form a bucket key   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':86,'href':'/en/docs/latest/gateway/references/filters/context_regex_replace/','title':"context_regex_replace",'section':"Online Filter",'content':"context_regex_replace #  Description #  The context_regex_replace filter is used to replace and modify relevant information in the request context by using regular expressions.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - context_regex_replace: context: \u0026quot;_ctx.request.path\u0026quot; pattern: \u0026quot;^/\u0026quot; to: \u0026quot;/cluster:\u0026quot; when: contains: _ctx.request.path: /_search - dump: request: true This example replaces curl localhost:8000/abc/_search in requests with curl localhost:8000/cluster:abc/_search.\nParameter Description #     Name Type Description     context string Request context and corresponding key   pattern string Regular expression used for matching and replacement   to string Target string used for replacement    A list of context variables that can be modified is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.header.[KEY] string Response header information   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    "});index.add({'id':87,'href':'/en/docs/latest/gateway/references/processors/dag/','title':"dag",'section':"Offline Processor",'content':"dag #  Description #  The dag processor is used to manage the concurrent scheduling of tasks.\nConfiguration Example #  The following example defines a service named racing_example and auto_start is set to true. Processing units to be executed in sequence are set in processor, the dag processor supports concurrent execution of multiple tasks and the wait_all and first_win aggregation modes.\npipeline: - name: racing_example auto_start: true processor: - echo: #ready, set, go message: read,set,go - dag: mode: wait_all #first_win, wait_all parallel: - echo: #player1 message: player1 - echo: #player2 message: player2 - echo: #player3 message: player3 end: - echo: #checking score message: checking score - echo: #announce champion message: 'announce champion' - echo: #done message: racing finished The echo processor above is very simple and is used to output a specified message. This pipeline simulates a race scene, in which players 1, 2, and 3 run at the same time. After they run, the scores are calculated and the winner is announced, and finally the completion information is output. The output of the program is as follows:\n[10-12 14:59:22] [INF] [echo.go:36] message:read,set,go [10-12 14:59:22] [INF] [echo.go:36] message:player1 [10-12 14:59:22] [INF] [echo.go:36] message:player2 [10-12 14:59:22] [INF] [echo.go:36] message:player3 [10-12 14:59:22] [INF] [echo.go:36] message:checking score [10-12 14:59:22] [INF] [echo.go:36] message:announce champion [10-12 14:59:22] [INF] [echo.go:36] message:racing finished Parameter Description #     Name Type Description     mode string Aggregation mode of task results. The value first_win indicates that the program continues further execution after any of the concurrent tasks is completed, and the value wait_all indicates that the program continues further execution only after all concurrent tasks are completed.   parallel array Task array list, in which multiple subtasks are defined in sequence.   end array Task array list, which lists tasks to be executed after concurrent tasks are completed.    "});index.add({'id':88,'href':'/en/docs/latest/gateway/references/filters/date_range_precision_tuning/','title':"date_range_precision_tuning",'section':"Online Filter",'content':"date_range_precision_tuning #  Description #  The date_range_precision_tuning filter is used to reset the time precision for time range query. After the precision is adjusted, adjacent repeated requests initiated within a short period of time can be easily cached. For scenarios with low time precision but a large amount of data, for example, if Kibana is used for report analysis, you can reduce the precision to cache repeated query requests to reduce the pressure of the back-end server and accelerate the front-end report presentation.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - date_range_precision_tuning: time_precision: 4 - get_cache: - elasticsearch: elasticsearch: dev - set_cache: Precision Description #  Queries sent by Kibana to Elasticsearch use the current time (Now) by default, which is accurate to milliseconds. You can set different precision levels to rewrite queries. See the following query example:\n{\u0026quot;range\u0026quot;:{\u0026quot;@timestamp\u0026quot;:{\u0026quot;gte\u0026quot;:\u0026quot;2019-09-26T08:21:12.152Z\u0026quot;,\u0026quot;lte\u0026quot;:\u0026quot;2020-09-26T08:21:12.152Z\u0026quot;,\u0026quot;format\u0026quot;:\u0026quot;strict_date_optional_time\u0026quot;} Set different precision levels. The query results after rewriting are as follows:\n   Precision New Query     0 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T23:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   1 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T00:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T09:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   2 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:00:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:59:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   3 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:20:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:29:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   4 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:00.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:59.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   5 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:10.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:19.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   6 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.000Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.999Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   7 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.100Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.199Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   8 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.150Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.159Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}   9 {\u0026ldquo;range\u0026rdquo;:{\u0026quot;@timestamp\u0026quot;:{\u0026ldquo;gte\u0026rdquo;:\u0026ldquo;2019-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;lte\u0026rdquo;:\u0026ldquo;2020-09-26T08:21:12.152Z\u0026rdquo;,\u0026ldquo;format\u0026rdquo;:\u0026ldquo;strict_date_optional_time\u0026rdquo;}    Parameter Description #     Name Type Description     time_precision int Precision length of time, that is, the digit length of displayed time. The default value is 4 and the valid range is from 0 to 9.   path_keywords array Keyword contained in a request. The time precision is reset only for requests that contain the keywords, to prevent parsing of unnecessary requests. The default values are _search and _async_search.    "});index.add({'id':89,'href':'/en/docs/latest/gateway/references/filters/drop/','title':"drop",'section':"Online Filter",'content':"drop #  Description #  The drop filter is used to discard a message and end the processing of a request in advance.\nConfiguration Example #  A simple example is as follows:\nflow: - name: drop filter: - drop: "});index.add({'id':90,'href':'/en/docs/latest/gateway/references/filters/dump/','title':"dump",'section':"Online Filter",'content':"dump #  Description #  The dump filter is used to dump relevant request information on terminals. It is mainly used for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: hello_world filter: - dump: uri: true request_header: true request_body: true response_body: true status_code: true Parameter Description #  The dump filter is relatively simple. After the dump filter is inserted into a required flow handling phase, the terminal can output request information about the phase, facilitating debugging.\n   Name Type Description     request bool Whether to output all complete request information   response bool Whether to output all complete response information   uri bool Whether to output the requested URI information   query_args bool Whether to output the requested parameter information   user bool Whether to output the requested user information   api_key bool Whether to output the requested API key information   request_header bool Whether to output the header information of the request   response_header bool Whether to output the header information of the response   status_code bool Whether to output the status code of the response   context array User-defined context information for output    Outputting Context #  You can use the context parameter to debug request context information. The following is an example of the configuration file.\nflow: - name: echo filter: - set_response: status: 201 content_type: \u0026quot;text/plain; charset=utf-8\u0026quot; body: \u0026quot;hello world\u0026quot; - set_response_header: headers: - Env -\u0026gt; Dev - dump: context: - _ctx.id - _ctx.tls - _ctx.remote_addr - _ctx.local_addr - _ctx.request.host - _ctx.request.method - _ctx.request.uri - _ctx.request.path - _ctx.request.body - _ctx.request.body_length - _ctx.request.query_args.from - _ctx.request.query_args.size - _ctx.request.header.Accept - _ctx.request.user - _ctx.response.status - _ctx.response.body - _ctx.response.content_type - _ctx.response.body_length - _ctx.response.header.Env Start the gateway and run the following command:\ncurl http://localhost:8000/medcl/_search\\?from\\=1\\\u0026amp;size\\=100 -d'{search:query123}' -v -u 'medcl:123' The gateway outputs the following information:\n---- dumping context ---- _ctx.id : 21474836481 _ctx.tls : false _ctx.remote_addr : 127.0.0.1:50925 _ctx.local_addr : 127.0.0.1:8000 _ctx.request.host : localhost:8000 _ctx.request.method : POST _ctx.request.uri : http://localhost:8000/medcl/_search?from=1\u0026amp;size=100 _ctx.request.path : /medcl/_search _ctx.request.body : {search:query123} _ctx.request.body_length : 17 _ctx.request.query_args.from : 1 _ctx.request.query_args.size : 100 _ctx.request.header.Accept : */* _ctx.request.user : medcl _ctx.response.status : 201 _ctx.response.body : hello world _ctx.response.content_type : text/plain; charset=utf-8 _ctx.response.body_length : 11 _ctx.response.header.Env : Dev "});index.add({'id':91,'href':'/en/docs/latest/gateway/references/processors/dump_hash/','title':"dump_hash",'section':"Offline Processor",'content':"dump_hash #  Description #  The dump_hash processor is used to export index documents of a cluster and calculate the hash value.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - dump_hash: #dump es1's doc indices: \u0026quot;medcl-dr3\u0026quot; scroll_time: \u0026quot;10m\u0026quot; elasticsearch: \u0026quot;source\u0026quot; query: \u0026quot;field1:elastic\u0026quot; fields: \u0026quot;doc_hash\u0026quot; output_queue: \u0026quot;source_docs\u0026quot; batch_size: 10000 slice_size: 5 Parameter Description #     Name Type Description     elasticsearch string Name of a target cluster   scroll_time string Scroll session timeout duration   batch_size int Scroll batch size, which is set to 5000 by default   slice_size int Slice size, which is set to 1 by default   sort_type string Document sorting type, which is set to asc by default   sort_field string Document sorting field   indices string Index   level string Request processing level, which can be set to cluster, indicating that node- and shard-level splitting are not performed on requests. It is applicable to scenarios in which there is a proxy in front of Elasticsearch.   query string Query filter conditions   fields string List of fields to be returned   sort_document_fields bool Whether to sort fields in _source before the hash value is calculated. The default value is false.   hash_func string Hash function, which can be set to xxhash32, xxhash64, or fnv1a. The default value is xxhash32.   output_queue string Name of a queue that outputs results    "});index.add({'id':92,'href':'/en/docs/latest/gateway/references/filters/elasticsearch/','title':"elasticsearch",'section':"Online Filter",'content':"elasticsearch #  Description #  The elasticsearch filter is used to forward requests to back-end Elasticsearch clusters.\nConfiguration Example #  Before using the elasticsearch filter, define one Elasticsearch cluster configuration node as follows:\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 The following shows a flow configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod The preceding example forwards requests to the prod cluster.\nAutomatic Update #  For a large cluster that contains many nodes, it is almost impossible to configure all back-end nodes individually. Instead, you only need to enable auto-discovery of back-end nodes on the Elasticsearch module. See the following example.\nelasticsearch: - name: prod enabled: true endpoint: http://192.168.3.201:9200 discovery: enabled: true refresh: enabled: true basic_auth: username: elastic password: pass Then, enable automatic configuration refresh on the filter. Now, all back-end nodes can be accessed and the status of online and offline nodes is automatically updated. See the following example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod refresh: enabled: true interval: 30s Setting the Weight #  If there are many back-end clusters, INFINI Gateway allows you to set different access weights for different nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s weights: - host: 192.168.3.201:9200 weight: 10 - host: 192.168.3.202:9200 weight: 20 - host: 192.168.3.203:9200 weight: 30 In the above example, the traffic destined for an Elasticsearch cluster is distributed to the 203, 202, and 201 nodes at a ratio of 3：2：1.\nFiltering Node #  INFINI Gateway can also filter requests based on node IP address, label, or role to avoid sending requests to specific nodes, such as the master and cold nodes. See the following configuration example.\nflow: - name: cache_first filter: - elasticsearch: elasticsearch: prod balancer: weight refresh: enabled: true interval: 30s filter: hosts: exclude: - 192.168.3.201:9200 include: - 192.168.3.202:9200 - 192.168.3.203:9200 tags: exclude: - temp: cold include: - disk: ssd roles: exclude: - master include: - data - ingest Parameter Description #     Name Type Description     elasticsearch string Name of an Elasticsearch cluster   max_connection_per_node int Maximum number of TCP connections that are allowed to access each node of an Elasticsearch cluster. The default value is 5000.   max_response_size int Maximum size of the message body returned in response to an Elasticsearch request. The default value is 100*1024*1024.   max_conn_wait_timeout int Timeout duration for Elasticsearch to wait for an idle connection. The default value is 10s.   max_idle_conn_duration int Idle duration of an Elasticsearch connection. The default value is 0s.   max_retry_times int Limit the number of retries on Elasticsearch errors, default 0   max_conn_duration int Duration of an Elasticsearch connection. The default value is 0s.   read_timeout int Read timeout duration of an Elasticsearch request. The default value is 0s.   write_timeout int Write timeout duration of an Elasticsearch request. The default value is 0s.   read_buffer_size int Read cache size for an Elasticsearch request. The default value is 4096*4.   write_buffer_size int Write cache size for an Elasticsearch request. The default value is 4096*4.   tls_insecure_skip_verify bool Whether to ignore TLS certificate verification of an Elasticsearch cluster. The default value is true.   balancer string Load balancing algorithm of a back-end Elasticsearch node. Currently, only the weight weight-based algorithm is available.   refresh.enable bool Whether to enable automatic refresh of node status changes, to perceive changes in the back-end Elasticsearch topology   refresh.interval int Interval of the node status refresh   weights array Priority of a back-end node. A node with a larger weight is assigned a higher proportion of request forwarding.   filter object Filtering rules for back-end Elasticsearch nodes. Rules can be set to forward requests to a specific node.   filter.hosts object Filtering based on the access address of Elasticsearch   filter.tags object Filtering based on the label of Elasticsearch   filter.roles object Filtering based on the role of Elasticsearch   filter.*.exclude array Conditions for excluding. Any matched node is denied handling requests as a proxy.   filter.*.include array Elasticsearch nodes that meet conditions are allowed to handle requests as a proxy. When the exclude parameter is not configured but include is configured, any condition in include must be met. Otherwise, the node is not allowed to handle requests as a proxy.    "});index.add({'id':93,'href':'/en/docs/latest/gateway/references/filters/elasticsearch_health_check/','title':"elasticsearch_health_check",'section':"Online Filter",'content':"elasticsearch_health_check #  Description #  The elasticsearch_health_check filter is used to detect the health status of Elasticsearch in traffic control mode. When a back-end fault occurs, the filter triggers an active cluster health check without waiting for the results of the default polling check of Elasticsearch. Traffic control can be configured to enable the filter to send check requests to the back-end Elasticsearch at a maximum of once per second.\nConfiguration Example #  A simple example is as follows:\nflow: - name: elasticsearch_health_check filter: - elasticsearch_health_check: elasticsearch: dev Parameter Description #     Name Type Description     elasticsearch string Cluster ID   interval int Minimum interval for executing requests, in seconds. The default value is 1.    "});index.add({'id':94,'href':'/en/docs/latest/gateway/references/filters/flow/','title':"flow",'section':"Online Filter",'content':"flow #  Description #  The flow filter is used to redirect to or execute one or a series of other flows.\nConfiguration Example #  A simple example is as follows:\nflow: - name: flow filter: - flow: flows: - request_logging Context mapped flow:\nflow: - name: dns-flow filter: - flow: ignore_undefined_flow: true context_flow: context: _ctx.request.host context_parse_pattern: (?P\u0026lt;uuid\u0026gt;^[0-9a-z_\\-]+)\\. flow_id_template: flow_$[[uuid]] - set_response: status: 503 content_type: application/json body: '{\u0026quot;message\u0026quot;:\u0026quot;invalid HOST\u0026quot;}' More information about context, please refer to Context .\nParameter Description #     Name Type Description     flow string Flow ID, the definition of how requests will be executed   flows array Flow ID, in the array format. You can specify multiple flows, which are executed in sequence   ignore_undefined_flow bool Ignore the undefined flow   context_flow.context string The context to use for mapping flow_id   context_flow.context_parse_pattern string The regexp pattern used to extract named variables from context value   context_flow.flow_id_template string The string template used to rendering flow_id   context_flow.continue string Will continue next filter after executed the context mapped flow, default false    "});index.add({'id':95,'href':'/en/docs/latest/gateway/references/processors/flow_runner/','title':"flow_runner",'section':"Offline Processor",'content':"flow_runner #  Description #  The flow_runner processor is used to asynchronously consume requests in a queue by using the processing flow used for online requests.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - flow_runner: input_queue: \u0026quot;primary_deadletter_requests\u0026quot; flow: primary-flow-post-processing when: cluster_available: [ \u0026quot;primary\u0026quot; ] Parameter Description #     Name Type Description     input_queue string Name of a subscribed queue   flow string Flow used to consume requests in consumption queues   commit_on_tag string A message is committed only when a specified tag exists in the context of the current request. The default value is blank, indicating that a message is committed immediately after the execution is complete.    "});index.add({'id':96,'href':'/en/docs/latest/gateway/references/filters/http/','title':"http",'section':"Online Filter",'content':"http #  Description #  The http filter is used to forward requests to a specified HTTP server as a proxy.\nConfiguration Example #  A simple example is as follows:\nflow: - name: default_flow filter: - basic_auth: valid_users: medcl: passwd - http: schema: \u0026quot;http\u0026quot; #https or http #host: \u0026quot;192.168.3.98:5601\u0026quot; hosts: - \u0026quot;192.168.3.98:5601\u0026quot; - \u0026quot;192.168.3.98:5602\u0026quot; Parameter Description #     Name Type Description     schema string http or https   host string Target host address containing the port ID, for example, localhost:9200   hosts array Host address list. The addresses are tried in sequence after a fault occurs.   skip_failure_host bool Skip hosts in failure, default true   max_connection_per_node int The max connections per node, default 5000   max_response_size int The max length of response supported   max_retry_times int The max num of retries, default 0   retry_delay_in_ms int The latency before next retry in millisecond, default 1000   skip_cleanup_hop_headers bool Remove Hop-by-hop Headers   max_conn_wait_timeout duration The max time wait to create new connections, default 30s   max_idle_conn_duration duration The max duration of idle connections, default 30s   max_conn_duration duration The max duration of keepalived connections, default 0s   timeout duration Request timeout duration, default 30s   read_timeout duration Read request timeout duration, default 0s   write_timeout duration Write request timeout duration, default 0s   read_buffer_size int Read buffer size, default 16384   write_buffer_size int Write buffer size, default 16384   tls_insecure_skip_verify bool Skip the TLS verification, default true    "});index.add({'id':97,'href':'/en/docs/latest/gateway/references/processors/index_diff/','title':"index_diff",'section':"Offline Processor",'content':"index_diff #  Description #  The index_diff processor is used to compare differences between two result sets.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - index_diff: diff_queue: \u0026quot;diff_result\u0026quot; buffer_size: 1 text_report: true #If data needs to be saved to Elasticsearch, disable the function and start the diff_result_ingest task of the pipeline. source_queue: 'source_docs' target_queue: 'target_docs' Parameter Description #     Name Type Description     source_queue string Name of source data   target_queue string Name of target data   diff_queue string Queue that stores difference results   buffer_size int Memory buffer size   keep_source bool Whether difference results contain document source information   text_report bool Whether to output results in text form    "});index.add({'id':98,'href':'/en/docs/latest/gateway/references/processors/indexing_merge/','title':"indexing_merge",'section':"Offline Processor",'content':"indexing_merge #  描述 #  The indexing_merge processor is used to consume pure JSON documents in the queue, and merge them into bulk requests and save them in the specified queue. It needs to be consumed with the bulk_indexing processor, and batch writes are used instead of single requests to improve write throughput.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: indexing_merge auto_start: true keep_running: true processor: - indexing_merge: input_queue: \u0026quot;request_logging\u0026quot; elasticsearch: \u0026quot;logging-server\u0026quot; index_name: \u0026quot;infini_gateway_requests\u0026quot; output_queue: name: \u0026quot;gateway_requests\u0026quot; label: tag: \u0026quot;request_logging\u0026quot; worker_size: 1 bulk_size_in_mb: 10 - name: logging_requests auto_start: true keep_running: true processor: - bulk_indexing: bulk: compress: true batch_size_in_mb: 10 batch_size_in_docs: 5000 consumer: fetch_max_messages: 100 queues: type: indexing_merge when: cluster_available: [ \u0026quot;logging-server\u0026quot; ] Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, in seconds. The default value is 5.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.   output_queue.name string The name of output queue   output_queue.label map The labels assign to the output queue，with label type:indexing_merge builtin.   failure_queue string The name of queue to save failure requests   invalid_queue string The name of queue to save invalid requests    "});index.add({'id':99,'href':'/en/docs/latest/gateway/references/filters/javascript/','title':"javascript",'section':"Online Filter",'content':"javascript #  Description #  The javascript filter can be used to execute your own processing flow by crafting the scripts in javascript, which provide the ultimate flexibility.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - javascript: source: \u0026gt; function process(ctx) { var console = require('console'); console.log(\u0026quot;hello from javascript\u0026quot;); } The process in this script is a built-in function that handles incoming context and allows to write your custom business logic.\nIf the script is complex, it can be loaded from a file:\nflow: - name: test filter: - javascript: file: example.js The example.js is where the file located.\nParameter Description #     Name Type Description     source string Inline Javascript source code.   file string Path to a script file to load. Relative paths are interpreted as relative to the ${INSTANCE_DATA_PATH}/scripts directory.   params map A dictionary of parameters that are passed to the register of the script.    Context API #  The Context object passed to the process method has the following API. To learn more about context, please refer to Request Context.\n   Method Description     Get(string) Get a value from the context. If the key does not exist null is returned. If no key is provided then an object containing all fields is returned. eg: var value = event.Get(key);   Put(string, value) Put a value into the context. If the key was already set then the previous value is returned. It throws an exception if the key cannot be set because one of the intermediate values is not an object. eg: var old = event.Put(key, value);   Rename(string, string) Rename a key in the context. The target key must not exist. It returns true if the source key was successfully renamed to the target key. eg: var success = event.Rename(\u0026quot;source\u0026quot;, \u0026quot;target\u0026quot;);   Delete(string) Delete a field from the context. It returns true on success. eg: var deleted = event.Delete(\u0026quot;user.email\u0026quot;);   Tag(string) Append a tag to the tags field if the tag does not already exist. Throws an exception if tags exists and is not a string or a list of strings. eg: event.Tag(\u0026quot;user_event\u0026quot;);   AppendTo(string, string) AppendTo is a specialized Put method that converts the existing value to an array and appends the value if it does not already exist. If there is an existing value that’s not a string or array of strings then an exception is thrown. eg: event.AppendTo(\u0026quot;error.message\u0026quot;, \u0026quot;invalid file hash\u0026quot;);    Parameterization #  The following example describes how to use params to pass variables to scripts that can be loaded from files for easy reuse of program scripts.\nflow: - name: test filter: - javascript: params: keyword: [ \u0026quot;hello\u0026quot;, \u0026quot;world\u0026quot;, \u0026quot;scripts\u0026quot; ] source: \u0026gt; var console = require('console'); var params = {keyword: []}; function register(scriptParams) { params = scriptParams; } function process(ctx) { console.info(\u0026quot;keyword comes from params: [%s]\u0026quot;, params.keyword); } register is a built-in function that initializes external parameters.\n"});index.add({'id':100,'href':'/en/docs/latest/gateway/references/processors/json_indexing/','title':"json_indexing",'section':"Offline Processor",'content':"json_indexing #  Description #  The json_indexing processor is used to consume pure JSON documents in queues and store them to a specified Elasticsearch server.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: request_logging_index auto_start: true keep_running: true processor: - json_indexing: index_name: \u0026quot;gateway_requests\u0026quot; elasticsearch: \u0026quot;dev\u0026quot; input_queue: \u0026quot;request_logging\u0026quot; idle_timeout_in_seconds: 1 worker_size: 1 bulk_size_in_mb: 10 Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, in seconds. The default value is 5.   bulk_size_in_kb int Size of a bulk request, in KB.   bulk_size_in_mb int Size of a bulk request, in MB.   elasticsearch string Name of a target cluster, to which requests are saved.   index_name string Name of the index stored to the target cluster.   type_name string Name of the index type stored to the target cluster. It is set based on the cluster version. The value is doc for Elasticsearch versions earlier than v7 and _doc for versions later than v7.    "});index.add({'id':101,'href':'/en/docs/latest/gateway/references/filters/ldap_auth/','title':"ldap_auth",'section':"Online Filter",'content':"ldap_auth #  Description #  The ldap_auth filter is used to set authentication based on the Lightweight Directory Access Protocol (LDAP).\nConfiguration Example #  A simple example is as follows:\nflow: - name: ldap_auth filter: - ldap_auth: host: \u0026quot;ldap.forumsys.com\u0026quot; port: 389 bind_dn: \u0026quot;cn=read-only-admin,dc=example,dc=com\u0026quot; bind_password: \u0026quot;password\u0026quot; base_dn: \u0026quot;dc=example,dc=com\u0026quot; user_filter: \u0026quot;(uid=%s)\u0026quot; The above configuration uses an online free LDAP test server, the test user is tesla, and the password is password.\n➜ curl http://127.0.0.1:8000/ -u tesla:password { \u0026quot;name\u0026quot; : \u0026quot;192.168.3.7\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;ZGTwWtBfSLWRpsS1VKQDiQ\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;7.8.0\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;757314695644ea9a1dc2fecd26d1a43856725e65\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2020-06-14T19:35:50.234439Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;8.5.1\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;6.8.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;6.0.0-beta1\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } ➜ curl http://127.0.0.1:8000/ -u tesla:password1 Unauthorized% Parameter Description #     Name Type Description     host string Address of the LDAP server   port int Port of the LDAP server. The default value is 389.   tls bool Whether the LDAP server uses the Transport Layer Security (TLS) protocol. The default value is false.   bind_dn string Information about the user who performs the LDAP query   bind_password string Password for performing the LDAP query   base_dn string Root domain for filtering LDAP users   user_filter string Query condition for filtering LDAP users. The default value is (uid=%s).   uid_attribute string Attribute of a user ID. The default value is uid.   group_attribute string Attribute of a user group. The default value is cn.   attribute array List of attributes returned by the LDAP query   max_cache_items int The max number of cached items   cache_ttl duration The expired TTL of cached items，default 300s    "});index.add({'id':102,'href':'/en/docs/latest/gateway/references/filters/logging/','title':"logging",'section':"Online Filter",'content':"logging #  Description #  The logging filter is used to asynchronously record requests to the local disk to minimize the delay of requests. In scenarios with heavy traffic, you are advised to use other request filters jointly to reduce the total number of logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - logging: queue_name: request_logging An example of a recorded request log is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #     Name Type Description     queue_name string Name of a queue that stores request logs in the local disk   format_header_keys bool Whether to standardize the header and convert it into lowercase letters. The default value is false.   remove_authorization bool Whether to remove authorization information from the header. The default value is true.   max_request_body_size int Whether to truncate a very long request message. The default value is 1024, indicating that 1024 characters are retained.   max_response_body_size int Whether to truncate a very long response message. The default value is 1024, indicating that 1024 characters are retained.   min_elapsed_time_in_ms int Request filtering based on response time, that is, the minimum time (ms) for request logging. A request with time that exceeds this value will be logged.   bulk_stats_details bool Whether to record detailed index-based bulk request statistics. The default value is true.    "});index.add({'id':103,'href':'/en/docs/latest/gateway/references/filters/queue/','title':"queue",'section':"Online Filter",'content':"queue #  Description #  The queue filter is used to store requests to message queues.\nConfiguration Example #  A simple example is as follows:\nflow: - name: queue filter: - queue: queue_name: queue_name Parameter Description #     Name Type Description     depth_threshold int Queue depth threshold. Only a request with the size exceeding this value can be stored to a queue. The default value is 0.   queue_name string Name of a message queue    "});index.add({'id':104,'href':'/en/docs/latest/gateway/references/processors/queue_consumer/','title':"queue_consumer",'section':"Offline Processor",'content':"queue_consumer #  Description #  The queue_consumer processor is used to asynchronously consume requests in a queue and send the requests to Elasticsearch.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: bulk_request_ingest auto_start: true keep_running: true processor: - queue_consumer: input_queue: \u0026quot;backup\u0026quot; elasticsearch: \u0026quot;backup\u0026quot; waiting_after: [ \u0026quot;backup_failure_requests\u0026quot;] worker_size: 20 when: cluster_available: [ \u0026quot;backup\u0026quot; ] Parameter Description #     Name Type Description     input_queue int Name of a subscribed queue   worker_size int Number of threads that concurrently execute consumption tasks, which is set to 1 by default.   idle_timeout_in_seconds int Timeout duration of the consumption queue, which is set to 1 by default.   elasticsearch string Name of a target cluster, to which requests are saved.   waiting_after array Data in the main queue can be consumed only after data in a specified queue is consumed.   failure_queue string Request that fails to be executed because of a back-end failure. The default value is %input_queue%-failure.   invalid_queue string Request, for which the returned status code is 4xx. The default value is %input_queue%-invalid.   compress bool Whether to compress requests. The default value is false.   safety_parse bool Whether to enable secure parsing, that is, no buffer is used and memory usage is higher. The default value is true.   doc_buffer_size bool Maximum document buffer size for the processing of a single request. You are advised to set it to be greater than the maximum size of a single document. The default value is 256*1024.    "});index.add({'id':105,'href':'/en/docs/latest/gateway/references/filters/ratio/','title':"ratio",'section':"Online Filter",'content':"ratio #  Description #  The ratio filter is used to forward normal traffic to another flow proportionally. It can implement canary release, traffic migration and export, or switch some traffic to clusters of different versions for testing.\nConfiguration Example #  A simple example is as follows:\nflow: - name: ratio_traffic_forward filter: - ratio: ratio: 0.1 flow: hello_world continue: true Parameter Description #     Name Type Description     ratio float Proportion of traffic to be migrated   flow string New traffic processing flow   continue bool Whether to continue the previous flow after traffic is migrated. The gateway returns immediately after it is set to false. The default value is false.    "});index.add({'id':106,'href':'/en/docs/latest/gateway/references/filters/record/','title':"record",'section':"Online Filter",'content':"record #  Description #  The record filter is used to record requests. Output requests can be copied to the console of Kibana for debugging.\nConfiguration Example #  A simple example is as follows:\nflow: - name: request_logging filter: - record: stdout: true filename: requests.txt Examples of the format of request logs output by the record filter are as follows:\nGET /_cluster/state/version,master_node,routing_table,metadata/* GET /_alias GET /_cluster/health GET /_cluster/stats GET /_nodes/0NSvaoOGRs2VIeLv3lLpmA/stats Parameter Description #     Name Type Description     filename string Filename of request logs stored in the data directory   stdout bool Whether the terminal also outputs the characters. The default value is false.    "});index.add({'id':107,'href':'/en/docs/latest/gateway/references/filters/redirect/','title':"redirect",'section':"Online Filter",'content':"redirect #  Description #  redirect filter used to redirect request to specify URL address。\nConfiguration Example #  A simple example is as follows:\nflow: - name: redirect filter: - redirect: uri: https://infinilabs.com Parameter Description #     Name Type Description     uri string The target URI   code int Status code，default 302    "});index.add({'id':108,'href':'/en/docs/latest/gateway/references/filters/redis_pubsub/','title':"redis_pubsub",'section':"Online Filter",'content':"redis_pubsub #  Description #  The redis filter is used to store received requests and response results to Redis message queues.\nConfiguration Example #  A simple example is as follows:\nflow: - name: redis_pubsub filter: - redis_pubsub: host: 127.0.0.1 port: 6379 channel: gateway response: true Parameter Description #     Name Type Description     host string Redis host name, which is localhost by default.   port int Redis port ID, which is 6379 by default.   password string Redis password   db int Default database of Redis, which is 0 by default.   channel string Name of a Redis message queue. It is mandatory and has no default value.   response bool Whether the response result is contained. The default value is true.    "});index.add({'id':109,'href':'/en/docs/latest/gateway/references/processors/replay/','title':"replay",'section':"Offline Processor",'content':"replay #  Description #  The replay processor is used to replay requests recorded by the record filter.\nConfiguration Example #  A simple example is as follows:\npipeline: - name: play_requests auto_start: true keep_running: false processor: - replay: filename: requests.txt schema: \u0026quot;http\u0026quot; host: \u0026quot;localhost:8000\u0026quot; Parameter Description #     Name Type Description     filename string Name of a file that contains replayed messages   schema string Request protocol type: http or https   host string Target server that receives requests, in the format of host:port    "});index.add({'id':110,'href':'/en/docs/latest/gateway/references/filters/request_api_key_filter/','title':"request_api_key_filter",'section':"Online Filter",'content':"request_api_key_filter #  Description #  When Elasticsearch conducts authentication through API keys, the request_api_key_filter is used to filter requests based on request API ID.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_api_key_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - VuaCfGcBCdbkQm-e5aOx The above example shows that requests from VuaCfGcBCdbkQm-e5aOx will be rejected. See the following information.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:02:37 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 17 \u0026lt; FILTERED: true \u0026lt; process: request_api_key_filter \u0026lt; * Connection #0 to host localhost left intact {\u0026quot;error\u0026quot;:true,\u0026quot;message\u0026quot;:\u0026quot;Request filtered!\u0026quot;}% ➜ ~ Parameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':111,'href':'/en/docs/latest/gateway/references/filters/request_api_key_limiter/','title':"request_api_key_limiter",'section':"Online Filter",'content':"request_api_key_limiter #  Description #  The request_api_key_limiter filter is used to control traffic by API key.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_api_key_limiter: id: - VuaCfGcBCdbkQm-e5aOx max_requests: 1 action: drop # retry or drop message: \u0026quot;your api_key reached our limit\u0026quot; The above configuration controls the traffic with the API ID of VuaCfGcBCdbkQm-e5aOx and the allowable maximum QPS is 1 per second.\n➜ ~ curl localhost:8000 -H \u0026quot;Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw==\u0026quot; -v * Rebuilt URL to: localhost:8000/ * Trying 127.0.0.1... * TCP_NODELAY set * Connected to localhost (127.0.0.1) port 8000 (#0) \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8000 \u0026gt; User-Agent: curl/7.54.0 \u0026gt; Accept: */* \u0026gt; Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw== \u0026gt; \u0026lt; HTTP/1.1 429 Too Many Requests \u0026lt; Server: INFINI \u0026lt; Date: Mon, 12 Apr 2021 15:14:52 GMT \u0026lt; content-type: text/plain; charset=utf-8 \u0026lt; content-length: 30 \u0026lt; process: request_api_key_limiter \u0026lt; * Connection #0 to host localhost left intact your api_key reached our limit% Parameter Description #     Name Type Description     id array IDs of APIs that will participate in traffic control. If this parameter is not set, all API keys will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':112,'href':'/en/docs/latest/gateway/references/filters/request_body_json_del/','title':"request_body_json_del",'section':"Online Filter",'content':"request_body_json_del #  Description #  The request_body_json_del filter is used to delete some fields from a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_del: path: - query.bool.should.[0] - query.bool.must Parameter Description #     Name Type Description     path array JSON path key value to be deleted   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    "});index.add({'id':113,'href':'/en/docs/latest/gateway/references/filters/request_body_json_set/','title':"request_body_json_set",'section':"Online Filter",'content':"request_body_json_set #  Description #  The request_body_json_set filter is used to modify a request body of the JSON format.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_json_set: path: - aggs.total_num.terms.field -\u0026gt; \u0026quot;name\u0026quot; - aggs.total_num.terms.size -\u0026gt; 3 - size -\u0026gt; 0 Parameter Description #     Name Type Description     path map It uses -\u0026gt; to identify the key value pair: JSON path and the value used for replacement.   ignore_missing bool Whether to ignore processing if the JSON path does not exist. The default value is false.    "});index.add({'id':114,'href':'/en/docs/latest/gateway/references/filters/request_body_regex_replace/','title':"request_body_regex_replace",'section':"Online Filter",'content':"request_body_regex_replace #  Description #  The request_body_regex_replace filter is used to replace string content in a request body by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_body_regex_replace: pattern: '\u0026quot;size\u0026quot;: 10000' to: '\u0026quot;size\u0026quot;: 100' - elasticsearch: elasticsearch: prod - dump: request_body: true The above example changes the size from 10000 to 100 in the request body sent to Elasticsearch. The filter can be used to dynamically fix errors or incorrect queries.\nThe test is as follows:\ncurl -XPOST \u0026quot;http://localhost:8000/myindex/_search\u0026quot; -H 'Content-Type: application/json' -d' { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 10000 }' The actual query is as follows:\n { \u0026quot;_index\u0026quot; : \u0026quot;gateway_requests\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;EH5bG3gBsbC2s3iWFzCF\u0026quot;, \u0026quot;_score\u0026quot; : 1.0, \u0026quot;_source\u0026quot; : { \u0026quot;tls\u0026quot; : false, \u0026quot;@timestamp\u0026quot; : \u0026quot;2021-03-10T08:57:30.645Z\u0026quot;, \u0026quot;conn_time\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;flow\u0026quot; : { \u0026quot;from\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;process\u0026quot; : [ \u0026quot;request_body_regex_replace\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;date_range_precision_tuning\u0026quot;, \u0026quot;get_cache\u0026quot;, \u0026quot;elasticsearch\u0026quot;, \u0026quot;set_cache\u0026quot;, \u0026quot;||\u0026quot;, \u0026quot;request_logging\u0026quot; ], \u0026quot;relay\u0026quot; : \u0026quot;192.168.43.101-Quartz\u0026quot;, \u0026quot;to\u0026quot; : [ \u0026quot;localhost:9200\u0026quot; ] }, \u0026quot;id\u0026quot; : 3, \u0026quot;local_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;remote_ip\u0026quot; : \u0026quot;127.0.0.1\u0026quot;, \u0026quot;request\u0026quot; : { \u0026quot;body_length\u0026quot; : 53, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot; { \u0026quot;query\u0026quot;: { \u0026quot;match_all\u0026quot;: {} },\u0026quot;size\u0026quot;: 100 } \u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;content-type\u0026quot; : \u0026quot;application/json\u0026quot;, \u0026quot;User-Agent\u0026quot; : \u0026quot;curl/7.54.0\u0026quot;, \u0026quot;Accept\u0026quot; : \u0026quot;*/*\u0026quot;, \u0026quot;Host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;53\u0026quot; }, \u0026quot;host\u0026quot; : \u0026quot;localhost:8000\u0026quot;, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:8000\u0026quot;, \u0026quot;method\u0026quot; : \u0026quot;POST\u0026quot;, \u0026quot;path\u0026quot; : \u0026quot;/myindex/_search\u0026quot;, \u0026quot;remote_addr\u0026quot; : \u0026quot;127.0.0.1:63309\u0026quot;, \u0026quot;started\u0026quot; : \u0026quot;2021-03-10T08:57:30.635Z\u0026quot;, \u0026quot;uri\u0026quot; : \u0026quot;http://localhost:8000/myindex/_search\u0026quot; }, \u0026quot;response\u0026quot; : { \u0026quot;body_length\u0026quot; : 441, \u0026quot;cached\u0026quot; : false, \u0026quot;elapsed\u0026quot; : 9.878, \u0026quot;status_code\u0026quot; : 200, \u0026quot;body\u0026quot; : \u0026quot;\u0026quot;\u0026quot;{\u0026quot;took\u0026quot;:0,\u0026quot;timed_out\u0026quot;:false,\u0026quot;_shards\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;successful\u0026quot;:1,\u0026quot;skipped\u0026quot;:0,\u0026quot;failed\u0026quot;:0},\u0026quot;hits\u0026quot;:{\u0026quot;total\u0026quot;:1,\u0026quot;max_score\u0026quot;:1.0,\u0026quot;hits\u0026quot;:[{\u0026quot;_index\u0026quot;:\u0026quot;myindex\u0026quot;,\u0026quot;_type\u0026quot;:\u0026quot;doc\u0026quot;,\u0026quot;_id\u0026quot;:\u0026quot;c132mhq3r0otidqkac1g\u0026quot;,\u0026quot;_score\u0026quot;:1.0,\u0026quot;_source\u0026quot;:{\u0026quot;name\u0026quot;:\u0026quot;local\u0026quot;,\u0026quot;enabled\u0026quot;:true,\u0026quot;endpoint\u0026quot;:\u0026quot;http://localhost:9200\u0026quot;,\u0026quot;basic_auth\u0026quot;:{},\u0026quot;discovery\u0026quot;:{\u0026quot;refresh\u0026quot;:{}},\u0026quot;created\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;,\u0026quot;updated\u0026quot;:\u0026quot;2021-03-08T21:48:55.687557+08:00\u0026quot;}}]}}\u0026quot;\u0026quot;\u0026quot;, \u0026quot;header\u0026quot; : { \u0026quot;UPSTREAM\u0026quot; : \u0026quot;localhost:9200\u0026quot;, \u0026quot;process\u0026quot; : \u0026quot;request_body_regex_replace-\u0026gt;get_cache-\u0026gt;date_range_precision_tuning-\u0026gt;get_cache-\u0026gt;elasticsearch-\u0026gt;set_cache\u0026quot;, \u0026quot;content-length\u0026quot; : \u0026quot;441\u0026quot;, \u0026quot;content-type\u0026quot; : \u0026quot;application/json; charset=UTF-8\u0026quot;, \u0026quot;Server\u0026quot; : \u0026quot;INFINI\u0026quot;, \u0026quot;CLUSTER\u0026quot; : \u0026quot;dev\u0026quot; }, \u0026quot;local_addr\u0026quot; : \u0026quot;127.0.0.1:63310\u0026quot; } } } Parameter Description #     Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    "});index.add({'id':115,'href':'/en/docs/latest/gateway/references/filters/request_client_ip_filter/','title':"request_client_ip_filter",'section':"Online Filter",'content':"request_client_ip_filter #  Description #  The request_client_ip_filter is used to filter traffic based on source user IP addresses of requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_client_ip_filter: exclude: - 192.168.3.67 The above example shows that requests from 192.168.3.67 are not allowed to pass through.\nThe following is an example of route redirection.\nflow: - name: echo filter: - echo: message: hello stanger - name: default_flow filter: - request_client_ip_filter: action: redirect_flow flow: echo exclude: - 192.168.3.67 Requests from 192.168.3.67 are redirected to another echo flow.\nParameter Description #     Name Type Description     exclude array List of IP arrays, from which requests are refused to pass through   include array List of IP arrays, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':116,'href':'/en/docs/latest/gateway/references/filters/request_client_ip_limiter/','title':"request_client_ip_limiter",'section':"Online Filter",'content':"request_client_ip_limiter #  Description #  The request_client_ip_limiter filter is used to control traffic based on the request client IP address.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_client_ip_limiter: ip: #only limit for specify ips - 127.0.0.1 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;your ip reached our limit\u0026quot; The above configuration controls the traffic with the IP address of 127.0.0.1 and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     ip array Client IP addresses that will participate in traffic control. If this parameter is not set, all IP addresses will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':117,'href':'/en/docs/latest/gateway/references/filters/request_header_filter/','title':"request_header_filter",'section':"Online Filter",'content':"request_header_filter #  Description #  The request_header_filter is used to filter traffic based on request header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_header_filter: include: - TRACE: true The above example shows that requests are allowed to pass through only when the headers of the requests contain TRACE: true.\ncurl 192.168.3.4:8000 -v -H 'TRACE: true' Parameter Description #     Name Type Description     exclude array Header information used to refuse to allow requests to pass through   include array Header information used to allow requests to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':118,'href':'/en/docs/latest/gateway/references/filters/request_host_filter/','title':"request_host_filter",'section':"Online Filter",'content':"request_host_filter #  Description #  The request_host_filter is used to filter requests based on a specified domain name or host name. It is suitable for scenarios in which there is only one IP address but access control is required for multiple domain names.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_host_filter: include: - domain-test2.com:8000 The above example shows that only requests that are used to access the domain name domain-test2.com:8000 are allowed to pass through.\nExample #  ✗ curl -k -u user:passwd http://domain-test4.com:8000/ -v * Trying 192.168.3.67... * TCP_NODELAY set * Connected to domain-test4.com (192.168.3.67) port 8000 (#0) * Server auth using Basic with user 'medcl' \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test4.com:8000 \u0026gt; Authorization: Basic 123= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 403 Forbidden \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:53:01 GMT \u0026lt; Content-Length: 0 \u0026lt; FILTERED: true \u0026lt; * Connection #0 to host domain-test4.com left intact * Closing connection 0 ✗ curl -k -u user:passwd http://domain-test2.com:8000/ -v * Trying 192.168.3.67... * TCP_NODELAY set * Connected to domain-test2.com (192.168.3.67) port 8000 (#0) * Server auth using Basic with user 'medcl' \u0026gt; GET / HTTP/1.1 \u0026gt; Host: domain-test2.com:8000 \u0026gt; Authorization: Basic 123= \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Fri, 15 Jan 2021 13:52:53 GMT \u0026lt; Content-Type: application/json; charset=UTF-8 \u0026lt; Content-Length: 480 \u0026lt; UPSTREAM: 192.168.3.203:9200 \u0026lt; CACHE-HASH: a2902f950b4ade804b21a062257387ef \u0026lt; { \u0026quot;name\u0026quot; : \u0026quot;node3\u0026quot;, \u0026quot;cluster_name\u0026quot; : \u0026quot;pi\u0026quot;, \u0026quot;cluster_uuid\u0026quot; : \u0026quot;Z_HcN_6ESKWicV-eLsyU4g\u0026quot;, \u0026quot;version\u0026quot; : { \u0026quot;number\u0026quot; : \u0026quot;6.4.2\u0026quot;, \u0026quot;build_flavor\u0026quot; : \u0026quot;default\u0026quot;, \u0026quot;build_type\u0026quot; : \u0026quot;tar\u0026quot;, \u0026quot;build_hash\u0026quot; : \u0026quot;04711c2\u0026quot;, \u0026quot;build_date\u0026quot; : \u0026quot;2018-09-26T13:34:09.098244Z\u0026quot;, \u0026quot;build_snapshot\u0026quot; : false, \u0026quot;lucene_version\u0026quot; : \u0026quot;7.4.0\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot; : \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot; : \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot; : \u0026quot;You Know, for Search\u0026quot; } * Connection #0 to host domain-test2.com left intact * Closing connection 0 Parameter Description #     Name Type Description     exclude array List of hosts, from which requests are refused to pass through   include array List of hosts, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':119,'href':'/en/docs/latest/gateway/references/filters/request_host_limiter/','title':"request_host_limiter",'section':"Online Filter",'content':"request_host_limiter #  Description #  The request_host_limiter filter is used to control traffic based on the request host (domain name).\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_host_limiter: host: - api.elasticsearch.cn:8000 - logging.elasticsearch.cn:8000 max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic used for accessing domain names api.elasticsearch.cn and logging.elasticsearch.cn and the maximum allowable QPS is 256 per second.\nParameter Description #     Name Type Description     host array Host domain names that will participate in traffic control. If this parameter is not set, all host domain names will participate in traffic control. If an accessed domain name contains a port ID, add the port ID here. For example, localhost:8080.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set to retry or drop and the default value is retry.   status string Status code returned after traffic control conditions are met. The default value is 429.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.   failed_retry_message string Rejection message returned for a request, for which the maximum retry count has been reached    "});index.add({'id':120,'href':'/en/docs/latest/gateway/references/filters/request_method_filter/','title':"request_method_filter",'section':"Online Filter",'content':"request_method_filter #  Description #  The request_method_filter is used to filter traffic based on request method.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_method_filter: exclude: - PUT - POST include: - GET - HEAD - DELETE Parameter Description #     Name Type Description     exclude array Methods of requests that are refused to pass through   include array Methods of requests that are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':121,'href':'/en/docs/latest/gateway/references/filters/request_path_filter/','title':"request_path_filter",'section':"Online Filter",'content':"request_path_filter #  Description #  The request_path_filter is used to filter traffic based on request path.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_path_filter: must: #must match all rules to continue prefix: - /medcl contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl must_not: # any match will be filtered prefix: - /.kibana - /_security - /_security - /gateway_requests* - /.reporting - /_monitoring/bulk contain: - _search suffix: - _count - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl should: prefix: - /medcl contain: - _search - _async_search suffix: - _refresh wildcard: - /*/_refresh regex: - ^/m[\\w]+dcl Parameter Description #     Name Type Description     must.* object Requests are allowed to pass through only when all conditions are met.   must_not.* object Requests are allowed to pass through only when none of the conditions are met.   should.* object Requests are allowed to pass through when any condition is met.   *.prefix array Whether a request begins with a specific character   *.suffix array Whether a request ends with a specific character   *.contain array Whether a request contains a specific character   *.wildcard array Whether a request meets pattern matching rules   *.regex array Whether a request meets regular expression matching rules   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If only the should condition is met, requests are allowed to pass through only when at least one item in should is met.\n"});index.add({'id':122,'href':'/en/docs/latest/gateway/references/filters/request_path_limiter/','title':"request_path_limiter",'section':"Online Filter",'content':"request_path_limiter #  Description #  The request_path_limiter filter is used to define traffic control rules for requests. It can implement index-level traffic control.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_path_limiter: message: \u0026quot;Hey, You just reached our request limit!\u0026quot; rules: - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;medcl)/_search\u0026quot; max_qps: 3 group: index_name - pattern: \u0026quot;/(?P\u0026lt;index_name\u0026gt;.*?)/_search\u0026quot; max_qps: 100 group: index_name In the above configuration, the query is performed against the medcl query, the allowable maximum QPS is 3, and the QPS is 100 for queries performed against other indexes.\nParameter Description #     Name Type Description     message string Message returned for a request, for which traffic control conditions are met   rules array Traffic control rule. Multiple rules can be configured, which are matched based on their configuration sequence. If a rule is matched earlier, the corresponding action is performed earlier.   rules.pattern string Regular expression rule used for URL path matching. One group name must be provided as the bucket key for traffic control.   rules.group string Group name defined in the regular expression, which is used to count the number of requests. Requests with the same group value are regarded as the same type of request.   rules.max_qps int Maximum QPS defined for each group of requests. When the actual value exceeds this value, the traffic control action is triggered.    "});index.add({'id':123,'href':'/en/docs/latest/gateway/references/filters/request_user_filter/','title':"request_user_filter",'section':"Online Filter",'content':"request_user_filter #  Description #  When Elasticsearch conducts authentication in Basic Auth mode, the request_user_filter is used to filter requests by request username.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - request_user_filter: include: - \u0026quot;elastic\u0026quot; The above example shows that only requests from elastic are allowed to pass through.\nParameter Description #     Name Type Description     exclude array List of usernames, from which requests are refused to pass through   include array List of usernames, from which requests are allowed to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':124,'href':'/en/docs/latest/gateway/references/filters/request_user_limiter/','title':"request_user_limiter",'section':"Online Filter",'content':"request_user_limiter #  Description #  The request_user_limiter filter is used to control traffic by username.\nConfiguration Example #  A configuration example is as follows:\nflow: - name: rate_limit_flow filter: - request_user_limiter: user: - elastic - medcl max_requests: 256 # max_bytes: 102400 #100k action: retry # retry or drop # max_retry_times: 1000 # retry_interval: 500 #100ms message: \u0026quot;you reached our limit\u0026quot; The above configuration controls the traffic of users medcl and elastic and the allowable maximum QPS is 256 per second.\nParameter Description #     Name Type Description     user array Users who will participate in traffic control. If this parameter is not set, all users will participate in traffic control.   interval string Interval for evaluating whether traffic control conditions are met. The default value is 1s.   max_requests int Maximum request count limit in the interval   max_bytes int Maximum request traffic limit in the interval   action string Processing action after traffic control is triggered. The value can be set as retry or drop and the default value is retry.   message string Rejection message returned for a request, for which traffic control conditions are met   retry_interval int Interval for traffic control retry, in milliseconds. The default value is 10.   max_retry_times int Maximum retry count in the case of traffic control retries. The default value is 1000.    "});index.add({'id':125,'href':'/en/docs/latest/gateway/references/filters/response_body_regex_replace/','title':"response_body_regex_replace",'section':"Online Filter",'content':"response_body_regex_replace #  Description #  The response_body_regex_replace filter is used to replace string content in a response by using a regular expression.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - echo: message: \u0026quot;hello infini\\n\u0026quot; - response_body_regex_replace: pattern: infini to: world The result output of the preceding example is hello world.\nParameter Description #     Name Type Description     pattern string Regular expression used for matching and replacement   to string Target string used for replacement    "});index.add({'id':126,'href':'/en/docs/latest/gateway/references/filters/response_header_filter/','title':"response_header_filter",'section':"Online Filter",'content':"response_header_filter #  Description #  The response_header_filter is used to filter traffic based on response header information.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: ... - response_header_filter: exclude: - INFINI-CACHE: CACHED The above example shows that a request is not allowed to pass through when the header information of the response contains INFINI-CACHE: CACHED.\nParameter Description #     Name Type Description     exclude array Response header information for refusing to allow traffic to pass through   include array Response header information for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':127,'href':'/en/docs/latest/gateway/references/filters/response_header_format/','title':"response_header_format",'section':"Online Filter",'content':"response_header_format #  Description #  The response_header_format filter is used to convert keys in response header information into lowercase letters.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_header_format: "});index.add({'id':128,'href':'/en/docs/latest/gateway/references/filters/response_status_filter/','title':"response_status_filter",'section':"Online Filter",'content':"response_status_filter #  Description #  The response_status_filter is used to filter traffic based on the status code responded by the back-end service.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - response_status_filter: message: \u0026quot;Request filtered!\u0026quot; exclude: - 404 include: - 200 - 201 - 500 Parameter Description #     Name Type Description     exclude array Response code for refusing to allow traffic to pass through   include array Response code for allowing traffic to pass through   action string Processing action after filtering conditions are met. The value can be set to deny or redirect_flow and the default value is deny.   status int Status code returned after the user-defined mode is matched   message string Message text returned in user-defined deny mode   flow string ID of the flow executed in user-defined redirect_flow mode    Note: If the include condition is met, requests are allowed to pass through only when at least one response code in include is met. If only the exclude condition is met, any request that does not meet exclude is allowed to pass through.  "});index.add({'id':129,'href':'/en/docs/latest/gateway/references/filters/retry_limiter/','title':"retry_limiter",'section':"Online Filter",'content':"retry_limiter #  Description #  The retry_limiter filter is used to judge whether the maximum retry count is reached for a request, to avert unlimited retries of a request.\nConfiguration Example #  A simple example is as follows:\nflow: - name: retry_limiter filter: - retry_limiter: queue_name: \u0026quot;deadlock_messages\u0026quot; max_retry_times: 3 Parameter Description #     Name Type Description     max_retry_times int Maximum retry count. The default value is 3.   queue_name string Name of a message queue, to which messages are output after the maximum retry count is reached   tag_on_success array Specified tag to be attached to request context after retry conditions are triggered    "});index.add({'id':130,'href':'/en/docs/latest/gateway/references/filters/sample/','title':"sample",'section':"Online Filter",'content':"sample #  Description #  The sample filter is used to sample normal traffic proportionally. In a massive query scenario, collecting logs of all traffic consumes considerable resources. Therefore, you are advised to perform sampling statistics and sample and analyze query logs.\nConfiguration Example #  A simple example is as follows:\nflow: - name: sample filter: - sample: ratio: 0.2 Parameter Description #     Name Type Description     ratio float Sampling ratio    "});index.add({'id':131,'href':'/en/docs/latest/gateway/references/filters/set_basic_auth/','title':"set_basic_auth",'section':"Online Filter",'content':"set_basic_auth #  Description #  The set_basic_auth filter is used to configure the authentication information used for requests. You can use the filter to reset the authentication information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_basic_auth filter: - set_basic_auth: username: admin password: password Parameter Description #     Name Type Description     username string Username   password string Password    "});index.add({'id':132,'href':'/en/docs/latest/gateway/references/filters/set_context/','title':"set_context",'section':"Online Filter",'content':"set_context #  Description #  The set_context filter is used to set relevant information for the request context.\nConfiguration Example #  A simple example is as follows:\nflow: - name: test filter: - set_response: body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' - set_context: context: # _ctx.request.uri: http://baidu.com # _ctx.request.path: new_request_path # _ctx.request.host: api.infinilabs.com # _ctx.request.method: DELETE # _ctx.request.body: \u0026quot;hello world\u0026quot; # _ctx.request.body_json.explain: true # _ctx.request.query_args.from: 100 # _ctx.request.header.ENV: dev # _ctx.response.content_type: \u0026quot;application/json\u0026quot; # _ctx.response.header.TIMES: 100 # _ctx.response.status: 419 # _ctx.response.body: \u0026quot;new_body\u0026quot; _ctx.response.body_json.success: true - dump: request: true Parameter Description #     Name Type Description     context map Request context and corresponding value    A list of supported context variables is provided below:\n   Name Type Description     _ctx.request.uri string Complete URL of a request   _ctx.request.path string Request path   _ctx.request.host string Request host   _ctx.request.method string Request method type   _ctx.request.body string Request body   _ctx.request.body_json.[JSON_PATH] string Path to the JSON request object   _ctx.request.query_args.[KEY] string URL query request parameter   _ctx.request.header.[KEY] string Request header information   _ctx.response.content_type string Request body type   _ctx.response.header.[KEY] string Response header information   _ctx.response.status int Returned status code   _ctx.response.body string Returned response body   _ctx.response.body_json.[JSON_PATH] string Path to the JSON response object    "});index.add({'id':133,'href':'/en/docs/latest/gateway/references/filters/set_hostname/','title':"set_hostname",'section':"Online Filter",'content':"set_hostname #  Description #  The set_hostname filter is used to set the host or domain name to be accessed in the request header.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_hostname filter: - set_hostname: hostname: api.infini.sh Parameter Description #     Name Type Description     hostname string Host information    "});index.add({'id':134,'href':'/en/docs/latest/gateway/references/filters/set_request_header/','title':"set_request_header",'section':"Online Filter",'content':"set_request_header #  Description #  The set_request_header filter is used to set header information for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_header filter: - set_request_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    "});index.add({'id':135,'href':'/en/docs/latest/gateway/references/filters/set_request_query_args/','title':"set_request_query_args",'section':"Online Filter",'content':"set_request_query_args #  Description #  The set_request_query_args filter is used to set the QueryString parameter information used for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_request_query_args filter: - set_request_query_args: args: - size -\u0026gt; 10 Parameter Description #     Name Type Description     args map It uses -\u0026gt; to identify a key value pair and set QueryString parameter information.    "});index.add({'id':136,'href':'/en/docs/latest/gateway/references/filters/set_response/','title':"set_response",'section':"Online Filter",'content':"set_response #  Description #  The set_response filter is used to set response information to be returned for requests.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response filter: - set_response: status: 200 content_type: application/json body: '{\u0026quot;message\u0026quot;:\u0026quot;hello world\u0026quot;}' Parameter Description #     Name Type Description     status int Request status code, which is 200 by default.   content_type string Type of returned content   body string Returned structural body    "});index.add({'id':137,'href':'/en/docs/latest/gateway/references/filters/set_response_header/','title':"set_response_header",'section':"Online Filter",'content':"set_response_header #  Description #  The set_response_header filter is used to set the header information used in responses.\nConfiguration Example #  A simple example is as follows:\nflow: - name: set_response_header filter: - set_response_header: headers: - Trial -\u0026gt; true - Department -\u0026gt; Engineering Parameter Description #     Name Type Description     headers map It uses -\u0026gt; to identify a key value pair and set header information.    "});index.add({'id':138,'href':'/en/docs/latest/gateway/references/filters/sleep/','title':"sleep",'section':"Online Filter",'content':"sleep #  Description #  The sleep filter is used to add a fixed delay to requests to reduce the speed.\nConfiguration Example #  A simple example is as follows:\nflow: - name: slow_query_logging_test filter: - sleep: sleep_in_million_seconds: 1024 Parameter Description #     Name Type Description     sleep_in_million_seconds int64 Delay to be added, in milliseconds    "});index.add({'id':139,'href':'/en/docs/latest/gateway/references/filters/switch/','title':"switch",'section':"Online Filter",'content':"switch #  Description #  The switch filter is used to forward traffic to another flow along the requested path, to facilitate cross-cluster operations. No alternation is required for Elasticsearch clusters, and all APIs in each cluster can be accessed, including APIs used for index read/write and cluster operations.\nConfiguration Example #  A simple example is as follows:\nflow: - name: es1-flow filter: - elasticsearch: elasticsearch: es1 - name: es2-flow filter: - elasticsearch: elasticsearch: es2 - name: cross_cluste_search filter: - switch: path_rules: - prefix: \u0026quot;es1:\u0026quot; flow: es1-flow - prefix: \u0026quot;es2:\u0026quot; flow: es2-flow - elasticsearch: elasticsearch: dev #elasticsearch configure reference name In the above example, the index beginning with es1: is forwarded to the es1 cluster, the index beginning with es2: is forwarded to the es2 cluster, and unmatched indexes are forwarded to the dev cluster. Clusters of different versions can be controlled within one Kibana. See the following example.\n# GET es1:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 37, \u0026quot;active_shards\u0026quot; : 37, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 9, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 80.43478260869566 } # GET es2:_cluster/health { \u0026quot;cluster_name\u0026quot; : \u0026quot;elasticsearch\u0026quot;, \u0026quot;status\u0026quot; : \u0026quot;yellow\u0026quot;, \u0026quot;timed_out\u0026quot; : false, \u0026quot;number_of_nodes\u0026quot; : 1, \u0026quot;number_of_data_nodes\u0026quot; : 1, \u0026quot;active_primary_shards\u0026quot; : 6, \u0026quot;active_shards\u0026quot; : 6, \u0026quot;relocating_shards\u0026quot; : 0, \u0026quot;initializing_shards\u0026quot; : 0, \u0026quot;unassigned_shards\u0026quot; : 6, \u0026quot;delayed_unassigned_shards\u0026quot; : 0, \u0026quot;number_of_pending_tasks\u0026quot; : 0, \u0026quot;number_of_in_flight_fetch\u0026quot; : 0, \u0026quot;task_max_waiting_in_queue_millis\u0026quot; : 0, \u0026quot;active_shards_percent_as_number\u0026quot; : 50.0 } You can run commands to achieve the same effect.\nroot@infini:/opt/gateway# curl -v 192.168.3.4:8000/es1:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es1:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:39 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 45 \u0026lt; X-Backend-Cluster: dev1 \u0026lt; X-Backend-Server: 192.168.3.188:9299 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 48 38 5 cdhilmrstw * LENOVO * Connection #0 to host 192.168.3.4 left intact root@infini:/opt/gateway# curl -v 192.168.3.4:8000/es2:_cat/nodes * Trying 192.168.3.4... * TCP_NODELAY set * Connected to 192.168.3.4 (192.168.3.4) port 8000 (#0) \u0026gt; GET /es2:_cat/nodes HTTP/1.1 \u0026gt; Host: 192.168.3.4:8000 \u0026gt; User-Agent: curl/7.58.0 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 200 OK \u0026lt; Server: INFINI \u0026lt; Date: Thu, 14 Oct 2021 10:37:48 GMT \u0026lt; content-type: text/plain; charset=UTF-8 \u0026lt; Content-Length: 146 \u0026lt; X-elastic-product: Elasticsearch \u0026lt; Warning: 299 Elasticsearch-7.14.0-dd5a0a2acaa2045ff9624f3729fc8a6f40835aa1 \u0026quot;Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-minimal-setup.html to enable security.\u0026quot; \u0026lt; X-Backend-Cluster: dev \u0026lt; X-Backend-Server: 192.168.3.188:9216 \u0026lt; X-Filters: filters-\u0026gt;switch-\u0026gt;filters-\u0026gt;elasticsearch-\u0026gt;skipped \u0026lt; 192.168.3.188 26 38 3 cdfhilmrstw - node-714-1 192.168.3.188 45 38 3 cdfhilmrstw * LENOVO 192.168.3.188 43 38 4 cdfhilmrstw - node-714-2 * Connection #0 to host 192.168.3.4 left intact Parameter Description #     Name Type Description     path_rules array Matching rule based on the URL   path_rules.prefix string Prefix string for matching. It is recommended that the prefix string end with :. After matching, the URL prefix is removed from the traffic, which is then forwarded to the subsequent flow.   path_rules.flow string Name of the flow for processing a matched request   remove_prefix bool Whether to remove matched prefix string before request forwarding. The default value is true.    "});index.add({'id':140,'href':'/en/docs/latest/gateway/references/filters/translog/','title':"translog",'section':"Online Filter",'content':"translog #  Description #  The translog filter is used to save received requests to local files and compress them. It can record some or complete request logs for archiving and request replay.\nConfiguration Example #  A simple example is as follows:\nflow: - name: translog filter: - translog: max_file_age: 7 max_file_count: 10 Parameter Description #     Name Type Description     path string Root directory for log storage, which is the translog subdirectory in the gateway data directory by default   category string Level-2 subdirectory for differentiating different logs, which is default by default.   filename string Name of the log storage file, which is translog.log by default.   compress bool Whether to compress and archive files after scrolling. The default value is true.   max_file_age int Maximum number of days that archived files can be retained, which is 30 days by default.   max_file_count int Maximum number of archived files that can be retained, which is 100 by default.   max_file_size_in_mb int Maximum size of a single archived file, in bytes. The default value is 1024 MB.    "});})();